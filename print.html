<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>blog</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/style3.css">


    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');
                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }
                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="about.html">About</a></li><li class="chapter-item expanded affix "><li class="part-title">机器学习</li><li class="spacer"></li><li class="chapter-item expanded "><a href="pytorch/index.html"><strong aria-hidden="true">1.</strong> PyTorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/pytorch_pocket_reference.html"><strong aria-hidden="true">1.1.</strong> pytorch pocket reference 笔记</a></li><li class="chapter-item "><a href="pytorch/mini-torch.html"><strong aria-hidden="true">1.2.</strong> mini torch</a></li></ol></li><li class="chapter-item expanded "><a href="tengine/index.html"><strong aria-hidden="true">2.</strong> Tengine</a></li><li class="chapter-item expanded "><a href="nlp/index.html"><strong aria-hidden="true">3.</strong> NLP</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="nlp/bert.html"><strong aria-hidden="true">3.1.</strong> Bert</a></li></ol></li><li class="chapter-item expanded "><a href="nncase/index.html"><strong aria-hidden="true">4.</strong> nncase</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="nncase/compile_flow.html"><strong aria-hidden="true">4.1.</strong> comile流程</a></li><li class="chapter-item "><a href="nncase/k210.html"><strong aria-hidden="true">4.2.</strong> k210 standalone sdk</a></li><li class="chapter-item "><a href="nncase/face_detect_example.html"><strong aria-hidden="true">4.3.</strong> face detect example</a></li></ol></li><li class="chapter-item expanded "><a href="ray/index.html"><strong aria-hidden="true">5.</strong> Ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/paper.html"><strong aria-hidden="true">5.1.</strong> Ray Paper</a></li><li class="chapter-item "><a href="ray/remote.html"><strong aria-hidden="true">5.2.</strong> Ray Remote</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Tensorflow</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tensorflow/index.html"><strong aria-hidden="true">6.1.</strong> tensorflow inside</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tensorflow/executor.html"><strong aria-hidden="true">6.1.1.</strong> Executor: 执行Computation Sub Graph</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tensorflow/executor-subgraph-preprocess.html"><strong aria-hidden="true">6.1.1.1.</strong> SubGraph预处理：Node/NodeItem/TaggedNode</a></li><li class="chapter-item "><a href="tensorflow/flow-control-op.html"><strong aria-hidden="true">6.1.1.2.</strong> Flow control op: switch/merge/enter/exit/nextIteration</a></li><li class="chapter-item "><a href="tensorflow/executor-frame.html"><strong aria-hidden="true">6.1.1.3.</strong> Frame: ControlFlowInfo/FrameInfo/FrameState/IterationState</a></li></ol></li><li class="chapter-item "><a href="tensorflow/direct-session.html"><strong aria-hidden="true">6.1.2.</strong> DirectSession: 单机执行computation graph</a></li><li class="chapter-item "><a href="tensorflow/rendezvous.html"><strong aria-hidden="true">6.1.3.</strong> RendezVous：跨设备，跨主机通信</a></li><li class="chapter-item "><a href="tensorflow/device.html"><strong aria-hidden="true">6.1.4.</strong> Device：计算单元抽象(CPU/GPU)</a></li></ol></li><li class="chapter-item "><a href="tensorflow/optimize.html"><strong aria-hidden="true">6.2.</strong> tensorflow模型对接工程优化</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tensorflow/export-keras-model-as-tf-frozen-graph.html"><strong aria-hidden="true">6.2.1.</strong> 将keras模型导出为tf frozen graph</a></li><li class="chapter-item "><a href="tensorflow/replace-placeholder-with-iterator.html"><strong aria-hidden="true">6.2.2.</strong> 使用dataset iterator 优化keras model预测的吞吐量</a></li><li class="chapter-item "><a href="tensorflow/stat-cpu-gpu-load.html"><strong aria-hidden="true">6.2.3.</strong> 统计cpu/gpu 负载率脚本</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="pthread/index.html"><strong aria-hidden="true">7.</strong> pthread</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pthread/pthread-primer.html"><strong aria-hidden="true">7.1.</strong> Pthread Primer笔记</a></li><li class="chapter-item "><a href="pthread/glibc-pthread-implement-thread-life-cycle.html"><strong aria-hidden="true">7.2.</strong> Pthread线程生命周期</a></li><li class="chapter-item "><a href="pthread/glibc-pthread-implement-sync.html"><strong aria-hidden="true">7.3.</strong> Pthread线程同步</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Rust</li><li class="spacer"></li><li class="chapter-item expanded "><a href="yew/index.html"><strong aria-hidden="true">8.</strong> Yew</a></li><li class="chapter-item expanded "><a href="axum/index.html"><strong aria-hidden="true">9.</strong> Axum</a></li><li class="chapter-item expanded "><a href="axum/hyper.html"><strong aria-hidden="true">10.</strong> Hyper</a></li><li class="chapter-item expanded "><a href="tokio/index.html"><strong aria-hidden="true">11.</strong> tokio</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tokio/executor.html"><strong aria-hidden="true">11.1.</strong> Executor</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tokio/park.html"><strong aria-hidden="true">11.1.1.</strong> Park</a></li><li class="chapter-item "><a href="tokio/thread-pool.html"><strong aria-hidden="true">11.1.2.</strong> thread pool</a></li></ol></li><li class="chapter-item "><a href="tokio/driver.html"><strong aria-hidden="true">11.2.</strong> driver</a></li><li class="chapter-item "><a href="tokio/io.html"><strong aria-hidden="true">11.3.</strong> io</a></li><li class="chapter-item "><a href="tokio/codec.html"><strong aria-hidden="true">11.4.</strong> codec</a></li><li class="chapter-item "><a href="tokio/channel.html"><strong aria-hidden="true">11.5.</strong> channel</a></li><li class="chapter-item "><a href="tokio/waker.html"><strong aria-hidden="true">11.6.</strong> waker</a></li></ol></li><li class="chapter-item expanded "><a href="crossbeam/index.html"><strong aria-hidden="true">12.</strong> crossbeam</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="crossbeam/skiplist.html"><strong aria-hidden="true">12.1.</strong> SkipList(draft)</a></li></ol></li><li class="chapter-item expanded "><a href="rust/index.html"><strong aria-hidden="true">13.</strong> draft Rust</a></li><li class="chapter-item expanded affix "><li class="part-title">GO</li><li class="spacer"></li><li class="chapter-item expanded "><a href="golang/index.html"><strong aria-hidden="true">14.</strong> Go</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="golang/pgm.html"><strong aria-hidden="true">14.1.</strong> Runtime PGM调度模型</a></li><li class="chapter-item "><a href="golang/goroutine-stack.html"><strong aria-hidden="true">14.2.</strong> Goroutine Stack</a></li><li class="chapter-item "><a href="golang/memory.html"><strong aria-hidden="true">14.3.</strong> Memory分配</a></li><li class="chapter-item "><a href="golang/GC.html"><strong aria-hidden="true">14.4.</strong> GC</a></li><li class="chapter-item "><a href="golang/context.html"><strong aria-hidden="true">14.5.</strong> Context</a></li><li class="chapter-item "><a href="golang/defer-panic-recover.html"><strong aria-hidden="true">14.6.</strong> defer-panic-recover</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">数据库</li><li class="spacer"></li><li class="chapter-item expanded "><a href="leveldb/index.html"><strong aria-hidden="true">15.</strong> LevelDB</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="leveldb/draft.html"><strong aria-hidden="true">15.1.</strong> draft</a></li><li class="chapter-item "><a href="leveldb/code-struct.html"><strong aria-hidden="true">15.2.</strong> 代码模块间关系</a></li><li class="chapter-item "><a href="leveldb/write.html"><strong aria-hidden="true">15.3.</strong> Write 流程</a></li><li class="chapter-item "><a href="leveldb/read.html"><strong aria-hidden="true">15.4.</strong> Read 流程</a></li><li class="chapter-item "><a href="leveldb/table-format.html"><strong aria-hidden="true">15.5.</strong> SSTable 文件格式和读写</a></li><li class="chapter-item "><a href="leveldb/versionset.html"><strong aria-hidden="true">15.6.</strong> versionset和Manifest</a></li><li class="chapter-item "><a href="leveldb/table-compact.html"><strong aria-hidden="true">15.7.</strong> Do Compact</a></li><li class="chapter-item "><a href="leveldb/iterator.html"><strong aria-hidden="true">15.8.</strong> Iterator迭代器</a></li><li class="chapter-item "><a href="leveldb/bloom-filter.html"><strong aria-hidden="true">15.9.</strong> Bloom filter</a></li></ol></li><li class="chapter-item expanded "><a href="rocksdb/index.html"><strong aria-hidden="true">16.</strong> RocksDB</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="rocksdb/draft.html"><strong aria-hidden="true">16.1.</strong> draft</a></li><li class="chapter-item "><a href="rocksdb/column-family.html"><strong aria-hidden="true">16.2.</strong> 主要struct引用关系</a></li><li class="chapter-item "><a href="rocksdb/wal.html"><strong aria-hidden="true">16.3.</strong> Write Ahead Log</a></li><li class="chapter-item "><a href="rocksdb/write.html"><strong aria-hidden="true">16.4.</strong> write 并发控制</a></li><li class="chapter-item "><a href="rocksdb/flush-and-compact.html"><strong aria-hidden="true">16.5.</strong> 后台flush和compact线程</a></li><li class="chapter-item "><a href="rocksdb/leveled-compaction-picker.html"><strong aria-hidden="true">16.6.</strong> Leveled Compaction Picker</a></li><li class="chapter-item "><a href="rocksdb/read.html"><strong aria-hidden="true">16.7.</strong> read 流程</a></li><li class="chapter-item "><a href="rocksdb/blob.html"><strong aria-hidden="true">16.8.</strong> Blob</a></li><li class="chapter-item "><a href="rocksdb/transaction.html"><strong aria-hidden="true">16.9.</strong> 事务</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="rocksdb/optimistic-transaction.html"><strong aria-hidden="true">16.9.1.</strong> Optimistic Transaction</a></li><li class="chapter-item "><a href="rocksdb/transaction-lock-mgr.html"><strong aria-hidden="true">16.9.2.</strong> Transaction lock mgr</a></li><li class="chapter-item "><a href="rocksdb/two-phase-commit.html"><strong aria-hidden="true">16.9.3.</strong> two phase commit</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="clickhouse/index.html"><strong aria-hidden="true">17.</strong> ClickHouse(daft)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="clickhouse/server-main.html"><strong aria-hidden="true">17.1.</strong> Server main</a></li><li class="chapter-item "><a href="clickhouse/block.html"><strong aria-hidden="true">17.2.</strong> block</a></li><li class="chapter-item "><a href="clickhouse/blockio.html"><strong aria-hidden="true">17.3.</strong> blockio</a></li><li class="chapter-item "><a href="clickhouse/storage.html"><strong aria-hidden="true">17.4.</strong> Storage</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="clickhouse/merge-tree-data.html"><strong aria-hidden="true">17.4.1.</strong> MergeTreeData</a></li><li class="chapter-item "><a href="clickhouse/storage-merge-tree.html"><strong aria-hidden="true">17.4.2.</strong> StorageMergeTree</a></li></ol></li></ol></li><li class="chapter-item expanded "><li class="part-title">PingCAP</li><li class="spacer"></li><li class="chapter-item expanded "><a href="tidb/index.html"><strong aria-hidden="true">18.</strong> TiDB</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/note.html"><strong aria-hidden="true">18.1.</strong> tidb学习资料整理</a></li><li class="chapter-item "><a href="tidb/main.html"><strong aria-hidden="true">18.2.</strong> Server Main Loop</a></li><li class="chapter-item "><a href="tidb/insert.html"><strong aria-hidden="true">18.3.</strong> Insert 语句</a></li><li class="chapter-item "><a href="tidb/select.html"><strong aria-hidden="true">18.4.</strong> Select 语句</a></li><li class="chapter-item "><a href="tidb/types.html"><strong aria-hidden="true">18.5.</strong> 数据类型</a></li><li class="chapter-item "><a href="tidb/expression.html"><strong aria-hidden="true">18.6.</strong> expression</a></li><li class="chapter-item "><a href="tidb/ddl.html"><strong aria-hidden="true">18.7.</strong> Online DDL</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/ddl-schema-in-tikv.html"><strong aria-hidden="true">18.7.1.</strong> Schema 存储</a></li><li class="chapter-item "><a href="tidb/ddl-schema-load.html"><strong aria-hidden="true">18.7.2.</strong> Schema Cache和加载</a></li><li class="chapter-item "><a href="tidb/ddl-schema-modification.html"><strong aria-hidden="true">18.7.3.</strong> Schema Modification</a></li><li class="chapter-item "><a href="tidb/ddl-online-schema-change.html"><strong aria-hidden="true">18.7.4.</strong> Online Schema Change</a></li></ol></li><li class="chapter-item "><a href="tidb/stat.html"><strong aria-hidden="true">18.8.</strong> 统计信息</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/basic-concepts.html"><strong aria-hidden="true">18.8.1.</strong> 基本概念</a></li><li class="chapter-item "><a href="tidb/stat-tables.html"><strong aria-hidden="true">18.8.2.</strong> stats tables</a></li><li class="chapter-item "><a href="tidb/stat-analyze.html"><strong aria-hidden="true">18.8.3.</strong> Analyze</a></li><li class="chapter-item "><a href="tidb/stat-feedback.html"><strong aria-hidden="true">18.8.4.</strong> Query Feedback</a></li><li class="chapter-item "><a href="tidb/use-stat.html"><strong aria-hidden="true">18.8.5.</strong> 统计信息使用场景</a></li></ol></li><li class="chapter-item "><a href="tidb/logical-optimize.html"><strong aria-hidden="true">18.9.</strong> Logical Optimize</a></li><li class="chapter-item "><a href="tidb/physical-optimize.html"><strong aria-hidden="true">18.10.</strong> Physical Optimize</a></li><li class="chapter-item "><a href="tidb/datasource.html"><strong aria-hidden="true">18.11.</strong> DataSource</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/datasource-build.html"><strong aria-hidden="true">18.11.1.</strong> buildDataSource</a></li><li class="chapter-item "><a href="tidb/range.html"><strong aria-hidden="true">18.11.2.</strong> 索引范围计算</a></li><li class="chapter-item "><a href="tidb/tablecodec.html"><strong aria-hidden="true">18.11.3.</strong> table/index存储编码</a></li><li class="chapter-item "><a href="tidb/datasource-paritionProcessor.html"><strong aria-hidden="true">18.11.4.</strong> paritionProcessor</a></li><li class="chapter-item "><a href="tidb/datasource-predict-push-down.html"><strong aria-hidden="true">18.11.5.</strong> PredicatePushDown</a></li><li class="chapter-item "><a href="tidb/datasource-physical-optimize.html"><strong aria-hidden="true">18.11.6.</strong> Physical Optimize</a></li><li class="chapter-item "><a href="tidb/datasource-executors.html"><strong aria-hidden="true">18.11.7.</strong> Executors</a></li></ol></li><li class="chapter-item "><a href="tidb/distsql.html"><strong aria-hidden="true">18.12.</strong> DistSQL</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/cop-region-cache.html"><strong aria-hidden="true">18.12.1.</strong> ReginCache</a></li><li class="chapter-item "><a href="tidb/tikv-grpc-client.html"><strong aria-hidden="true">18.12.2.</strong> TiKV GRPC Client</a></li><li class="chapter-item "><a href="tidb/cop-task.html"><strong aria-hidden="true">18.12.3.</strong> CopTask</a></li><li class="chapter-item "><a href="tidb/cop-iterator-worker.html"><strong aria-hidden="true">18.12.4.</strong> CopIteratorWorker</a></li><li class="chapter-item "><a href="tidb/coprocessor.html"><strong aria-hidden="true">18.12.5.</strong> Coprocessor</a></li></ol></li><li class="chapter-item "><a href="tidb/join.html"><strong aria-hidden="true">18.13.</strong> Join</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/join-alg.html"><strong aria-hidden="true">18.13.1.</strong> Join算法</a></li><li class="chapter-item "><a href="tidb/join-logical-optimize.html"><strong aria-hidden="true">18.13.2.</strong> Logical Optimize</a></li><li class="chapter-item "><a href="tidb/join-physical-optimize.html"><strong aria-hidden="true">18.13.3.</strong> Physical Optimize</a></li><li class="chapter-item "><a href="tidb/hash-join.html"><strong aria-hidden="true">18.13.4.</strong> Executor: Hash Join</a></li><li class="chapter-item "><a href="tidb/merge-join.html"><strong aria-hidden="true">18.13.5.</strong> Executor: Merge Join</a></li><li class="chapter-item "><a href="tidb/index-lookup-join.html"><strong aria-hidden="true">18.13.6.</strong> Executor: Index Lookup Join</a></li></ol></li><li class="chapter-item "><a href="tidb/agg.html"><strong aria-hidden="true">18.14.</strong> Agg</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tidb/agg-func.html"><strong aria-hidden="true">18.14.1.</strong> AggFunc</a></li><li class="chapter-item "><a href="tidb/hash-agg.html"><strong aria-hidden="true">18.14.2.</strong> Executor: HashAgg</a></li><li class="chapter-item "><a href="tidb/stream-agg.html"><strong aria-hidden="true">18.14.3.</strong> Executor: StreamAgg</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="pd/index.html"><strong aria-hidden="true">19.</strong> PD</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="tikv/index.html"><strong aria-hidden="true">20.</strong> TiKV</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/raft2.html"><strong aria-hidden="true">20.1.</strong> raft-rs</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/raft_rawnode.html"><strong aria-hidden="true">20.1.1.</strong> RawNode</a></li><li class="chapter-item "><a href="tikv/raft_storage.html"><strong aria-hidden="true">20.1.2.</strong> Storage</a></li><li class="chapter-item "><a href="tikv/progress_tracker.html"><strong aria-hidden="true">20.1.3.</strong> ProgressTracker</a></li><li class="chapter-item "><a href="tikv/raft_hearbeat.html"><strong aria-hidden="true">20.1.4.</strong> Hearbeat</a></li><li class="chapter-item "><a href="tikv/raft_election.html"><strong aria-hidden="true">20.1.5.</strong> Election</a></li><li class="chapter-item "><a href="tikv/raft_log_entry.html"><strong aria-hidden="true">20.1.6.</strong> LogEntry</a></li><li class="chapter-item "><a href="tikv/raft_snapshot.html"><strong aria-hidden="true">20.1.7.</strong> Snapshot</a></li><li class="chapter-item "><a href="tikv/raft_conf_change.html"><strong aria-hidden="true">20.1.8.</strong> ConfChange</a></li><li class="chapter-item "><a href="tikv/raft_read_index.html"><strong aria-hidden="true">20.1.9.</strong> ReadIndex</a></li></ol></li><li class="chapter-item "><a href="tikv/raft-kv.html"><strong aria-hidden="true">20.2.</strong> RaftKV</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/batch-system2.html"><strong aria-hidden="true">20.2.1.</strong> BatchSystem</a></li><li class="chapter-item "><a href="tikv/raft_message.html"><strong aria-hidden="true">20.2.2.</strong> RaftMessage</a></li><li class="chapter-item "><a href="tikv/raft_client.html"><strong aria-hidden="true">20.2.3.</strong> Raft Client</a></li><li class="chapter-item "><a href="tikv/region2.html"><strong aria-hidden="true">20.2.4.</strong> Region</a></li><li class="chapter-item "><a href="tikv/peer_storage2.html"><strong aria-hidden="true">20.2.5.</strong> PeerStorage</a></li><li class="chapter-item "><a href="tikv/thread_local_engine.html"><strong aria-hidden="true">20.2.6.</strong> Thread Local Engine</a></li><li class="chapter-item "><a href="tikv/leader_lease.html"><strong aria-hidden="true">20.2.7.</strong> Leader Lease</a></li><li class="chapter-item "><a href="tikv/read_index.html"><strong aria-hidden="true">20.2.8.</strong> Read Index</a></li><li class="chapter-item "><a href="tikv/async_snapshot2.html"><strong aria-hidden="true">20.2.9.</strong> Async Snapshot</a></li><li class="chapter-item "><a href="tikv/async_write2.html"><strong aria-hidden="true">20.2.10.</strong> Async Write</a></li><li class="chapter-item "><a href="tikv/region_epoch.html"><strong aria-hidden="true">20.2.11.</strong> Region Epoch</a></li><li class="chapter-item "><a href="tikv/conf_change2.html"><strong aria-hidden="true">20.2.12.</strong> Conf Change</a></li><li class="chapter-item "><a href="tikv/split_region2.html"><strong aria-hidden="true">20.2.13.</strong> Split Region</a></li><li class="chapter-item "><a href="tikv/merge_region2.html"><strong aria-hidden="true">20.2.14.</strong> Merge Region</a></li></ol></li><li class="chapter-item "><a href="tikv/storage2.html"><strong aria-hidden="true">20.3.</strong> Storage</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/percolator.html"><strong aria-hidden="true">20.3.1.</strong> Percolator</a></li><li class="chapter-item "><a href="tikv/percolator_in_tikv.html"><strong aria-hidden="true">20.3.2.</strong> Percolator In TiKV</a></li><li class="chapter-item "><a href="tikv/tikv_2pc.html"><strong aria-hidden="true">20.3.3.</strong> 2PC</a></li><li class="chapter-item "><a href="tikv/async_commit.html"><strong aria-hidden="true">20.3.4.</strong> AsyncCommit</a></li><li class="chapter-item "><a href="tikv/tikv_1pc.html"><strong aria-hidden="true">20.3.5.</strong> OnePC</a></li><li class="chapter-item "><a href="tikv/pessimistic_lock.html"><strong aria-hidden="true">20.3.6.</strong> 悲观事务</a></li><li class="chapter-item "><a href="tikv/txn_recovery.html"><strong aria-hidden="true">20.3.7.</strong> Resolve Lock</a></li><li class="chapter-item "><a href="tikv/scheduler2.html"><strong aria-hidden="true">20.3.8.</strong> Scheduler</a></li><li class="chapter-item "><a href="tikv/wait_lock.html"><strong aria-hidden="true">20.3.9.</strong> Wait Lock</a></li><li class="chapter-item "><a href="tikv/dead_lock.html"><strong aria-hidden="true">20.3.10.</strong> 死锁检测</a></li><li class="chapter-item "><a href="tikv/group_commit.html"><strong aria-hidden="true">20.3.11.</strong> TiDB分组提交</a></li><li class="chapter-item "><a href="tikv/storage_scanner.html"><strong aria-hidden="true">20.3.12.</strong> Scanner</a></li></ol></li><li class="chapter-item "><a href="tikv/coprocessor.html"><strong aria-hidden="true">20.4.</strong> Coprocessor</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/rpn_expression.html"><strong aria-hidden="true">20.4.1.</strong> RpnExpression</a></li><li class="chapter-item "><a href="tikv/aggr_function.html"><strong aria-hidden="true">20.4.2.</strong> AggrFunction</a></li><li class="chapter-item "><a href="tikv/batch_executor.html"><strong aria-hidden="true">20.4.3.</strong> BatchExecutor</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tikv/ranges_scanner.html"><strong aria-hidden="true">20.4.3.1.</strong> RangesScanner</a></li><li class="chapter-item "><a href="tikv/scan_executor.html"><strong aria-hidden="true">20.4.3.2.</strong> ScanExecutor</a></li><li class="chapter-item "><a href="tikv/batch_executor_selection.html"><strong aria-hidden="true">20.4.3.3.</strong> Selection</a></li><li class="chapter-item "><a href="tikv/batch_executor_agg.html"><strong aria-hidden="true">20.4.3.4.</strong> AggExecutor</a></li></ol></li></ol></li><li class="chapter-item "><a href="tikv/performance.html"><strong aria-hidden="true">20.5.</strong> Performance(draft)</a></li><li class="chapter-item "><a href="tikv/yatp.html"><strong aria-hidden="true">20.6.</strong> yatp</a></li></ol></li><li class="chapter-item expanded "><a href="bevy/index.html"><strong aria-hidden="true">21.</strong> Bevy</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="bevy/first-impression-draft.html"><strong aria-hidden="true">21.1.</strong> bevy draft</a></li></ol></li><li class="chapter-item expanded "><a href="blender/index.html"><strong aria-hidden="true">22.</strong> Blender</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="blender/manual-notes.html"><strong aria-hidden="true">22.1.</strong> blender manual notes</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">23.</strong> Trash</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/index.html"><strong aria-hidden="true">23.1.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/records/records.html"><strong aria-hidden="true">23.1.1.</strong> records</a></li></ol></li><li class="chapter-item "><a href="react/index.html"><strong aria-hidden="true">23.2.</strong> react</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="react/from-jsx-to-dom.html"><strong aria-hidden="true">23.2.1.</strong> 从jsx到html dom的流程分析</a></li></ol></li><li class="chapter-item "><a href="godot/index.html"><strong aria-hidden="true">23.3.</strong> Godot</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="godot/learning-note.html"><strong aria-hidden="true">23.3.1.</strong> godot 学习笔记</a></li><li class="chapter-item "><a href="godot/gdquest.html"><strong aria-hidden="true">23.3.2.</strong> gdquest tutorial</a></li></ol></li><li class="chapter-item "><a href="kafka/index.html"><strong aria-hidden="true">23.4.</strong> Kafka</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kafka/client-producer.html"><strong aria-hidden="true">23.4.1.</strong> client: producer</a></li><li class="chapter-item "><a href="kafka/group-coordinator.html"><strong aria-hidden="true">23.4.2.</strong> group coordinator</a></li><li class="chapter-item "><a href="kafka/kafka-produce-fetch.html"><strong aria-hidden="true">23.4.3.</strong> produce and fetch</a></li><li class="chapter-item "><a href="kafka/log.html"><strong aria-hidden="true">23.4.4.</strong> log</a></li><li class="chapter-item "><a href="kafka/partition.html"><strong aria-hidden="true">23.4.5.</strong> Partition</a></li><li class="chapter-item "><a href="kafka/controller-main.html"><strong aria-hidden="true">23.4.6.</strong> Controller</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kafka/controller-channel-manager.html"><strong aria-hidden="true">23.4.6.1.</strong> 通信管理 channelManager</a></li><li class="chapter-item "><a href="kafka/controller-elect.html"><strong aria-hidden="true">23.4.6.2.</strong> 选举</a></li><li class="chapter-item "><a href="kafka/controller-zk.html"><strong aria-hidden="true">23.4.6.3.</strong> zk监听处理</a></li></ol></li><li class="chapter-item "><a href="kafka/replica-assignment.html"><strong aria-hidden="true">23.4.7.</strong> 副本迁移</a></li><li class="chapter-item "><a href="kafka/paritition-replica-statemachine.html"><strong aria-hidden="true">23.4.8.</strong> Partition/Replica状态机</a></li><li class="chapter-item "><a href="kafka/txn_coordinator.html"><strong aria-hidden="true">23.4.9.</strong> 事务</a></li><li class="chapter-item "><a href="kafka/stream.html"><strong aria-hidden="true">23.4.10.</strong> Stream</a></li></ol></li><li class="chapter-item "><a href="java/index.html"><strong aria-hidden="true">23.5.</strong> hotspot</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="java/hotspot-debug-under-osx.html"><strong aria-hidden="true">23.5.1.</strong> osx下编译调试hotspot</a></li><li class="chapter-item "><a href="java/hotspot-thread-created-when-init.html"><strong aria-hidden="true">23.5.2.</strong> jvm的初始化时创建的线程</a></li><li class="chapter-item "><a href="java/hotspot-class-file-load-and-run.html"><strong aria-hidden="true">23.5.3.</strong> class文件的加载和执行</a></li></ol></li><li class="chapter-item "><a href="papers/index.html"><strong aria-hidden="true">23.6.</strong> Paper notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="papers/raft.html"><strong aria-hidden="true">23.6.1.</strong> raft paper</a></li><li class="chapter-item "><a href="papers/gfs.html"><strong aria-hidden="true">23.6.2.</strong> draft: gfs</a></li><li class="chapter-item "><a href="papers/bw-tree.html"><strong aria-hidden="true">23.6.3.</strong> draft: Bw-tree(draft)</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">blog</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <!-- Page table of contents -->
                    <div class="sidetoc"><nav class="pagetoc"></nav></div>
                    <main>
                        <h1 id="about"><a class="header" href="#about">About</a></h1>
<p>记录了一些代码阅读和读书笔记.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pytorch"><a class="header" href="#pytorch">PyTorch</a></h1>
<h2 id="相关缩写"><a class="header" href="#相关缩写">相关缩写</a></h2>
<pre><code>C10 = Caffe Tensor Library(Core Tensor Library)存在这两种说法吧
aten = a tensor library
THP = TorcH Python
TH = TorcH
THC = TorcH Cuda
THCS = TorcH Cuda Sparse
THCUNN = TorcH Cuda Neural Network
THD = TorcH Distributed
THNN = TorcH Neural Network
TH = TorcH Sparse
</code></pre>
<h2 id="tch-rs"><a class="header" href="#tch-rs">tch-rs</a></h2>
<p>tensor 相关结构之间关系</p>
<p><img src="pytorch/./dots/pytorch_hello.svg" alt="" /></p>
<h2 id="autograd"><a class="header" href="#autograd">autograd</a></h2>
<p><img src="pytorch/./dots/compuate_graph.svg" alt="" /></p>
<p>梯度反向传播</p>
<p>requires_grad 这个具有传染性, 
grad_fn</p>
<pre><code class="language-python">&gt;&gt; t1 = torch.randn((3,3), requires_grad=True);
&gt;&gt; t2 = 3 * t2;
&gt;&gt; t2.requires_grad
True
&gt;&gt;&gt; t2.grad_fn
&lt;MulBackward0 object at 0x100df4f70&gt;
</code></pre>
<p>torch.nn.Autograd.Function class</p>
<p>toch/autograd/function.py:222</p>
<pre><code class="language-py"># mypy doesn't understand `with_metaclass` from torch._six
class Function(with_metaclass(FunctionMeta, _C._FunctionBase, FunctionCtx, _HookMixin)):  # type: ignore[misc]
    r&quot;&quot;&quot;Base class to create custom `autograd.Function`

    To create a custom `autograd.Function`, subclass this class and implement
    the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom
    op in the forward pass, call the class method ``apply``. Do not call
    :meth:`forward` directly.

    To ensure correctness and best performance, make sure you are calling the
    correct methods on ``ctx`` and validating your backward function using
    :func:`torch.autograd.gradcheck`.

    See :ref:`extending-autograd` for more details on how to use this class.
</code></pre>
<p>最主要的两个方法，forward 和backward</p>
<blockquote>
<p>gradient that is backpropagated to f from the layers in front of it multiplied by the local 
gradient of the output of f with respect to it's inputs.</p>
</blockquote>
<p>向自己的每个input 反向传播。</p>
<pre><code class="language-python">def backward (incoming_gradients):
	self.Tensor.grad = incoming_gradients

	for inp in self.inputs:
		if inp.grad_fn is not None:
			new_incoming_gradients = //
			  incoming_gradient * local_grad(self.Tensor, inp)
			
			inp.grad_fn.backward(new_incoming_gradients)
		else:
			pass
</code></pre>
<h2 id="学习资料"><a class="header" href="#学习资料">学习资料</a></h2>
<ol>
<li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd</a></li>
<li><a href="https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb">Implementing word2vec in PyTorch (skip-gram model)</a></li>
<li><a href="https://blog.paperspace.com/ultimate-guide-to-pytorch/">The Ultimate Guide To PyTorch</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pytorch-pocket-reference-笔记"><a class="header" href="#pytorch-pocket-reference-笔记">《Pytorch Pocket Reference》 笔记</a></h1>
<p><img src="pytorch/./dots/Pytorch_Pocket_Reference.png" alt="" /></p>
<!-- toc -->
<h2 id="tensor"><a class="header" href="#tensor">Tensor</a></h2>
<p>tensor相关属性和方法</p>
<pre><code class="language-python">import torch

x = torch.tensor([[1.0,2], [3,4]], requires_grad=True)
print(x)
print(x[1,1])
print(x[1,1].item())
print(&quot;shape of x is&quot;, x.shape)
print(&quot;ndim of x is&quot;, x.ndim)
print(&quot;device of x is &quot;, x.device)
print(&quot;layout of x is&quot;, x.layout)

f = x.pow(2).sum()
print(f)
print(&quot;before backward, grad of x is&quot;, x.grad)
f.backward()
print(&quot;after backwrad, grad of x is&quot;, x.grad)
print(&quot;grad_fn of x is&quot;, x.grad_fn)
print(&quot;grad_fn of f is&quot;, f.grad_fn)
</code></pre>
<p><img src="pytorch/./dots/tensor.svg" alt="" /></p>
<h2 id="deep-learning-dev-with-pytorch"><a class="header" href="#deep-learning-dev-with-pytorch">deep learning dev with Pytorch</a></h2>
<p>深度学习开发流程，先数据预处理，然后做模型训练，最后做模型推理部署。</p>
<p><img src="pytorch/./dots/pytorch_dev_flow.png" alt="" /></p>
<h3 id="数据预处理"><a class="header" href="#数据预处理">数据预处理</a></h3>
<p><img src="pytorch/./dots/pytorch_dev_flow.svg" alt="" /></p>
<h3 id="模型开发"><a class="header" href="#模型开发">模型开发</a></h3>
<p>torch.nn 预制了很多函数和神经网络层</p>
<ol>
<li>神经网络layer(有全连接层，卷积，池化，normalized, dropout, 各种非线性激活函数）</li>
<li>损失函数</li>
<li>优化器</li>
<li>layer的container.</li>
</ol>
<p><img src="pytorch/./dots/model_dev.svg" alt="" /></p>
<h3 id="模型训练"><a class="header" href="#模型训练">模型训练</a></h3>
<pre><code class="language-python">from torch import optim
from torch import nn


# 定义模型
model = LeNet5().to(device)

# 定义损失函数
criterion = nn.CrossEntropyLoss()

# 定义优化器
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# 模型训练和验证
N_EPOCHS = 10
for epoch in range(N_EPOCHS):
  # Training
  train_loss = 0.0
  model.train()
  for inputs, labels in trainloader:
    inputs = inputs.to(device)
    labels = labels.to(device)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()

  # Validation
  val_loss = 0.0
  model.eval()
  for inputs, labels in valloader:
    inputs = inputs.to(device)
    labels = labels.to(device)

</code></pre>
<h3 id="模型部署"><a class="header" href="#模型部署">模型部署</a></h3>
<p>训练好的模型保存到指定的文件，然后在后面使用时，使用<code>load_state_dict</code>重新加载模型</p>
<pre><code class="language-python">model.train()
# train model
# save model to file
torch.save(model.state_dict(), &quot;./lenet5_model.pt&quot;)

# load model
model = LeNet5().to(device)
model.load_state_dict(torch.load(&quot;./lenet5_model.pt&quot;))
# use model predict
model.eval()
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mini-torch"><a class="header" href="#mini-torch">mini torch</a></h1>
<p>https://minitorch.github.io</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tengine"><a class="header" href="#tengine">Tengine</a></h1>
<p>repo 地址: https://github.com/OAID/Tengine</p>
<p><img src="tengine/./dots/tengine_first_view.dot.svg" alt="" /></p>
<h2 id="函数调用主流程"><a class="header" href="#函数调用主流程">函数调用主流程</a></h2>
<h3 id="register-op"><a class="header" href="#register-op">register op</a></h3>
<p><img src="tengine/./dots/op_registry.dot.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nlp"><a class="header" href="#nlp">NLP</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bert"><a class="header" href="#bert">Bert</a></h1>
<p><img src="nlp/./dots/bert2.svg" alt="" /></p>
<h2 id="how-to-code-bert-笔记"><a class="header" href="#how-to-code-bert-笔记">How to code bert 笔记</a></h2>
<p>https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial
<img src="nlp/./dots/BERT-embeddings-1024x340.png" alt="" /></p>
<h2 id="the-annotated-transformer-笔记"><a class="header" href="#the-annotated-transformer-笔记">The Annotated transformer 笔记</a></h2>
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence</p>
<p>symbol representation -&gt; encode -&gt; continuous representations -&gt; decode -&gt; symbol representation</p>
<p><img src="nlp/./dots/the-annotated-transformer_14_0.png" alt="" /></p>
<h3 id="attention"><a class="header" href="#attention">Attention</a></h3>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key</p>
<p><img src="nlp/./dots/the-annotated-transformer_33_0.png" alt="" /></p>
<p>he two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention</p>
<h2 id="huggingface-transformers"><a class="header" href="#huggingface-transformers">HuggingFace transformers</a></h2>
<h2 id="huggingfacetransformers"><a class="header" href="#huggingfacetransformers">huggingface/transformers</a></h2>
<p><a href="https://github.com/huggingface/transformers/blob/master/README_zh-hans.md">transformers</a></p>
<p><img src="nlp/./dots/bert.svg" alt="" /></p>
<p>Questions: 怎么杨使用已经下载的model?</p>
<h2 id="相关资料"><a class="header" href="#相关资料">相关资料:</a></h2>
<ol>
<li><a href="https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial">how to code bert using pytorch tutorial</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">illustrated transformer</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nncase"><a class="header" href="#nncase">nncase</a></h1>
<p>nncase 是一个为 AI 加速器设计的神经网络编译器。</p>
<ul>
<li><a href="https://github.com/kendryte/nncase">github</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comile流程"><a class="header" href="#comile流程">comile流程</a></h1>
<!-- toc -->
<h2 id="compile过程"><a class="header" href="#compile过程">Compile过程</a></h2>
<p><img src="nncase/./dots/cli_compile.svg" alt="" /></p>
<p><img src="nncase/./dots/target.svg" alt="" /></p>
<p>k210的kernels, 为啥看到的都是c++代码？不是应该直接调driver的方法吗？</p>
<p><img src="nncase/./dots/k210_kernels.svg" alt="" /></p>
<h2 id="kpu"><a class="header" href="#kpu">kpu</a></h2>
<p>riscv-plic-spec:https://github.com/riscv/riscv-plic-spec/blob/master/riscv-plic.adoc</p>
<p>RISC-V Platform-Level Interrupt Controller Specification</p>
<p><img src="nncase/./dots/k210_drivers_kpu.svg" alt="" /></p>
<h2 id="facedetect-example"><a class="header" href="#facedetect-example">facedetect example</a></h2>
<p><img src="nncase/./dots/face_detect_example.svg" alt="" /></p>
<h2 id="kpu-1"><a class="header" href="#kpu-1">kpu</a></h2>
<h3 id="kpu_load_kmodel"><a class="header" href="#kpu_load_kmodel">kpu_load_kmodel</a></h3>
<h3 id="kpu_run_kmodel"><a class="header" href="#kpu_run_kmodel">kpu_run_kmodel</a></h3>
<p>关键函数为<code>ai_step</code>，是pli中断的callback, 会一层层的执行kmodel
看了kpu.c中的代码，貌似只有conv2d是在kpu上跑的，其他算子的都是c++代码
应该是在cpu上跑的。</p>
<p><img src="nncase/./dots/kpu_run_kmodel.svg" alt="" /></p>
<h2 id="dmac"><a class="header" href="#dmac">dmac</a></h2>
<p><img src="nncase/./dots/dmac.svg" alt="" /></p>
<h2 id="plic"><a class="header" href="#plic">plic</a></h2>
<p><a href="https://github.com/riscv/riscv-plic-spec/blob/master/riscv-plic.adoc#introduction">RISC-V Platform-Level Interrupt Controller Specification</a></p>
<p>全局中断，也就是所说的外部中断，其他外设统统都是外部中断。外部中断连接在Platform-Level Interrupt Controller (PLIC)上。</p>
<p>PLIC需要一个仲裁决定谁先中断，存在个优先级的问题。</p>
<p><img src="nncase/./dots/plic.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="k210-standalone-sdk"><a class="header" href="#k210-standalone-sdk">k210 Standalone SDK</a></h1>
<!-- toc -->
<h2 id="相关资料文档"><a class="header" href="#相关资料文档">相关资料文档</a></h2>
<ul>
<li>Github地址：<a href="https://github.com/kendryte/kendryte-standalone-sdk">https://github.com/kendryte/kendryte-standalone-sdk</a></li>
<li>K210 sdk开发文档: <a href="https://s3.cn-north-1.amazonaws.com.cn/dl.kendryte.com/documents/kendryte_datasheet_20181011163248_en.pdf">英文pdf</a>, <a href="https://s3.cn-north-1.amazonaws.com.cn/dl.kendryte.com/documents/kendryte_standalone_programming_guide_20190704110318_zh-Hans.pdf">中文pdf</a></li>
</ul>
<h2 id="platform"><a class="header" href="#platform">platform</a></h2>
<p><img src="nncase/./dots/k210_arch.jpeg" alt="" /></p>
<p>根据platform.h中的地址定义，地址空间布局如下:</p>
<p><img src="nncase/./dots/platform.svg" alt="" /></p>
<h2 id="kpu-2"><a class="header" href="#kpu-2">kpu</a></h2>
<p>AXI BUS是啥</p>
<blockquote>
<p>你可以理解为一种用于传输数据的模块或者总线。用于两个模块或者多个模块之间相互递数据。反正它有一堆优点。。被SOC广泛采用了。</p>
</blockquote>
<p><img src="nncase/./dots/kpu_arch.png" alt="kpu internal" /></p>
<p>riscv-plic-spec:https://github.com/riscv/riscv-plic-spec/blob/master/riscv-plic.adoc</p>
<p>RISC-V Platform-Level Interrupt Controller Specification</p>
<p><img src="nncase/./dots/k210_drivers_kpu.svg" alt="" /></p>
<h3 id="kpu_load_kmodel-1"><a class="header" href="#kpu_load_kmodel-1">kpu_load_kmodel</a></h3>
<h3 id="kpu_run_kmodel-1"><a class="header" href="#kpu_run_kmodel-1">kpu_run_kmodel</a></h3>
<p>关键函数为<code>ai_step</code>，是pli中断的callback, 会一层层的执行kmodel
看了kpu.c中的代码，貌似只有conv2d是在kpu上跑的，其他算子的都是c++代码
应该是在cpu上跑的。</p>
<p><img src="nncase/./dots/kpu_run_kmodel.svg" alt="" /></p>
<h2 id="dmac-1"><a class="header" href="#dmac-1">dmac</a></h2>
<p><img src="nncase/./dots/dmac.svg" alt="" /></p>
<h2 id="plic-1"><a class="header" href="#plic-1">plic</a></h2>
<p><a href="https://github.com/riscv/riscv-plic-spec/blob/master/riscv-plic.adoc#introduction">RISC-V Platform-Level Interrupt Controller Specification</a></p>
<p>全局中断，也就是所说的外部中断，其他外设统统都是外部中断。外部中断连接在Platform-Level Interrupt Controller (PLIC)上。</p>
<p>PLIC需要一个仲裁决定谁先中断，存在个优先级的问题。</p>
<p><img src="nncase/./dots/plic.svg" alt="" /></p>
<h2 id="参考文献"><a class="header" href="#参考文献">参考文献</a></h2>
<ol>
<li><a href="https://s3.cn-north-1.amazonaws.com.cn/dl.kendryte.com/documents/kendryte_standalone_programming_guide_20190704110318_zh-Hans.pdf">k210 SDK 文档</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="face-detect-example"><a class="header" href="#face-detect-example">face detect example</a></h1>
<p><img src="nncase/./dots/face_detect_example.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ray"><a class="header" href="#ray">Ray</a></h1>
<h2 id="相关资料整理"><a class="header" href="#相关资料整理">相关资料整理</a></h2>
<ul>
<li><a href="https://docs.ray.io/en/latest/index.html">官方doc</a></li>
<li><a href="https://github.com/ray-project/ray">github</a></li>
<li><a href="https://arxiv.org/abs/1712.05889">ray paper</a></li>
</ul>
<h2 id="主要组件"><a class="header" href="#主要组件">主要组件</a></h2>
<ul>
<li>Gcs: Global Control State, 存储了代码，输入参数，返回值</li>
<li>Raylet:Local Scheduler, Worker通过Raylet和Gcs通信。</li>
<li>Redis</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ray-paper"><a class="header" href="#ray-paper">Ray Paper</a></h1>
<ul>
<li>论文arxiv 地址 <a href="https://arxiv.org/abs/1712.05889">ray paper</a></li>
</ul>
<p><img src="ray/./dots/ray_draft.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ray-remote"><a class="header" href="#ray-remote">Ray Remote</a></h1>
<p>Actor 生命周期是怎么管理的。</p>
<p>LocalSchedule task保存在哪里？</p>
<p>dynamic task dag是怎么构建的？</p>
<p>Ray和Dask的区别是什么？</p>
<p><img src="ray/./dots/remote.svg" alt="" /></p>
<p>GcsServer</p>
<p><img src="ray/./dots/gcs.svg" alt="" /></p>
<p>GcsServer Star流程</p>
<p><img src="ray/./dots/gcs_start.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow"><a class="header" href="#tensorflow">tensorflow</a></h1>
<p>some notes on reading tensorflow source code</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tensorflow-graph-executor草稿"><a class="header" href="#tensorflow-graph-executor草稿">Tensorflow Graph Executor(草稿)</a></h2>
<h2 id="摘要"><a class="header" href="#摘要">摘要</a></h2>
<p>Tensorflow中单机版的(direct session)会按照device将graph先划分成子图subgraph, 然后每个subgraph会交给一个execturo去执行，分布式的（GrpSession) 首先会将graph按照worker划分，每个worker划分成一个子图，然后注册到每个worker的graph_mgr, 并在graph_mgr中再按照device将worker_subgraph划分成device的subgraph, 最后每个device对应的subgraph会由executor去执行，Tensorflow中的graph执行示意图如下(图片来自<a href="https://wookayin.github.io/tensorflow-talk-debugging/#1">tensorflow-talk-debugging</a>)。</p>
<p><img src="tensorflow/./images/tensors_flowing.gif" alt="tensors_flowing" /></p>
<p>本文主要分析了executor在执行graph时，Node的执行调度以及node的输入输出数据, 执行状态是如何保存的，最后结合代码和<a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">Tensorflow control flow implemention</a>这部分文档分析了的control flow的具体实现。主要涉及的代码为common_runtime/executor.cc</p>
<h3 id="executor中主要类"><a class="header" href="#executor中主要类">Executor中主要类</a></h3>
<h4 id="executor"><a class="header" href="#executor">Executor</a></h4>
<p>Executor为基类，对外提供了两个接口Run和RunAsync, 其中Run是对RunAsync简单的一层包装。</p>
<pre><code class="language-cpp">
  // Synchronous wrapper for RunAsync().
  Status Run(const Args&amp; args) {
    Status ret;
    Notification n;
    RunAsync(args, [&amp;ret, &amp;n](const Status&amp; s) {
      ret = s;
      n.Notify();
    });
    n.WaitForNotification();
    return ret;
  }
</code></pre>
<p>Executor基类只要去实现RunAsync就行。</p>
<pre><code class="language-cpp">  virtual void RunAsync(const Args&amp; args, DoneCallback done) = 0;
</code></pre>
<h4 id="executorimpl"><a class="header" href="#executorimpl">ExecutorImpl</a></h4>
<p>ExecutorImpl继承实现了Executor，它的RunAsync实现转发给了ExecutorState::RunAsync, ExecutorImpl主要的工作是从Graph中解析出一些静态信息，比如FrameInfo, GraphView, 由后面的ExecutorState执行的时候使用。</p>
<pre><code class="language-cpp">void ExecutorImpl::RunAsync(const Args&amp; args, DoneCallback done) {
  (new ExecutorState(args, this))-&gt;RunAsync(std::move(done));
}
</code></pre>
<h4 id="executorstate"><a class="header" href="#executorstate">ExecutorState</a></h4>
<h3 id="executor中的调用关系"><a class="header" href="#executor中的调用关系">Executor中的调用关系</a></h3>
<h3 id="executorimpl-call-flow"><a class="header" href="#executorimpl-call-flow">ExecutorImpl call flow</a></h3>
<p>Executor被调用的入口为NewLocalExecutor, 在DirectSesion中会为每个subgraph创建一个executor, 然后交给ExecutorBarrier同时执行多个Executor。NewLocalExecutor在ExecutorImpl成员函数中的调用过程如下：</p>
<p><img src="tensorflow/./images/executor_impl_call.jpeg" alt="executor impl call flow" /></p>
<p>Exector::RunAsync这个会被转发给ExecutorState::RunAsync（这个函数的执行逻辑见下文）</p>
<h3 id="executorimplinitialize"><a class="header" href="#executorimplinitialize">ExecutorImpl::Initialize</a></h3>
<p>在ExecutorImpl::Initialize中，对于graph中的每个node, 创建对应的NodeItem, 主要包含了三块：</p>
<ol>
<li>调用params.create_kernal, 创建nodeItem-&gt;kernal.</li>
<li>记录nodeItem.input_start, input_start 是该node在它所属frame的input_tensors中的偏移index, 这个在后面的ProcessInputs和ProcessOutputs中会用到。</li>
<li>创建node对应的pending_id， pending_id用于找到记录它执行状态的pendingCount, 这个在后面的ActiveNode中会用到.</li>
</ol>
<p>在BuildCtronlFlow中会建立好framename之间的父子关系, frameInfo是frame的静态信息（对应着执行时候的FrameState动态信息），并且建立了从node id找到node所属frame name的映射关系，包含了frame中的total inputs, 这个frame所包含的node.</p>
<p><img src="tensorflow/./images/tf-executor-init.jpeg" alt="image" /></p>
<h2 id="executorstaterunasync"><a class="header" href="#executorstaterunasync">ExecutorState::RunAsync</a></h2>
<p><img src="tensorflow/./images/tf-executor-call-flow.jpeg" alt="image" /></p>
<h3 id="executorimplprocess"><a class="header" href="#executorimplprocess">ExecutorImpl::Process</a></h3>
<p><img src="tensorflow/./images/tf-executor-data-flow.jpeg" alt="image" /></p>
<h2 id="control-flow"><a class="header" href="#control-flow">Control Flow</a></h2>
<p>后来在[1]中发现节点还有Switch, Merge, IterNext, Enter, Exit 五个flow control node，用来实现while循环，为此tensorflowe引入了frame的概念，可以粗略的认为和函数调用一样吧, 在遇到Enter node的时候，就新建一个child frame，把inputs(类似于函数函数调用时候参数入栈)一样，forward到child frame中，在遇到Exit node，就把输出放到parent frame 中(类似于将函数的return值入栈)。</p>
<p>未完待续</p>
<h2 id="executor中数据流程"><a class="header" href="#executor中数据流程">Executor中数据流程</a></h2>
<h2 id="参考"><a class="header" href="#参考">参考</a></h2>
<ol>
<li>[Tensorflow control flow implemention]</li>
<li><a href="https://wookayin.github.io/tensorflow-talk-debugging/#1">tensorflow-talk-debugging</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="sub-graph-预处理-node--nodeitem--taggednode-draft"><a class="header" href="#sub-graph-预处理-node--nodeitem--taggednode-draft">Sub Graph 预处理: Node =&gt; NodeItem =&gt; TaggedNode (Draft)</a></h2>
<h3 id="引言"><a class="header" href="#引言">引言</a></h3>
<p>下图是一个graph中每个node被处理的过程，首先在ExecutorImpl::Initialize的时候，将node 处理成NodeItem，创建node对应的kernal, 然后在node ready可执行的时候，会创建一个TaggedNode(TaggedNode主要多了个frame指针，记录了当前执行的frame), 并将它放入Ready队列中，最后交给ExecutorState::Process去执行这个Node。
<img src="tensorflow/./images/node_process_flow.jpeg" alt="node process flow" /></p>
<h3 id="nodeitem"><a class="header" href="#nodeitem">NodeItem</a></h3>
<p>NodeItem的主要作用是将Graph中每个node的op，转换成可以在device上执行的kernal, 另一方面，记录该node输入tensor的位置，并且使用PendingCount来记录Node的执行状态。 Gview可以看成是NodeItem的容器，根据node的id就可以找到相应的NodeItem, 对于graph中的每个node, 在ExecutorImpl::Initialize中都会创建一个NodeItem，放到Gview中。</p>
<h4 id="nodeitem-主要包含的字段"><a class="header" href="#nodeitem-主要包含的字段">NodeItem 主要包含的字段</a></h4>
<ol>
<li><b>kernel</b>:  由params.create_kernel创建，kernel是在device上执行的主要对象，kerenl 并将在ExecutorImpl的析构函数被params.delete_kernel删除。</li>
</ol>
<pre><code class="language-cpp">  // The kernel for this node.
  OpKernel* kernel = nullptr;
</code></pre>
<ol start="2">
<li><b>input_start</b>：纪录了在当前IteratorState的input_tensors中开始的index。这个node的输入为：input_tensors[input_start: input_start + num_inputs]这部分对应的Tensors。</li>
</ol>
<pre><code class="language-cpp">  // Cached values of node-&gt;num_inputs() and node-&gt;num_outputs(), to
  // avoid levels of indirection.
  int num_inputs;
  int num_outputs;

  // ExecutorImpl::tensors_[input_start] is the 1st positional input
  // for this node.
  int input_start = 0;

  // Number of output edges.
  size_t num_output_edges;

</code></pre>
<ol start="3">
<li><b>pending_id</b>: 根据这个id在当前的IteratorState中找到对应的PendingCount，从而找到这个nodeItem的执行状态。</li>
</ol>
<pre><code class="language-cpp">  PendingCounts::Handle pending_id;
</code></pre>
<ol start="4">
<li><b>expensive/async kernel</b>: 标志表明kernel是否是Async的和expensive的。</li>
</ol>
<pre><code>  bool kernel_is_expensive : 1;  // True iff kernel-&gt;IsExpensive()
  bool kernel_is_async : 1;      // True iff kernel-&gt;AsAsync() != nullptr
</code></pre>
<ol start="5">
<li><b>control node</b> ，标志该node是否是Control flow node, 以及类型</li>
</ol>
<pre><code class="language-cpp">
  bool is_merge : 1;             // True iff IsMerge(node)
  bool is_enter : 1;             // True iff IsEnter(node)
  bool is_exit : 1;              // True iff IsExit(node)
  bool is_control_trigger : 1;   // True iff IsControlTrigger(node)
  bool is_sink : 1;              // True iff IsSink(node)
  // True iff IsEnter(node) || IsExit(node) || IsNextIteration(node)
  bool is_enter_exit_or_next_iter : 1;
</code></pre>
<ol start="6">
<li><b>allocate attribute</b>: 影响device所返回的allocator,从而影响kernal执行时候，申请内存时候的处理行为。</li>
</ol>
<pre><code class="language-cpp">  // Return array of per-output allocator attributes.
  const AllocatorAttributes* output_attrs() const { return output_attr_base(); }
</code></pre>
<p>InferAllocAttr主要根据device, send, recv等节点, 来设置是否是gpu_compatible的，</p>
<pre><code class="language-cpp">      attr-&gt;set_nic_compatible(true);
      attr-&gt;set_gpu_compatible(true);
</code></pre>
<p>其中AllocatorAttributes主要影响GpuDevice所返回的allocator上。</p>
<pre><code class="language-cpp">//common_runtime/gpu/gpu_device_factory.cc

  Allocator* GetAllocator(AllocatorAttributes attr) override {
    if (attr.on_host()) {
      if (attr.gpu_compatible() || force_gpu_compatible_) {
        ProcessState* ps = ProcessState::singleton();
        return ps-&gt;GetCUDAHostAllocator(0);
      } else {
        return cpu_allocator_;
      }
    } else {
      return gpu_allocator_;
    }
}
</code></pre>
<h3 id="taggednode"><a class="header" href="#taggednode">TaggedNode</a></h3>
<p>TaggedNode 增加了了一个FrameState指针，指向了Node将要执行的FrameState, input_iter, input_frame加上input_iter可以确定了</p>
<pre><code class="language-cpp">  struct TaggedNode {
    const Node* node = nullptr;
    FrameState* input_frame = nullptr;
    int64 input_iter = -1;
    bool is_dead = false;

    TaggedNode(const Node* t_node, FrameState* in_frame, int64 in_iter,
               bool dead) {
      node = t_node;
      input_frame = in_frame;
      input_iter = in_iter;
      is_dead = dead;
    }
</code></pre>
<p>在node处于ready 可执行状态的时候，会创建一个TaggedNode, 并放到TaggedNodeSeq队列中，等待调度执行。</p>
<pre><code>ExecutorState::FrameState::ActivateNodes ==&gt;
    ready-&gt;push_back(TaggedNode(dst_item-&gt;node, this, iter, dst_dead));

ExecutorState::RunAsync ==&gt;
    for (const Node* n : impl_-&gt;root_nodes_) {
      DCHECK_EQ(n-&gt;in_edges().size(), 0);
      ready.push_back(TaggedNode{n, root_frame_, 0, false});
    }
</code></pre>
<h3 id="获取node输入tensors指针"><a class="header" href="#获取node输入tensors指针">获取node输入tensors指针</a></h3>
<p>首先根据TaggedNode中的input_frame，input_iter获取node的输入tensors</p>
<pre><code>  Entry* GetInputTensors(FrameState* input_frame,
                         int64 input_iter) const NO_THREAD_SAFETY_ANALYSIS {
    return input_frame-&gt;GetIteration(input_iter)-&gt;input_tensors;
  }
</code></pre>
<p>然后根据NodeItem中定义的input_start获取first_input tensor的指针</p>
<pre><code>//在ExecutorState::Process中:
    Entry* input_tensors = GetInputTensors(input_frame, input_iter);
    Entry* first_input = input_tensors + item.input_start;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="flow-control-op"><a class="header" href="#flow-control-op">Flow Control op</a></h2>
<p>在Tensorflow中，graph中每个node的op，都在一个execution Frame中执行，Enter/Exit分别负责execution Frame的创建和删除，如果把execution frame和函数调用做类比的话，那么Enter有点类似于传参，而Exit则类似于return 返回值。
而switch/merge/nextIteration 则用于实现类似于while/if之类的分支跳转和循环。本节主要参照 <a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">1</a> 这篇文章。</p>
<h3 id="flow-control-op-1"><a class="header" href="#flow-control-op-1">flow control op</a></h3>
<p><img src="tensorflow/./images/control-flow-op.jpeg" alt="control flow op" /></p>
<p>Tensorflow中control flow op对应具体定义如下</p>
<h4 id="switch"><a class="header" href="#switch">switch</a></h4>
<blockquote>
<p>A Switch operator forwards the input tensor d to one of its outputs depending on the boolean tensor of the control input p. A Switch is enabled for execution when both its inputs are available.</p>
</blockquote>
<p>Switch 根据predict将输入tensor导出到相应的true/false输出。没获得输出的分支会被标记为dead状态(有点类似于if/else中没被执行到的代码), 这个dead状态会往下传播。</p>
<h4 id="merge"><a class="header" href="#merge">Merge</a></h4>
<blockquote>
<p>A Merge operator forwards one of its available inputs to its output. A Merge is enabled for execution when any of its inputs is available. It is unspecified which available input it outputs if there are multiple inputs available.</p>
</blockquote>
<p>Merge 将输入tensor中的一个导出到输出（先到先得）,一般配合switch用</p>
<h4 id="enter"><a class="header" href="#enter">Enter</a></h4>
<blockquote>
<p>An Enter operator forwards its input to the execution frame that is uniquely identified by the given name. This Enter op is used to pass a tensor in one execution frame to a child execution frame. There can be multiple Enter ops to the same child execution frame, each making a tensor available (asynchronously) in that child execution frame. An Enter is enabled for execution when its input is available. A new execution frame is instantiated in the TensorFlow runtime when the first Enter op to that frame is executed </p>
</blockquote>
<p>Enter node将输入tensor导入到一个frame中。frame name是唯一的，可以根据frame name来找到对应的frame， 在执行的时候，如果frame不存在的话，Enter会创建相应的子frame, Enter node所在的frame是该frame的parent frame.</p>
<h4 id="exit"><a class="header" href="#exit">Exit</a></h4>
<blockquote>
<p>An Exit operator forwards a value from an execution frame to its parent execution frame.  This Exit op is used to return a tensor computed in a child execution frame back to its parent frame. There can be multiple Exit ops to the parent frame, each asynchronously passing a tensor back to the parent frame. An Exit is enabled when its input is available.</p>
</blockquote>
<p>Exit node 从Frame中导出一个tensor到parent frame中。</p>
<h4 id="nextiteration"><a class="header" href="#nextiteration">NextIteration</a></h4>
<blockquote>
<p>A NextIteration operator forwards its input to the next iteration in the current execution frame. The TensorFlow runtime keeps track of iterations in an execution frame. Any op executed in an execution frame has a unique iteration id, which allows us to uniquely identify different invocations of the same op in an iterative computation. Note that there can be multiple NextIteration ops in an execution frame. The TensorFlow runtime starts iteration N+1 when the first NextIteration op is executed at iteration N. As more tensors enter an iteration by executing NextIteration ops, more ops in that iteration will be ready for execution. A NextIteration is enabled when its input is available.</p>
</blockquote>
<p>NextIteration将输入导出到下个iteration, NextIteration导出的应该是循环变量，比如下面代码中的j和sum</p>
<pre><code class="language-cpp">for(int j=0, sum=0; j &lt; 100;){
    int tmp = i + 1;
    j * = 2
    sum += j;
}
</code></pre>
<h3 id="while-loop"><a class="header" href="#while-loop">While loop</a></h3>
<p>可以通过上述的五个flow control node来实现tensorflow中的while loop</p>
<pre><code class="language-python">tf.while_loop(lambda i: i &lt; 10, lambda i: tf.add(i, 1), [0])
</code></pre>
<p><img src="tensorflow/./images/while_loop.jpeg" alt="while loop" /></p>
<p>可以看到NextIteration导入导出的是循环变量i，merge node可以用来初始化变量, 类似于 <code>i= i || 0</code>的效果, switch控制是否结束循环，Exit跳出循环。</p>
<p>在文献<a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">1</a>中还讲述了dead传播，分布式的whileloop，以及while loop对应的gradient op.讲的比较深，后面再补上吧。</p>
<h3 id="参考文献-1"><a class="header" href="#参考文献-1">参考文献：</a></h3>
<ol>
<li><a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">Tensorflow control flow implemention</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executor-frame"><a class="header" href="#executor-frame">Executor Frame</a></h1>
<h2 id="引言-1"><a class="header" href="#引言-1">引言</a></h2>
<p>在Executor 执行Graph的时候，会首先分析Graph, 创建关于Graph中frame的静态信息，比如ControlFlowInfo和FrameInfo，对于graph中的每个node, 可以根据ControlFlowInfo去得到它对应的frame_name, 然后根据frame_name可以得到FrameInfo的一些信息。</p>
<p>而FrameState和IterationState这两个是动态的状态，由Executor在执行Graph时候动态创建的。FrameState对应着整个while loop，而IterationState则对应着while loop中的某个迭代。 FrameState中包了total_input(frame中所有node input个数等信息），IterationState中有个EntryVec用于保存某次迭代时候，node之间输入输出的Entry。</p>
<p>本文主要分析了Executor中ControlFlowInfo， FrameInfo, FrameState, IterationState，这几个和Executor Frame相关的struct， 以及它们之间的关系。</p>
<h2 id="executorimplcontrolflowinfo"><a class="header" href="#executorimplcontrolflowinfo">ExecutorImpl::ControlFlowInfo</a></h2>
<p>ControlFlowInfo里面<code>unique_frame_names</code>保存了computation graph中所有frame的名字，frame_names则是个倒查表，索引对应于<code>node-&gt;id</code>, 可以根据<code>frame_names[node-&gt;id()]</code>找到node对应的frame_name.</p>
<pre><code class="language-cpp">struct ControlFlowInfo {
  gtl::FlatSet&lt;string&gt; unique_frame_names;
  std::vector&lt;string&gt; frame_names;
};
</code></pre>
<h3 id="controlflowinfo的创建"><a class="header" href="#controlflowinfo的创建">ControlFlowInfo的创建</a></h3>
<p>BuildControlFlowInfo 会遍历整个graph, 然后处理Enter/Exit node, 填充好ControlFlowInfo中的字段, </p>
<ol>
<li>如果遇到Enter node, 则进入子Frame, Enter node的每个输出node对应的frame_name都是EnterNode对应的 &quot;frame_node&quot;属性</li>
</ol>
<pre><code class="language-cpp">//Enter node包含了frame_name 属性，
GetNodeAttr(curr_node-&gt;attrs(), &quot;frame_name&quot;, &amp;frame_name));
</code></pre>
<ol start="2">
<li>如果是Exit node, 则退出子Frame, Exit node的每个输出node对应的frame_name都是Exit node parent node的 frame_name</li>
</ol>
<pre><code class="language-cpp">//other code
else if (IsExit(curr_node)) {
    parent = parent_nodes[curr_id];
    frame_name = cf_info-&gt;frame_names[parent-&gt;id()];
    parent = parent_nodes[parent-&gt;id()];
}
</code></pre>
<ol start="3">
<li>如果是其他类型的node, 则node的每个输出node frame和当前node一致</li>
</ol>
<pre><code class="language-cpp"> parent = parent_nodes[curr_id];
 frame_name = cf_info-&gt;frame_names[curr_id];
</code></pre>
<h4 id="controlflow-info被用到的地方"><a class="header" href="#controlflow-info被用到的地方">controlflow info被用到的地方</a></h4>
<p>在executor中首先会根据node-&gt;id找到frame_name, 然后根据frame_name找到对应的FrameInfo</p>
<pre><code class="language-cpp">    const string&amp; frame_name = cf_info.frame_names[id];
    FrameInfo* frame_info = EnsureFrameInfo(frame_name);
</code></pre>
<h2 id="executorimplframeinfo"><a class="header" href="#executorimplframeinfo">ExecutorImpl::FrameInfo</a></h2>
<p>FrameInfo包含的主要字段如下:</p>
<pre><code class="language-cpp">    // The total number of inputs to a frame.
    int input_count;

    int total_inputs;

    PendingCounts::Layout pending_counts_layout;
    PendingCounts* pending_counts;  // Owned
</code></pre>
<h3 id="input_count"><a class="header" href="#input_count">input_count</a></h3>
<p>input_count 代表graph中Enter到该frame的Enter Node个数, 统计个数的代码如下：</p>
<pre><code class="language-cpp">//ExecutorImpl::Initialize
  for (const Node* n : graph_-&gt;nodes()) {
    //other code..

    if (IsEnter(n)) {
      string enter_name;
      TF_RETURN_IF_ERROR(GetNodeAttr(n-&gt;attrs(), &quot;frame_name&quot;, &amp;enter_name));
      EnsureFrameInfo(enter_name)-&gt;input_count++;
    }
  }
</code></pre>
<h3 id="total_inputs"><a class="header" href="#total_inputs">total_inputs</a></h3>
<p>total_inputs会在ExecutorState::IteratorState中用到，它的值为frame中所有node的inputs个数的总和。</p>
<pre><code class="language-cpp">// The total number of input tensors of a frame.
// == sum(nodes[*].num_inputs()) where nodes are the nodes in the frame.
int total_inputs;
</code></pre>
<p>total_inputs在后面的影响如下：</p>
<pre><code>FrameInfo.total_inputs ==&gt; FrameState.total_input_tensors ==&gt; IterationsState.input_tensors(new Entry[total_input_tensors])
</code></pre>
<h3 id="pendingcounts"><a class="header" href="#pendingcounts">PendingCounts</a></h3>
<ol start="3">
<li>PendingCounts相关，pending_counts_layout在后面会用来创建Node的PendingCount, pending count会用来跟踪Node的状态（比如是否所有的input都已ready, Node是否已经执行过了，Node是否在Dead path)，</li>
</ol>
<p>struct FrameInfo由EnsureFrameInfo这个函数lazy创建，并在Intialize填充好它的字段。</p>
<pre><code class="language-cpp">  FrameInfo* EnsureFrameInfo(const string&amp; fname) {
    auto slot = &amp;frame_info_[fname];
    if (*slot == nullptr) {
      *slot = new FrameInfo;
    }
    return *slot;
  }
</code></pre>
<p>FrameInfo将在ExecutorImpl的析构函数中被删掉。</p>
<pre><code class="language-cpp">  ~ExecutorImpl() override {
    //other code
    for (auto fiter : frame_info_) {
      delete fiter.second;
    }
</code></pre>
<h2 id="executorstateframestate"><a class="header" href="#executorstateframestate">ExecutorState::FrameState</a></h2>
<p>前面两个ControlFlowInfo/FrameInfo都是静态的信息(所以叫XXXInfo)，而FrameState和IterationState都是动态信息，会在Graph执行的时候动态创建。</p>
<h4 id="创建framestate-findorcreatechildframe"><a class="header" href="#创建framestate-findorcreatechildframe">创建FrameState: FindOrCreateChildFrame</a></h4>
<p>在FindOrCreateChildFrame中，会调用InitializeFrameInfo从FrameInfo中抽取有用的字段</p>
<pre><code class="language-cpp">    void InitializeFrameInfo(const string&amp; enter_name) {
      auto it_frame_info = executor-&gt;frame_info_.find(enter_name);
      DCHECK(it_frame_info != executor-&gt;frame_info_.end());
      ExecutorImpl::FrameInfo* finfo = it_frame_info-&gt;second;
      pending_counts = finfo-&gt;pending_counts;
      total_input_tensors = finfo-&gt;total_inputs;
      num_pending_inputs = finfo-&gt;input_count;
      nodes = finfo-&gt;nodes;
    }
</code></pre>
<p>FindOrCreateChildFrame被调用的stack</p>
<pre><code>Process -&gt; PropagationOutputs -&gt; FindOrCreateChildFrame
</code></pre>
<h3 id="删除framestate-deleteframe"><a class="header" href="#删除framestate-deleteframe">删除FrameState: DeleteFrame</a></h3>
<p>1.在PropgateOutputs中，如果is_frame_done，就会调用DeleteFrame, DeleteFrame会向parent frame传播dead_exits（TODO: 这部分描述细化）</p>
<p>IterationState删除的地方</p>
<ol>
<li>CleanupFrameIterations</li>
<li>frame-&gt;CleanupIterations</li>
</ol>
<h2 id="executorstateiterationstate"><a class="header" href="#executorstateiterationstate">ExecutorState::IterationState</a></h2>
<pre><code class="language-cpp">    Entry* input_tensors;
    // The number of outstanding ops for each iteration.
    size_t outstanding_ops;
    int outstanding_frame_count;
    PendingCounts counts_;
</code></pre>
<h6 id="framestate和iterationstate创建地方"><a class="header" href="#framestate和iterationstate创建地方">FrameState和IterationState创建地方:</a></h6>
<ol>
<li>
<p>在ExecutorState的构造函数中会创建一个FrameState作为rootframe, 同时也会创建该frameState的第一个IterationState。</p>
</li>
<li>
<p>在执行完一个Node之后，PropagateOutputs在遇到Enter节点的时候，会调用FindOrCreateChildFrame来创建一个新的FrameState,以及该FrameState的第一个IterationState</p>
</li>
<li>
<p>在PropgateOutputs的时候，遇到NextIteration Node 会去调用FrameState::IncreatementIteration新增一个IterationState</p>
</li>
<li>
<p>所有的framesate都放在了outstanding_frames 这个map中，新建的framestate会插到这个map中，删除的时候会从这个map中去掉。</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tensorflow-direct-session-draft"><a class="header" href="#tensorflow-direct-session-draft">Tensorflow Direct Session (Draft)</a></h2>
<h3 id="摘要-1"><a class="header" href="#摘要-1">摘要</a></h3>
<p>本文主要分析了tensorflow 中DirectSession部分的代码。如果把executor 执行graph当成一个函数的话，那么Tensorflow中Session主要功能是把用户传过来的一些参数Feeds到compute graph中，然后运行到graph target node，最后在graph computation完成之后，取出用户指定名字的一些tensor。</p>
<p>DirectSession 则主要工作以下几方面：</p>
<ol>
<li>Rewrite Graph： 将FeedInputs和FetchOutputs节点加到graph中，然后去掉graph中运行不到的节点，最后采用并查集的方式，给graph中每个node分配一个device。</li>
<li>Graph partition：根据每个node所device，将node划分成不同的subgraph, subgraph之间添加send和recv节点做不同device之间的通信。</li>
<li>CeateExecutors：每个device的subgraph会创建一个Executor来执行graph computation。</li>
<li>Fetch outputs：对于DirectSession来说，FeedInputs和FetchOutputs 所添加的节点是<code>_Arg</code>和<code>_RetVal</code>，这两个节点会通过directSession的callframe来读写input，output。</li>
</ol>
<h3 id="rewritegraph"><a class="header" href="#rewritegraph">RewriteGraph</a></h3>
<p>RewriteGraph这块的callstack如下图所示，主要主要涉及到 GraphExecutionState, SubGraph, Placer这三块。</p>
<p>GraphExecutionState据文档所说(graph_execution_state.h)，其主要作用是按照BuildGraphOptions选项将Graph转换成可执行的Client Graph。</p>
<blockquote>
<p>GraphExecutionState is responsible for generating an
executable ClientGraph from the original GraphDef that specifies
the complete graph and from BuildGraphOptions which specifies
input/output nodes.</p>
</blockquote>
<p>ClientGraph与GraphDef的区别是： ClientGraph中每个node都被Assign了某个Device，这部分由Placer完成；另外添加了input/output nodes, 去掉了执行不到的node, 这部分由subgraph完成。</p>
<blockquote>
<p>An executable Graph differs from a GraphDef by being Placed,
meaning that each Node is assigned to a single Device in the
available set.</p>
</blockquote>
<p><img src="tensorflow/./images/direct_session_rewrite_graph.jpeg" alt="rewrite graph" /></p>
<h3 id="call-frame-feed-and-fetch"><a class="header" href="#call-frame-feed-and-fetch">Call frame: feed and fetch</a></h3>
<p>DirectSession中采用了call frame的方式读写compution graph中的inputs/outputs </p>
<p><img src="tensorflow/./images/direct_session_call_frame.jpeg" alt="direct session call frame" /></p>
<p>DirectSession::Run的时候，首先会创建一个FunctionCallFrame, 把要feed的tensor填充到<code>FunctionCallFrame::args_</code>。</p>
<pre><code class="language-cpp">// In DirectSession::Run

  FunctionCallFrame call_frame(executors_and_keys-&gt;input_types,
                               executors_and_keys-&gt;output_types);
  gtl::InlinedVector&lt;Tensor, 4&gt; feed_args(inputs.size());
  for (const auto&amp; it : inputs) {
    if (it.second.dtype() == DT_RESOURCE) {
      Tensor tensor_from_handle;
      TF_RETURN_IF_ERROR(
          ResourceHandleToInputTensor(it.second, &amp;tensor_from_handle));
      feed_args[executors_and_keys-&gt;input_name_to_index[it.first]] =
          tensor_from_handle;
    } else {
      feed_args[executors_and_keys-&gt;input_name_to_index[it.first]] = it.second;
    }
  }
  const Status s = call_frame.SetArgs(feed_args);
</code></pre>
<p>在创建Executor的时候，通过Executor::Args.call_frame把call_frame放到OpkernalContext中。</p>
<pre><code class="language-cpp">//## DirectSessioin::Runinternal

  Executor::Args args;
  args.step_id = step_id;
  args.call_frame = call_frame;

  //other code...
  //每个device subgraph对应一个item, item.executor为这个subgraph的exeuctor.
  item.executor-&gt;RunAsync(args, barrier-&gt;Get());


//## ExecutorState::Process
  OpKernelContext::Params params;
  params.step_id = step_id_;
  params.call_frame = call_frame_;

  //other code ...
  // Synchronous computes.
  OpKernelContext ctx(&amp;params, item.num_outputs);
  nodestats::SetOpStart(stats);
  device-&gt;Compute(CHECK_NOTNULL(op_kernel), &amp;ctx);

</code></pre>
<p>当所有的subgraph Executor执行完毕后，通过FunctionCallFrame::ConsumeRetVals的方式把输出的tensor取出来。</p>
<pre><code class="language-cpp">// DirectSession::Run

  if (outputs) {
    std::vector&lt;Tensor&gt; sorted_outputs;
    const Status s = call_frame.ConsumeRetvals(&amp;sorted_outputs);
    if (errors::IsInternal(s)) {
 //other code
</code></pre>
<h3 id="device-placer"><a class="header" href="#device-placer">Device placer</a></h3>
<p>Placer 在初始的时候，用户会指定某些节点的device, 比如有的节点是gpu:0, 有的cpu:0, 有的node是gpu:1, 然后将有相同<code>class_</code>属性<code>@loc:xxx</code>的node节点放到一个集合里面，随后根据以下约束, 采用并查集的方式，对node集合进行进一步的划分:</p>
<ol>
<li>用户指定了device，就将node放到用户指定的device上</li>
<li>Generateo node 和output node放到同一个device上</li>
<li>Meta node（比如cast操作) 和input node放到同一个device上</li>
<li>Reftype 的Input, input和output节点尽量放到同一个device上</li>
<li>采用并查集的方式将node place给device</li>
<li>对于stateful的node, 不改变它的device assign。</li>
</ol>
<p>stateful node 在placed之后，就不能移到别的device上了, 对于这种node，GraphExecutionState的做法是在placer run之前将stateful node的device assign保存以下，在placer run 之后再恢复回去。</p>
<blockquote>
<p>Map of placed stateful nodes, i.e. nodes for which is_stateful()
is true, such as &quot;params&quot; and &quot;queue&quot; nodes.  Once placed these
nodes can not be moved to a different device.  Maps node names to
device names.</p>
</blockquote>
<p>可以通过打开<code>log_device_placement</code>的方式让placer在stderr中把node的device place情况打出来:</p>
<pre><code class="language-py">config=tf.ConfigProto(log_device_placement=True)
sess = tf.Session(config=config)
</code></pre>
<h3 id="graph-partition"><a class="header" href="#graph-partition">Graph partition</a></h3>
<p>Graph partition根据上面Placemnet的结果，将graph partition成不同的子图，子图之间添加send 和recv节点，send和recv节点会用rendzvous来传送tensor。有时候除了send和recv node还需要添加一些control flow node。</p>
<p><img src="tensorflow/./images/distributed_graph.png" alt="graph partition" /></p>
<p>（这个地方需要看下tf implement那个文档，了解下具体情况）</p>
<h3 id="executor-cache"><a class="header" href="#executor-cache">Executor Cache</a></h3>
<p>提交给DirectSessoin在经过Graph Partition之后，会划分成不同的子图，比如下图将一个大的graph划分成了3个subgraph分别放置在了在CPU, GPU1, GPU2上，device之间通过rendezvous来通信，每个subgraph都会创建一个executor去执行。</p>
<p><img src="tensorflow/./images/direct_session_create_executors.png" alt="Graph Executors" /></p>
<p>在模型的训练通常会多次迭代run, 因此要加一层cache避免多次做graph的parition，多次创建executor。</p>
<pre><code class="language-py">with tf.Session(config=config) as sess:
    sess.run([merge, gd_step], feed_dict={x: batch_xs, y_label: batch_ys})
</code></pre>
<p>cache的key为input, output，target tensor的names 连起来的。还有一个key是吧input, output, target的names分别sort之后再连起来。</p>
<p>DirectSession::Run中cache的key很有意思，有两个key,  首先去是未排序的，另外一个是排序的。未排序的为了快速查找，而排序的key是为了避免由于input_names中names顺序不一样导致cache miss。</p>
<pre><code>  // Fast lookup path, no sorting.
  // Fast查询的key, 没排序
  const string key = strings::StrCat(
      str_util::Join(inputs, &quot;,&quot;), &quot;-&gt;&quot;, str_util::Join(outputs, &quot;,&quot;), &quot;/&quot;,
      str_util::Join(target_nodes, &quot;,&quot;), &quot;/&quot;, run_state_args-&gt;is_partial_run,
      &quot;/&quot;, debug_tensor_watches_summary);


 // 将names分别排序然后concat起来.
  std::vector&lt;string&gt; inputs_sorted(inputs.begin(), inputs.end());
  std::sort(inputs_sorted.begin(), inputs_sorted.end());
  std::vector&lt;string&gt; outputs_sorted(outputs.begin(), outputs.end());
  std::sort(outputs_sorted.begin(), outputs_sorted.end());
  std::vector&lt;string&gt; tn_sorted(target_nodes.begin(), target_nodes.end());
  std::sort(tn_sorted.begin(), tn_sorted.end());

  const string sorted_key = strings::StrCat(
      str_util::Join(inputs_sorted, &quot;,&quot;), &quot;-&gt;&quot;,
      str_util::Join(outputs_sorted, &quot;,&quot;), &quot;/&quot;, str_util::Join(tn_sorted, &quot;,&quot;),
      &quot;/&quot;, run_state_args-&gt;is_partial_run, &quot;/&quot;, debug_tensor_watches_summary);
  // Set the handle, if its needed to log memory or for partial run.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tensorflow-rendezvous"><a class="header" href="#tensorflow-rendezvous">Tensorflow Rendezvous</a></h2>
<h3 id="摘要-2"><a class="header" href="#摘要-2">摘要</a></h3>
<p>Rendezvous负责在Send和Recv node之间传递tensor, tensor的传递可能会跨设备(cross device), 也可能跨主机(GRPC，MPI，Rdam）等。如何提供统一简洁的接口，并同时实现不同场景下tensor高效传递是关键，Rendezvous功能上主要涉及以下两点：</p>
<ol>
<li>Send操作不会被block，而Recv操作可能会block，一直等到有tensor，才会返回或者调用异步的callback。</li>
<li>由于send 和recv node可能在同一个worker的不同device上，也有可能在不同worker的不同device上，所以Rendezvous又分为LocalRendezvous, IntraProcessRendezvous, RemoteRendezvous 以对应不同的场景。</li>
</ol>
<h3 id="rendezvous"><a class="header" href="#rendezvous">Rendezvous</a></h3>
<h4 id="继承关系"><a class="header" href="#继承关系">继承关系</a></h4>
<p>Rendezvous中各个层级实现功能如下：</p>
<ul>
<li>LocalRendezvor实现了核心Send和Recv操作，每个key对应了一个queue, send根据key放到相应的队列里，recv根据key去对应的队列取。</li>
<li>IntraProcessRendezvou使用CopyTensor::ViaDMA处理了不同device间的copy问题，其send, recv还是会交由LocalRendezvous去做。 </li>
<li>RpcProcessRendezvous实现了将woker的本地tensor(tensor如果在GPU上的话，需要先从GPu上copy到内存中）通过grpc buffer传递给调用者。</li>
</ul>
<p><img src="tensorflow/./images/rendezvous_inherit.jpeg" alt="rendezvous inherit" /></p>
<h4 id="localrendezvous-send-and-recv"><a class="header" href="#localrendezvous-send-and-recv">LocalRendezvous: Send and Recv</a></h4>
<p>LocalRendezvous 实现了send和recv最基本的操作，按照send请求和recv请求顺序做了不同的处理：</p>
<ol>
<li>
<p>如果recv先到，就新创建一个item，把recv请求放到queue里面，等待send tensor抵达的时候，调用item.waiter回调函数通知recv， tensor已经到了。</p>
</li>
<li>
<p>如果send先到，就新创建一个item, 把item放到queue里面，等recv请求到达的时候，从队列中取出最开头的一个，调用recv.waiter回调函数，通知tensor已经到了。这里send请求就是简单的把tensor放入key对应的队列中，并不会block住。</p>
</li>
</ol>
<p><img src="tensorflow/./images/rendezvous_send_recv.jpeg" alt="local rendezvous send recv" /></p>
<h4 id="intraprocessrendezvous"><a class="header" href="#intraprocessrendezvous">IntraProcessRendezvous</a></h4>
<p>IntraProcessRendezvous 用于处理进程内的通信, 他的send和recv是委托给LocalRendezvous, 在Local的RecvAsync的回调函数中，它会调用SameWokerRecvDone, 使用CopyTensor::ViaDMA处理跨device通信问题。</p>
<pre><code class="language-cpp">void IntraProcessRendezvous::SameWorkerRecvDone(...)
  //other code ...
  //case 1：都在内存中，直接用使用tensor的operator=
  if (src_host &amp;&amp; dst_host) {
    *out = in;
    done(Status::OK());
    return;
  }
  //other code ...
  //case 2: 使用ViaDMA处理不同device之间的tensor通信
  CopyTensor::ViaDMA(parsed.edge_name, send_args.device_context,
</code></pre>
<h4 id="copytensorviadma"><a class="header" href="#copytensorviadma">CopyTensor::ViaDMA</a></h4>
<p>CopyTensor::ViaDMA处理了device之间的copy tensor。 Tensor的copy有3个方向：</p>
<ol>
<li>HOST_TO_DEVICE</li>
<li>DEVICE_TO_HOST</li>
<li>DEVICE_TO_DEVICE</li>
</ol>
<p>从下图可以看出这些操作最终调用的还是stream_executor的ThenMemcpy所封装的函数。 </p>
<p><img src="tensorflow/./images/copy_tensor_via_dma.jpeg" alt="copy tensor via dma" /></p>
<p>VarientDeviceCopy这个处理数据是DT_VARIENT结构的Tensor的，最后调用的是TensorListeDeviceCopy函数，这个函数所对应的deviceCopyFn就是stream_executor所封装的Memcpy, 这里的VarientDeviceCopy和copyfn都采用了static registor的模式（这种模式在tensorflow中用的非常多）。</p>
<pre><code class="language-cpp">static Status TensorListDeviceCopy(
    const TensorList&amp; from, TensorList* to,
    const UnaryVariantOpRegistry::AsyncTensorDeviceCopyFn&amp; copy) {
  to-&gt;element_shape = from.element_shape;
  to-&gt;element_dtype = from.element_dtype;
  to-&gt;tensors.reserve(from.tensors.size());
  for (const Tensor&amp; t : from.tensors) {
    Tensor tmp(t.dtype());
    TF_RETURN_IF_ERROR(copy(t, &amp;tmp));
    to-&gt;tensors.push_back(tmp);
  }
  return Status::OK();
}
</code></pre>
<h4 id="baseremoterendezvous"><a class="header" href="#baseremoterendezvous">BaseRemoteRendezvous</a></h4>
<p>BaseRemoteRendezvous 的RecvAsync中会检查是否是同一个recv 和sender是否在同一个worker上。</p>
<pre><code class="language-cpp">// 检查是否是同一个worker
bool BaseRemoteRendezvous::IsSameWorker(DeviceNameUtils::ParsedName src,
                                        DeviceNameUtils::ParsedName dst) {
  return DeviceNameUtils::IsSameAddressSpace(src, dst);
}
</code></pre>
<p>如果是同一个worker的话就采用类似IntraProcessRendezvous方式来处理，否则需要通过远程调RecvFromRemoteAsync。</p>
<pre><code class="language-cpp">void BaseRemoteRendezvous::RecvAsync(const ParsedKey&amp; parsed,
  //other code ..
  //case1: 是同一个worker, 说明在本地上
  if (IsSameWorker(parsed.src, parsed.dst)) {
    local_-&gt;RecvAsync(
        parsed, recv_args,
        [this, parsed, done](
        //other code ... 
        //in recv done callback
        SameWorkerRecvDone(parsed, send_args, recv_args, in, out,
  } else {
  //case2: 不是同一个worker需要用RPC 去取。
    RecvFromRemoteAsync(parsed, recv_args, std::move(done));
  }
</code></pre>
<p>RemoteRendezvous中加了个一个Initialize的接口, 这样绑定了一个WorkerSession, 然后在SameWorkerRecvDone的时候，通过这个workerSession去找到对应的device。</p>
<pre><code class="language-cpp">Status BaseRemoteRendezvous::Initialize(WorkerSession* session) {
//other codes...
}
</code></pre>
<p>在SameWorkerRecvDone中通过workerSession找到src_device和dst_device</p>
<pre><code class="language-cpp">void BaseRemoteRendezvous::SameWorkerRecvDone(
  //other code ...
  Status s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.src_device, &amp;src_device);
  //other code ...
  s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.dst_device, &amp;dst_device);
  //other code ..
  //通过ViaDMA实现各个device之间的copy
  CopyTensor::ViaDMA(parsed.edge_name, send_args.device_context,
</code></pre>
<h4 id="rpcremoterendezvous"><a class="header" href="#rpcremoterendezvous">RpcRemoteRendezvous</a></h4>
<p>RpcRemoteRendezvous在BaseRemoteRendezvous的基础上，实现了RecvFromeRemoteAsync的功能, 首先找到send所在的src_worker, 
然后通过rpc调用去取的远程src_worker上的tensor。</p>
<pre><code class="language-cpp">void RpcRemoteRendezvous::RecvFromRemoteAsync(
  //other code..
  RpcRecvTensorCall* call = get_call_freelist()-&gt;New();

  //1. 找到远程的src_worker
  WorkerSession* sess = session();
  WorkerInterface* rwi = sess-&gt;worker_cache-&gt;CreateWorker(call-&gt;src_worker_);

  //2. 找到要copy到的device
  s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.dst_device, &amp;dst_device);

 //other code ..
  //3. Grpc call
  call-&gt;Init(rwi, step_id_, parsed.FullKey(), recv_args.alloc_attrs, dst_device,
             recv_args, std::move(done));
  call-&gt;Start([this, call]() {
 //other code ..
</code></pre>
<p>在RpcRecvTensorCall中会call worker的RecvTensorAsync。</p>
<pre><code class="language-cpp">  void StartRTCall(std::function&lt;void()&gt; recv_done) {
   //other code
    wi_-&gt;RecvTensorAsync(&amp;opts_, &amp;req_, &amp;resp_, std::move(cb));
  }
</code></pre>
<p>中间经过worker service，最终会去call GrpcWorker::GrpcRecvTensorAsync.</p>
<pre><code class="language-cpp">void GrpcWorker::GrpcRecvTensorAsync(CallOptions* opts,
    // Case 1: 如果目标tensor在GPU上的话，需要先cp到host上
    if (src_dev-&gt;tensorflow_gpu_device_info() &amp;&amp; (!on_host)) {
        StatusCallback copy_ready = [response, done, copy, is_dead](const Status&amp; s) {  
            //other code ..
            // copy到response buffer中
            grpc::EncodeTensorToByteBuffer(is_dead, *copy, response);
            done(s);
        }
        GPUUtil::CopyGPUTensorToCPU(src_dev, send_dev_context, &amp;val, copy, copy_ready);
        } else {
        //Case 2: 在Host上直接cp到response的buffer中。
            grpc::EncodeTensorToByteBuffer(is_dead, val, response);
            done(Status::OK());
        }
    }
</code></pre>
<h3 id="rendezvousmgr"><a class="header" href="#rendezvousmgr">RendezvousMgr</a></h3>
<p>RendezvousMgr的作用是维护一个从step_id到Rendezvous的映射。</p>
<blockquote>
<p>RendezvousMgr keeps track of a set of local rendezvous instances.
All tensors sent by this worker are buffered in a RendezvousMgr
until the tensor is received.  Each global unique &quot;step_id&quot;
corresponds to one local rendezvous instance managed by a
RendezvousMgr.</p>
</blockquote>
<p>RendezvousMgr的继承关系如下
<img src="tensorflow/./images/rendezvous_mgr.jpeg" alt="rendezvous mgr" /></p>
<p>映射的table在BaseRendezvousMgr中。</p>
<pre><code class="language-cpp">  //BaseRendezvousMgr的数据成员
  typedef gtl::FlatMap&lt;int64, BaseRemoteRendezvous*&gt; Table;
  mutex mu_;
  Table table_ GUARDED_BY(mu_);
</code></pre>
<p>它的派生类比如RpcRendezvousMgr通过override它的Create函数来创建自己版本的rendezvous。</p>
<pre><code class="language-cpp">  //BaseRendezvousMgr 的CreateRendezvous的纯虚函数
 protected:
  virtual BaseRemoteRendezvous* Create(int64 step_id,
                                       const WorkerEnv* worker_env) = 0;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tensorflow-device"><a class="header" href="#tensorflow-device">Tensorflow Device</a></h2>
<h3 id="摘要-3"><a class="header" href="#摘要-3">摘要</a></h3>
<p>Device包含了自己的memory的计算单元，它是对GPU， TPU， CPU等计算device统一抽象，主要的接口有以下几个：</p>
<ol>
<li>GetAllocator: 这个返回一个allocator，负责在device上分配memory</li>
<li>Compute，ComputeAsync: 负责执行OpKernel中的运算。</li>
<li>ResourceMgr: 负责管理分配在Device上的Variable</li>
<li>tensorflow device thread pool: 调度执行device compute的线程池。</li>
</ol>
<p>其中1，2最重要，分别负责allocate memory和执行opkernel的compute。</p>
<h3 id="device"><a class="header" href="#device">Device</a></h3>
<h4 id="device的继承关系"><a class="header" href="#device的继承关系">Device的继承关系</a></h4>
<p><img src="tensorflow/./images/device_inherit.jpeg" alt="Device继承关系" /></p>
<h4 id="device-thread-pool"><a class="header" href="#device-thread-pool">Device thread pool</a></h4>
<p>Gpu对应的线程池创建有三种模式:global, gpu_private, gpu_shared，由环境变量TF_GPU_THREAD_MODE控制, 默认是global的。</p>
<ol>
<li>global: GPU uses threads shared with CPU in the main compute, thread-pool. This is currently the default.</li>
<li>gpu_private: GPU uses threads dedicated to this device.</li>
<li>gpu_shared: All GPUs share a dedicated thread pool.</li>
</ol>
<p>在DirectSession::Ruinternal调用executor的时候，会把device_thread_pool 传给Executor</p>
<pre><code class="language-cpp">//  DirectSession::RunInternal

    thread::ThreadPool* device_thread_pool =
        item.device-&gt;tensorflow_device_thread_pool();
    if (!device_thread_pool) {
      args.runner = default_runner;
    } else {
      args.runner = [this, device_thread_pool](Executor::Args::Closure c) {
        SchedClosure(device_thread_pool, std::move(c));
      };
    }
    item.executor-&gt;RunAsync(args, barrier-&gt;Get());
  }
</code></pre>
<p>在分布式tensorflow中，GraphMgr::StartParallelExecutors, 通过类似的方法吧device_thread_pool 传给executor。</p>
<pre><code class="language-cpp">//GraphMgr::StartParallelExecutors
    thread::ThreadPool* device_thread_pool =
        item.device-&gt;tensorflow_device_thread_pool();
    if (!device_thread_pool) {
      args.runner = default_runner;
    } else {
      args.runner = [this, device_thread_pool](Executor::Args::Closure c) {
        SchedClosure(device_thread_pool, std::move(c));
      };
    }
    item.executor-&gt;RunAsync(args, barrier-&gt;Get());
  }
</code></pre>
<p>在Executor::schedulReady中，会使用这个runner去执行node的process。</p>
<pre><code class="language-cpp">// Executor::ScheduleReady
//Case 1

//other code and 
// Schedule to run all the ready ops in thread pool.
runner_([=]() { Process(tagged_node, scheduled_usec); });

//other code and if...
// Dispatch to another thread since there is plenty of work to
// do for this thread.
runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec));

//other code under some if ...
 // There are inline nodes to run already. We dispatch this expensive
 // node to other thread.
runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec));
</code></pre>
<h3 id="device-context"><a class="header" href="#device-context">Device Context</a></h3>
<p>GpuDeviceContext有点复杂，有不少的代码逻辑是用来处理一个GPU 启动了多个streams的，graph中的每个node会分配一个stream_id。</p>
<h4 id="device-context-map"><a class="header" href="#device-context-map">device context map</a></h4>
<p>每个node对应OpKernel的device_context会使用这个stream_i来CopyCpuTensorToDevice， CopyDeviceTensorToCpu, 在Compute的时候，opkernel的计算也会这个stream_id对应的stream上执行。</p>
<p><img src="tensorflow/./images/device_context.jpeg" alt="device context" /></p>
<p>不过现在好玩的是现在BaseGPuDevice的构造函数中max_stream传的值为1，使用多个stream的特性没开，大家用的是同一个stream，在stackflow上搜到了一个为啥这么做的回答：</p>
<blockquote>
<p>Yeah, you are looking at code that is a bit stale; we've tried experimenting with multiple compute streams and have found that, so far, it hasn't helped that much in important use cases. We technically support multiple streams, but we never turn it on.</p>
<p>At some point in the future, we want to start playing around with using multiple compute streams again though, so it's nice to have the code there.</p>
<p>Devices can use as many DeviceContexts as they want; on GPU we only use a few and we use FillContextMap to do the mapping, but it really depends on the hardware how they should be used</p>
</blockquote>
<p>目前这个特性是实验性的，在重要的use cases中没起到重要的作用，所以这个特性没开, 后续可能会开，所以这部分代码保留了。</p>
<p>除此之外，还在stream_id的基础上做了一个EigenDevice，估计是给Eigen计算提供的吧。无论怎样，DeviceContext给每个Opkernel包了stream_id，然后在执行的时候，会找到这个stream_id对应的cuda_stream。</p>
<h4 id="eigengpudevice"><a class="header" href="#eigengpudevice">Eigen::GpuDevice</a></h4>
<p>给Eigen::GpuDevice封装了一个EigenCudaStreamDevice, 用来给Eigen::GpuDevice allocate和deallocate memroy, 具体的怎么用的估计要去挖Eigen的代码了, 还有scratch buffer的作用也不是很明白。</p>
<pre><code class="language-cpp">class EigenCudaStreamDevice : public ::Eigen::StreamInterface 
  // allocate
  void* allocate(size_t num_bytes) const override{
    //使用device的allocate进行内存分配
  }

  //deallocate
  void deallocate(void* buffer) const override {
   //异步的AsyncFreeData，最终调用的是Device的allocate去free内存
  }
</code></pre>
<h4 id="compute"><a class="header" href="#compute">Compute</a></h4>
<p>Gpu的Compute部分主要有BaseGpuDevice::ComputeHelper来处理，主要是如果gpu使用了多个stream特性的话，需要等待input的stream都完成之后，再执行op对应的stream。</p>
<pre><code class="language-cpp">void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
  //如果是多个stream,需要等待所有input的stream执行完毕。
  if (num_streams &gt; 1) {
    // If this op's device context is different from the other contexts,
    // we must wait on the stream.
    for (int i = 0; i &lt; context-&gt;num_inputs(); ++i) {
      const GPUDeviceContext* idc =
          static_cast&lt;GPUDeviceContext*&gt;(context-&gt;input_device_context(i));
      //other code: 主要是log
      if (idc-&gt;stream() != stream) stream-&gt;ThenWaitFor(idc-&gt;stream());
    }
  gpu::cuda::ScopedActivateExecutorContext scoped_activation{stream-&gt;parent()};
  op_kernel-&gt;Compute(context);
  //other code: 主要是cuda执行状态检查
</code></pre>
<h3 id="device-factory"><a class="header" href="#device-factory">Device Factory</a></h3>
<p>DeviceFactory的继承关系如下：</p>
<p><img src="tensorflow/./images/device_factory.jpeg" alt="image" /></p>
<p>DeviceFactory包含了一些静态函数： AddDevices, NewDevices, Register, GetFactory, 和一个virutal CreateDevices。
NewDevices用于自动化测试，对外主要接口是AddDevices, Register负责device factory的注册, 这两者的调用关系如下：</p>
<p><img src="tensorflow/./images/device_factory_add_devices.jpeg" alt="image" /></p>
<p>DeviceFactory也采用了static registor的方法，自动注册了DeviceFactory,</p>
<pre><code class="language-cpp">//device_type, DeviceFactoryClass, Prority
REGISTER_LOCAL_DEVICE_FACTORY(&quot;CPU&quot;, ThreadPoolDeviceFactory, 60);
REGISTER_LOCAL_DEVICE_FACTORY(&quot;CPU&quot;, GPUCompatibleCPUDeviceFactory, 70);
REGISTER_LOCAL_DEVICE_FACTORY(&quot;GPU&quot;, GPUDeviceFactory, 210);
</code></pre>
<p>这个宏展开后是声明了一个Registrar的 static var, 在它的构造函数中会去调用DeviceFactory的Register注册Factory，
而Register函数最后会把Factory 加入到static device_factories中。</p>
<pre><code class="language-cpp">template &lt;class Factory&gt;
class Registrar {
 public:
  explicit Registrar(const string&amp; device_type, int priority = 50) {
    DeviceFactory::Register(device_type, new Factory(), priority);
  }
}
</code></pre>
<p>在创建一个DirectSesion, 或者GrpServer::Init(每个worker都会起一个GrpcServer)的时候，会调用AddDevices获取worker上的devices.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow-model-optimize"><a class="header" href="#tensorflow-model-optimize">tensorflow model optimize</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h2 id="将keras模型导出为tf-frozen-graph"><a class="header" href="#将keras模型导出为tf-frozen-graph">将keras模型导出为tf frozen graph</a></h2>
<h4 id="frozen-keras-model"><a class="header" href="#frozen-keras-model">frozen keras model</a></h4>
<p>将keras的h5文件转换为tensorflow的pb文件,  这里面使用了 <code>convert_variables_to_constants</code>将模型中的变量都convert成了常量（方便后续采用quantilize或者tensorrt， 对模型推断部分做进一步的优化）</p>
<pre><code class="language-python">import keras
from keras.layers.core import K
import tensorflow as tf

def frozen_keras_model(keras_model_path, output_node_names, export_path):
    output_node_namess = output_nodes.split(&quot;,&quot;)
    model = keras.models.load_model(keras_model_path)
    print(&quot;the model output nodes is {}&quot;.format(model.outputs))
    with K.get_session() as sess:
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess,
            tf.get_default_graph().as_graph_def(),
            output_nodes_names,
            variable_names_blacklist=['global_step']
        )
        with tf.gfile.Gfile(export_path, &quot;wb&quot;) as f:
            f.write(output_graph_def.SerializeToString())
    
</code></pre>
<p>将<code>global_step</code>放到<code>variable_names_blacklist</code>是因为2中的bug.</p>
<pre><code class="language-python">    variable_names_blacklist=['global_step']
</code></pre>
<p>可以通过print <code>model.outputs</code>来查看keras的输出节点，可以通过tensorboard来看keras模型，然后找到最后的输出节点。一般keras模型的输出节点有好多个（比如训练用的之类的)，预测输出节点为其中的一个。</p>
<h4 id="使用tensorboard展示keras-model对应的graph"><a class="header" href="#使用tensorboard展示keras-model对应的graph">使用tensorboard展示keras model对应的graph</a></h4>
<p>首先使用tf summary创建相应的log</p>
<pre><code class="language-python">def keras_model_graph(keras_model_path, log_dir):
    model = keras.model.load_model(keras_model_path)
    with K.get_session() as sess:
        train_writer = tf.summary.FileWriter(log_dir)
        train_writer.add_graph(sess.graph)
</code></pre>
<p>启动tensorboard</p>
<pre><code>$tensorboard --log_dir logdir
</code></pre>
<h3 id="参考文献-2"><a class="header" href="#参考文献-2">参考文献</a></h3>
<ol>
<li>
<p><a href="https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">Stackoverflow: How to export Keras .h5 to tensorflow .pb</a></p>
</li>
<li>
<p><a href="https://github.com/tensorflow/tensorflow/issues/14452">BUG: freeze_graph producing invalid graph_def in tensorflow </a></p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="使用dataset-iterator-优化keras-model预测的吞吐量"><a class="header" href="#使用dataset-iterator-优化keras-model预测的吞吐量">使用dataset iterator 优化keras model预测的吞吐量</a></h2>
<h3 id="predict_on_generator"><a class="header" href="#predict_on_generator">predict_on_generator</a></h3>
<p>现在做的项目，需要在短时间内一次性预测一组大量的图片，刚开始的时候，采用了keras的predict_on_generator和Sequnce，速度比一个个feed dict的形式快了不少, 但是吞吐量还是没达到要求，感觉还有优化的地方。</p>
<pre><code class="language-python">class BatchSequnce(Sequence):
    def __len__(self):
        # 返回batch总个数
        return self.batch_count

    def __getitem__(self, idx):
        #返回一个batch的数据
        #这里可能会做一些数据预处理的工作，比如将图片从文件中加载到内存中然后做特征预处理
        pass
</code></pre>
<pre><code class="language-python"> model = keras.load_model(model_path)
 generator = BatchSequnce(....)
 ret = model.predict_generator(
         generator=generator,
         steps=None,
         workers=10,
         verbose=True,
 )
</code></pre>
<h3 id="dataset"><a class="header" href="#dataset">Dataset</a></h3>
<p>经分析, GPU每次都要等 BatchSequnce的<code>__getitem___</code>处理完之后，才能fetch到数据，如果<code>__getitem__</code>做了比较耗时间的操作的话，会让GPU一直在等待, 而且GPU在处理每个Batch数据的时候，都要等一次, tensorflow的Prefech感觉可以缓解这个问题，后来尝试了下，所消耗的时间优化到了以前的70%左右。</p>
<h4 id="使用iterator-改造keras模型"><a class="header" href="#使用iterator-改造keras模型">使用iterator 改造keras模型</a></h4>
<ol>
<li>
<p>首先采用<a href="tensorflow/./export-keras-model-as-tf-frozen-graph.html">将keras模型导出为tf frozen graph</a>中的方式，将Keras的h5模型转换成tensorflow的pb文件。</p>
</li>
<li>
<p>使用<code>tf.data.Iterator.from_structure</code>(可重新初始化迭代器可以通过多个不同的 Dataset 对象进行初始)的形式, 声明iterator的输出dtype和TensorShape,</p>
</li>
<li>
<p>调用<code>tf.import_graph_def</code> 导入模型, 导入的时候，使用input_map将placeholde,比如&quot;input&quot;替换成Dataset的itereator next_element</p>
</li>
</ol>
<p>这部分代码如下</p>
<pre><code class="language-python">    def load_model(self, sess, frozen_model_file):
        with tf.name_scope(&quot;dataset&quot;):
            iterator = tf.data.Iterator.from_structure(
                    tf.float32,
                    tf.TensorShape([self.batch_size, 450, 450, 3]))
            next_element = iterator.get_next()
            next_element = tf.convert_to_tensor(next_element, tf.float32)

        with tf.gfile.GFile(frozen_model_file, &quot;rb&quot;) as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())

        tf.import_graph_def(
                graph_def,
                name=&quot;&quot;,
                input_map={&quot;input_1:0&quot;: next_element})
        output_op_name = &quot;y&quot;
        output_op = sess.graph.get_operation_by_name(output_op_name).outputs[0]
        return iterator, output_op
</code></pre>
<h3 id="设计dataset"><a class="header" href="#设计dataset">设计DataSet</a></h3>
<p>这里面需要注意的时候, 真正的map函数需要采用py_func包一层, 同事指定py_func的输出tensor shape, 这里的num_map_parall一般取cpu的个数.</p>
<pre><code class="language-python">class DataSetFactory(object):
    def make_dataset(self):
        def generator():
            #返回要处理的文件路径, 或者坐标等
            yield [x, y, w, h]

        output_types = (tf.float32)
        output_shapes = (tf.TensorShape([4]))
        ds = tf.data.Dataset.from_generator(
                generator,
                output_types,
                output_shapes=output_shapes)

        ds = ds.map(lambda region: self.map_func(region), num_map_parall=80)

        ds = ds.prefetch(buffer_size=self.batch_size * 256)
        ds = ds.batch(self.batch_size)
        ds = ds.prefetch(buffer_size=self.batch_size * 10)

        return ds

    def map_func(self, region):
        def do_map(region):
            # 加载图片和预处理
            return img_data
        # 这里采用了py_func，可以执行任意的Python函数，同时需要后面通过reshape的方式设置
        # image_data的shape。
        img_data = tf.py_func(do_map, [region], [tf.float64])
        img_data = tf.reshape(img_data, [450, 450, 3])
        img_data = tf.cast(img_data, tf.float32)
        return image_data
</code></pre>
<h4 id="prefetch_to_device"><a class="header" href="#prefetch_to_device">prefetch_to_device</a></h4>
<p>tensorflow 后来加了prefetch_to_device, 经测试可以提高5%左右的效率吧,但是和structure iterator初始化的时候有冲突，因此这个地方把它去掉了。</p>
<pre><code class="language-python"># 由于prefech_to_device必须是dataset的最后一个处理单元，
# structure iterator用这个ds初始化的时候会有问题，
# 因此这个地方将prefetch_to_gpu注释掉了
# gpu_prefetch = tf.contrib.data.prefetch_to_device(
#         &quot;/device:GPU:0&quot;,
#         buffer_size=self.batch_size * 10)
# ds = ds.apply(gpu_prefetch)
</code></pre>
<h4 id="使用dataset初始化iterator"><a class="header" href="#使用dataset初始化iterator">使用dataset初始化iterator</a></h4>
<pre><code class="language-python">    def init_iterator(self, dataset):
        # 这里的output_op就是load_model时返回的iterator
        init_iterator_op = self.iterator.make_initializer(dataset)
        self.sess.run(init_iterator_op)

    def predict(self):
        # 这里的output_op就是load_model时返回的output_op
        while True:
            outputs = self.sess.run(self.output_op)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="统计gpucpu利用率脚本"><a class="header" href="#统计gpucpu利用率脚本">统计gpu,cpu利用率脚本</a></h2>
<pre><code class="language-bash">#!/bin/bash
start=$(date +%s)
while [ 1 ]
do
    cpu=$(awk -v a=&quot;$(awk '/cpu /{print $2+$4,$2+$4+$5}' /proc/stat; sleep 1)&quot; '/cpu /{split(a,b,&quot; &quot;); print 100*($2+$4-b[1])/($2+$4+$5-b[2])}'  /proc/stat)
    seconds=$(expr $(date +%s) - $start)
    gpu_util=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)
    echo &quot;$seconds, $cpu, $gpu_util&quot;
    #sleep 1
done

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pthread"><a class="header" href="#pthread">pthread</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pthread-primer-笔记"><a class="header" href="#pthread-primer-笔记">Pthread primer 笔记</a></h1>
<h2 id="进程和线程"><a class="header" href="#进程和线程">进程和线程</a></h2>
<h4 id="在kernel中process的context"><a class="header" href="#在kernel中process的context">在kernel中process的context</a></h4>
<ul>
<li>cpu相关：program counter pointer, stack top pointer, cpu general registers, sates.</li>
<li>内存：memory map</li>
<li>user: uid, gid, euid, egid, cwd.</li>
<li>信号: signal dispatch table</li>
<li>File: file descriptors</li>
</ul>
<img alt="process-struct" src="pthread/./images/process-struct.jpeg" width=500px/>
<h4 id="thread的context-data"><a class="header" href="#thread的context-data">thread的context data</a></h4>
<ul>
<li>cpu相关：program counter pointer, stack top pointer, cpu general registers, sates.</li>
<li>内存相关: stack</li>
</ul>
<img alt="process-struct" src="pthread/./images/thread-struct.jpeg" width=500px/>
<p><b>线程的stack是分配在process的heap上的</b></p>
<pre><code class="language-cpp">//设置和获取线程的stack address
include &lt;pthread.h&gt;
int pthread_attr_setstack(pthread_attr_t *attr, void* stackaddr, size_t stacksize);
int pthread_attr_getstack(const pthread_attr_t* attr, void** stackaddr, size_t* stacksize);
</code></pre>
<p><b>整个进程只有一份signal dispatch table</b></p>
<p>所以signal 中断的时候，说不准会中断到那个thread里面，需要加signal mask来处理。</p>
<h4 id="使用thread的好处"><a class="header" href="#使用thread的好处">使用thread的好处</a></h4>
<ol>
<li><code>context switch</code>: process的上下文切换比thread的context switch 耗时间.</li>
<li><code>memory share</code>: thread之间的通信，共享process的内存，file等资源比process之间的通信，share内存方便.</li>
</ol>
<h2 id="线程调度和生命周期"><a class="header" href="#线程调度和生命周期">线程调度和生命周期</a></h2>
<h4 id="线程调度"><a class="header" href="#线程调度">线程调度</a></h4>
<p>线程有两种调度方式，一种是完全在user space, 由thread库做调度，优点是省了system call 从而省下了从user space 到kernel space的切换, 比较快，缺点是，有一个线程挂在IO上后，整个process都会被挂起.(可以把block的system call 改成nonblock的，使用asyc io来解决这个问题).</p>
<p>另外一种是kernel 实现的light weight process(lwp), lwp避免了整个线程被挂起的缺点，但是需要从user space 到kernel space的切换, 比完全user space实现的线程慢一点。</p>
<p>现实中这两种的实现的方式可以混合起来， 混合方式如下：</p>
<ul>
<li>多个线程对应一个lwp</li>
<li>一个线程对应一个lwp</li>
<li>多个线程对应多个lwp</li>
</ul>
<p>在pthread 中可以这么设置调度的属性:</p>
<pre><code class="language-cpp">//pthread中设置调度scope
//PTHREAD_SCOPE_SYSTEM 表示system 全局的， PTHREAD_SCOPE_PROCESS 表示process scope的。
pthread_attr_t attr;
pthread_attr_init(&amp;attr);
pthread_setscope(&amp;atttr, PTHREAD_SCOPE_SYSTEM);
pthread_create(&amp;tid, &amp;attr, foo, NULL);
</code></pre>
<p><b>影响线程调度的一些属性</b></p>
<ul>
<li>scope: PTHREAD_SCOPE_PROCESS, PTHREAD_SCOPE_GLOBAL</li>
<li>policy: SCHED_RR, SCHED_FIFO, SCHED_OTHER</li>
<li>priority</li>
<li>inheritance</li>
</ul>
<p><b>线程状态以及状态之间的迁移关系如下图：</b></p>
<img alt="threads states" src="pthread/./images/thread-states.jpeg" width=500px/>
<p><b>四种running中的线程被切出去的状况</b></p>
<ul>
<li>synchronization 线程require lock的失败被挂在lock的sleep queue上。</li>
<li>preemption 被抢占了，T1在运行的时候，一个更高优先级的线程T2到了runnable的状态, T1会被T2抢占了。</li>
<li>yielding. 线程T1主动调用sched_yield, 如果有和T1优先权一样的T2线程，就切换到T2线程，如果没有，T1就接着运行。</li>
<li>time-slicing. T1的时间片用完了，和T1有同样优先权的T2接着运行。</li>
</ul>
<h4 id="创建和退出线程"><a class="header" href="#创建和退出线程">创建和退出线程</a></h4>
<pre><code class="language-cpp">//create
int pthread_create(pthread_t* thread, const pthread_attr_t* attr, void*(* start_routine)(void*), void* arg);
//exit
void pthread_exit(status);
</code></pre>
<p>线程的返回值，一种是函数执行结束后，直接return的值，另外一种是pthread_exit(status)这个的返回值。</p>
<h4 id="join-等待线程执行结束"><a class="header" href="#join-等待线程执行结束">join: 等待线程执行结束</a></h4>
<p>join之后线程会处于阻塞状态直到等待的线程T1执行完毕，join之后t1线程的相关内存会被清理掉，所以说一个子线程只能被join一次.</p>
<p>设置线程的属性为joinable</p>
<pre><code class="language-cpp">pthread_t thread_id;
pthread_attr_t attr;
pthread_attr_init(&amp;attr);
pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE);
pthread_create(&amp;thread_id, &amp;attr, work, (void*)arg);
</code></pre>
<p>阻塞等待线程的执行结果，获取线程的返回结果</p>
<pre><code class="language-cpp">//等待t1线程执行结束, exit_status 是子线程的返回值.
pthread_join(t1, &amp;exit_status)
</code></pre>
<p>joinable线程和detehced线程的区别是线程结束的时候，资源(线程对应的标识符pthread_t, 线程返回信息)该怎么释放.</p>
<p>对于joinable线程t1, 只有当其他线程对t1调用了pthread_join之后, 线程t1才会释放所占用的资源, 否则 会进入类似于进程的zombile状态，这些资源不会被会回收掉.</p>
<h4 id="使用信号量-等待线程执行结束"><a class="header" href="#使用信号量-等待线程执行结束">使用信号量 等待线程执行结束</a></h4>
<p>使用信号量等待一堆子线程执行结束, 在主线程里面调用thread_signle_barrier, 然后子线程结束的时候调用<code>SEM_POST(barrier)</code></p>
<pre><code class="language-cpp">void thread_signle_barrier(sem_t* barrier, int count){
    while( count &gt; 0) {
        SEM_WAIT(barrier);
        count--;
    }
}
</code></pre>
<h4 id="detach"><a class="header" href="#detach">detach</a></h4>
<p>如果想要t1线程执行结束收系统自动回收t1的资源, 而不是通过调用pthread_join回收资源(会阻塞线程), 我们可以将线程设置为deteched, 有三种方式可以设置线程为deteched.</p>
<ul>
<li>创建线程时指定线程的 detach 属性: pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED);</li>
<li>通过在子线程中调用 pthread_detach(pthread_self());</li>
<li>在主线程中调用 pthread_detach(thread_id);(非阻塞, 执行完会立即会返回)</li>
</ul>
<h4 id="取消线程的执行"><a class="header" href="#取消线程的执行">取消线程的执行</a></h4>
<p>在pthread中可以通过pthread_cancel(t1)来取消线程t1的执行, 这个会设置线程t1的cancel state, 由线程t1在自己在cancel point 检查是否退出线程, 在退出线程的时候会执行cleanup stack中的函数(比如释放自己hold的锁). 一般会block的函数调用，比如sem_wait, pthread_cond_wait或者会block的系统调用前后检查check point.</p>
<p>如下代码段：</p>
<pre><code class="language-cpp">void cleanup_lock2(void* arg){
    pthread_mutex_unlock((pthread_mutex_t*)arg)
}

void thread1_run(){
    pthread_mutex_lock(&amp;answer_lock);
    pthread_cleanup_push(cleanup_lock2, (void*)&amp;answer_lock);
    while(!firest_thread_to_find_answer) {
        pthread_cond_wait(&amp;cvn, &amp;answer_lock);
    }
    pthread_cleanup_pop(0)
}
</code></pre>
<p>也可以通过<code>pthread_setcanceltype</code>设置为异步取消PTHREAD_CANCEL_ASYNCHRONOUS，这样会给t1线程发送<code>SIGCANCEL</code>信号，t1线程在信号处理函数中结束自己的执行。</p>
<h4 id="signal-信号处理"><a class="header" href="#signal-信号处理">Signal 信号处理</a></h4>
<p>Linux 多线程应用中，每个线程可以通过调用 pthread_sigmask() 设置本线程的信号掩码, pthread_kill像某个线程发送signal.</p>
<h5 id="signal-handler-异步的方式处理信号"><a class="header" href="#signal-handler-异步的方式处理信号">signal handler 异步的方式处理信号</a></h5>
<p>多线程处理signal时候需要注意事项</p>
<ul>
<li>信号处理函数尽量只执行简单的操作，譬如只是设置一个外部变量，其它复杂的操作留在信号处理函数之外执行；</li>
<li>errno 是线程安全，即每个线程有自己的 errno，但不是异步信号安全。如果信号处理函数比较复杂，且调用了可能会改变 errno 值的库函数，必须考虑在信号处理函数开始时保存、结束的时候恢复被中断线程的 errno 值；</li>
<li>信号处理函数只能调用可以重入的 C 库函数(只能调用async safe 的函数)；譬如不能调用 malloc（），free（）以及标准 I/O 库函数等；</li>
<li>信号处理函数如果需要访问全局变量，在定义此全局变量时须将其声明为 volatile，以避免编译器不恰当的优化</li>
</ul>
<h5 id="sigwait-同步串行方式"><a class="header" href="#sigwait-同步串行方式">sigwait, 同步串行方式</a></h5>
<p>等待信号的到来，以串行的方式从信号队列中取出信号进行处理.</p>
<pre><code class="language-cpp">void signal_hanlder_thread() {
    sigemptyset(&amp;waitset);  
    sigaddset(&amp;waitset, SIGRTMIN);  
    sigaddset(&amp;waitset, SIGUSR1);  
    while (1)  {  
        //串行的方式处理信号
        rc = sigwaitinfo(&amp;waitset, &amp;info);  
        if (rc != -1) {  
        sig_handler(info.si_signo);  
    }
}
</code></pre>
<h3 id="thread-local-storage"><a class="header" href="#thread-local-storage">Thread local storage</a></h3>
<p>TLS是只在线程自己可见的全局数据, 而不必担心别的线程会改变这个全局数据, 比如要实现每个线程对db的connection单例模式的话，可以把线程的全局connection单例变量存在TLS中。 在使用中有两种方式，一个是pthread_key的方式，另外一个是使用gcc提供的<code>__thread</code>.</p>
<h4 id="thread-specific-data"><a class="header" href="#thread-specific-data">Thread Specific Data</a></h4>
<pre><code class="language-code">pthread_keycreate
pthread_setspecific
pthread_getspecific
</code></pre>
<h4 id="__thread"><a class="header" href="#__thread"><code>__thread</code></a></h4>
<p><code>__thread</code>是gcc提内置的attr, 它只能用于修饰POD类型，不能修饰class类型，因为它无法自动调用构造函数和析构函数。 <code>__thread</code>每个线程都有一份独立的实体，线程之间相互不影响.</p>
<pre><code class="language-cpp">int g_var; // 全局变量
__thread int t_var; //thread变量
</code></pre>
<h2 id="线程的同步"><a class="header" href="#线程的同步">线程的同步</a></h2>
<h3 id="atomic-指令"><a class="header" href="#atomic-指令">atomic 指令</a></h3>
<p>线程执行的时候，在两个指令之间，随时都可能会被抢占掉, 所以需要一个atomic的指令来避免这种状况.</p>
<h4 id="atomic-test-and-set-style-ldstub"><a class="header" href="#atomic-test-and-set-style-ldstub">atomic test and set style: ldstub</a></h4>
<p>ldstub (load and store unsigned byte) 就是一个atomic test and set的指令, 从内存中载入一个unsigned字节，并且把内存中那个字节设置为1.</p>
<p><b>一个mutex lock的实现</b></p>
<pre><code>try_agin: ldstub address -&gt; register
          compare register, 0
          branch_equal got_it
          call go_to_sleep
          jump try_again
got_it:  return
</code></pre>
<p>从这儿可以看到，线程从go_to_sleep返回之后，需要去重新获取lock, 如果获取失败，就接着go_to_sleep.</p>
<h3 id="basic-primitive"><a class="header" href="#basic-primitive">basic primitive</a></h3>
<p>所有线程之前shared的数据需要被用lock保护起来，比如全局数据，传入到另外一个线程的Data struct， 还有static数据。</p>
<h4 id="mutex-lock互斥锁"><a class="header" href="#mutex-lock互斥锁">mutex lock(互斥锁)</a></h4>
<p>线程获取mutex lock失败以后，会被放到mutex对应的sleep队列中。</p>
<pre><code class="language-cpp">pthread_mutex_lock
//critical section
pthread_mutex_unlock
</code></pre>
<img alt="mutex lock sleep queue" src="pthread/./images/mutex-lock.jpeg" width=500px/>
<p>另外一种非阻塞的获取锁的方法<code>pthread_mutex_trylock</code> 如果获取锁成功返回0，否则返回<code>EBUSY</code>.</p>
<h4 id="semaphores信号量"><a class="header" href="#semaphores信号量">semaphores(信号量)</a></h4>
<p>信号量机制用于协调多个资源的使用(比如一个队列或者缓冲区)，semaphores的值表示可用资源的数量(队列中可用资源的个数)。常用于解决生产者和消费者问题.</p>
<pre><code>// 初始化
int sem_init(sem_t *sem, int pshared, unsigned int val);
// 没有可用的信号量就等待，否则
int sem_wait(sem_t *sem);
// 释放一个信号量，信号量的值加1
int sem_post(sem_t *sem);
</code></pre>
<p>信号量处理流程</p>
<img src="pthread/./images/semaphore-flow.jpeg" width=400px/>
<p>生产者消费者问题, 假设队列的长度是20:</p>
<img src="pthread/./images/producer-consumer.jpeg" width=340px/>
<pre><code class="language-cpp">#include &lt;semaphore.h&gt;

//shared global vars
sem_t sem_producer;
sem_t sem_consumer;
//list

void producer(){
    while(1){
        sem_wait(sem_consumer);
        pthread_mutex_lock(list_lock);
        add(list);
        pthread_mutex_unlock(list_lock);
        sem_post(sem_producer);
    }
}

void consumer(){
    while(1) {
        sem_wait(sem_producer);
        pthread_mutex_lock(list_lock);
        consume(list);
        pthread_mutex_unlock(list_lock);
        sem_post(sem_consumer);
    }
}
void main(){
    sem_init(&amp;sem_producer, 0);
    sem_init(&amp;sem_consumer, 20);

    pthread_t producer_tid;
    pthread_t consumer_tid;

    pthread_create(&amp;producer_tid, nullptr, producer, nullptr);
    pthread_create(&amp;consumer_tid, nullptr, consumer, nullptr);
}
</code></pre>
<h4 id="condition-var-条件变量"><a class="header" href="#condition-var-条件变量">condition var (条件变量)</a></h4>
<p>condition var 的流程, condition var 访问需要用个mutex lock保护起来, condition判断失败之后，会unlock 保护condition var 的lock, 然后进入sleep, 之后被唤醒的时候，会再次去获取condition var的lock。</p>
<img src="pthread/./images/condition-flow.jpeg" width=400px/>
<pre><code class="language-cpp">&lt;code&gt;
// 初始化
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
// 动态初始化
int pthread_cond_init(pthread_cond_t* restrict cond, const pthread_condattr_t* restrict attr);

//销毁
int pthread_cond_destroy(pthread_cond_t* cond);

//等待
int pthread_cond_wait( pthread_cond_t*   restrict cond, pthread_mutex_t*  restrict mutex );
int pthread_cond_timedwait( pthread_cond_t*         restrict cond, pthread_mutex_t*        restrict mutex, const struct timespec*  restrict abstime );


// 通知
// singal 函数一次只能唤醒一个线程, 而 broadcast 会唤醒所有在当前条件变量下等待的线程.
int pthread_cond_broadcast(pthread_cond_t* cond);
int pthread_cond_signal(pthread_cond_t* cond);

</code></pre>
<p>wait for condition</p>
<pre><code class="language-cpp">// safely examine the condition, prevent other threads from
// altering it
pthread_mutex_lock (&amp;lock);
while ( SOME-CONDITION is false)
    pthread_cond_wait (&amp;cond, &amp;lock);

// Do whatever you need to do when condition becomes true
do_stuff();
pthread_mutex_unlock (&amp;lock);
</code></pre>
<p>signal condition</p>
<pre><code class="language-cpp">// ensure we have exclusive access to whathever comprises the condition
pthread_mutex_lock (&amp;lock);

ALTER-CONDITION

// Wakeup at least one of the threads that are waiting on the condition (if any)
pthread_cond_signal (&amp;cond);

// allow others to proceed
pthread_mutex_unlock (&amp;lock)
</code></pre>
<h3 id="read-write-lock-读写锁"><a class="header" href="#read-write-lock-读写锁">read write lock (读写锁)</a></h3>
<p>在某个时间内，多个线程可以同时获得读锁, 如果已经有线程获得了读锁，那么尝试获取写锁的将被block, 如果已经有线程获取了读锁，那么其他线程的尝试获取读锁或者写锁将会被block.</p>
<img src="pthread/./images/rw-lock.jpeg" width=400px/>
<pre><code class="language-cpp">pthread_rwlock_t rwlock;
int pthread_rwlock_init(pthread_rwlock_t* restrict rwlock, const pthread_rwlockattr_t * restrict attr);
int pthread_rwlock_destroy(pthread_rwlock_t* rwlock);

// 获取读锁
int pthread_rwlock_rdlock(pthread_rwlock_t* rwlock);
// 获取写锁
int pthread_rwlock_wrlock(pthread_rwlock_t* rwlock);
// 释放锁  
int pthread_rwlock_unlock(pthread_rwlock_t* rwlock);
</code></pre>
<h3 id="spin-lock-自旋锁"><a class="header" href="#spin-lock-自旋锁">Spin lock (自旋锁)</a></h3>
<p>多次trylock, 如果失败了再block, 它的出发点是trylock这个指令的时间很短（比如2us)然后mutex block一次可能需要42us,所以它先尝试几次, 如果在这几us内，lock被释放了，那么能够成功的获取锁了。</p>
<pre><code class="language-cpp">spin_lock(mutex_t* m) {
    for(int i = 0; i &lt; SPIN_COUNT; i++) {
        if (pthread_mutex_trylock(m) != EBUSY) {
            return;
        }
    }
    pthread_mutex_lock(m);
    return;
}
</code></pre>
<p><b>Adaptive Spin lock</b></p>
<p>在很多kernel里面使用的，kernel先看拥有锁的线程在不在running(如果在跑的话，那么线程可能短时间内会释放这个锁，所以值得spin几次去尝试下), 如果不在running 状态的话，就直接去require lock了,然后线程会被block.</p>
<p>使用spin lock的时候，需要好好的评估下到底值不值得，就是critical section hold住lock的时间会不会很长。。如果一般很短的话，值得用spin lock，否则的话用spin lock反而浪费时间。</p>
<h3 id="barriers"><a class="header" href="#barriers">Barriers</a></h3>
<pre><code class="language-cpp">pthread_barrier_t mybarrier;
//初始化
pthread_barrier_init(&amp;mybarrier, NULL, THREAD_COUNT + 1);
pthread_barrier_destroy(&amp;mybarrier);
pthread_barrier_wait(&amp;mybarrier);
</code></pre>
<p>等待最后一个线程达到barrier点。
<img src="pthread/./images/barrier-wait.jpeg" width=500px/></p>
<h2 id="附录"><a class="header" href="#附录">附录</a></h2>
<ol>
<li>linux中的process的virutal memory layout 参见<a href="http://www.enseignement.polytechnique.fr/informatique/INF583/INF583_5.pdf">Processes and Memory Management</a></li>
</ol>
<img src="pthread/./images/process-memeroy-address-layout.png" width=500px/>
<h2 id="参考-1"><a class="header" href="#参考-1">参考</a></h2>
<ol>
<li><a href="http://www8.cs.umu.se/kurser/TDBC64/VT03/pthreads/pthread-primer.pdf">pthread primer</a></li>
<li><a href="http://www.enseignement.polytechnique.fr/informatique/INF583/INF583_5.pdf">Processes and Memory Management</a></li>
<li><a href="http://blog.zhangjikai.com/2016/04/25/%E3%80%90Pthreads%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/">pthread学习笔记, 基本使用</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="glibc的pthread实现代码研读-1-线程的生命周期"><a class="header" href="#glibc的pthread实现代码研读-1-线程的生命周期">Glibc的pthread实现代码研读 1: 线程的生命周期</a></h2>
<p>本文主要包含pthread线程在linux上的创建，执行，exit, detach, join, cancel, thread local storage。</p>
<h3 id="pthread_t"><a class="header" href="#pthread_t">pthread_t</a></h3>
<p>struct pthread定义在<code>nptl/descr.h</code>中, 这边抽几组主要的field来说明下(这里为了方便描述，对field在struct的顺序做了重新的编排)。</p>
<p>首先是创建完线程之后，系统给的id和各种flag attribute.</p>
<pre><code class="language-cpp">/* Flags.  Including those copied from the thread attribute.  */
 int flags;
 pid_t tid;
 /* Lock to synchronize access to the descriptor.  */
 int lock;
</code></pre>
<p>然后最显而易见的是, 线程要执行的函数指针，函数参数以及函数执行的结果, 这几个字段会在线程的入口start_thread中用到。对于result字段: pthread_join(t1, &amp;status), 这个会等待线程t1执行结束，然后把结果放到status中。</p>
<pre><code class="language-cpp"> //保存线程返回结果
  void *result;
 // 线程执行的函数和参数
  void *(*start_routine) (void *);
  void *arg;
</code></pre>
<p>然后一些field用于处理下面这几种异常情况: 线程如果抛异常了，线程调用pthread_exit提前exit了，线程被其它线程pthread_cancel了。</p>
<pre><code class="language-cpp">// 线程cancel的状态
int cancelhandling;
// 线程被cancel的时候，处理cleanup callback和cleanup jmp
struct _pthread_cleanup_buffer* cleanup;
struct pthread_unwind_buf* cleanup_jmp_buf;
/* Machine-specific unwind info.  */
struct _Unwind_Exception exc;
</code></pre>
<p>标明线程是被join的还是已经deteched字段, 这个字段涉及到线程的pthread struct该什么时候释放。</p>
<pre><code class="language-cpp"> struct pthread* joinid;
 #define IS_DETACHED(pd) ((pd)-&gt;joinid == (pd))
</code></pre>
<p>stack相关的field, 在ALLOCATE_STACK和回收statck的时候会用到，由于pthread的这个struct也是放在stack上的，因此需要一些参数记录pthread的offset, user_statck表示是否是由用户提供的stack。</p>
<pre><code class="language-cpp">/* True if the user provided the stack.  */
 bool user_stack;
 void *stackblock;
 size_t stackblock_size;
 /* Size of the included guard area.  */
 size_t guardsize;
 /* This is what the user specified and what we will report.  */
 size_t reported_guardsize;
</code></pre>
<p>thread specific data相关的字段</p>
<pre><code class="language-cpp">
// 用于thread specific data, thread local storage
struct pthread_key_data
{
  uintptr_t seq;
  void* data;
} specific_1stblock[PTHREAD_KEY_2NDLEVEL_SIZE];

struct pthread_key_data* specific[PTHREAD_KEY_1STLEVEL_SIZE];
</code></pre>
<p>最后调度策略和调度参数相关的字段，在线程create的时候，会调用sched_setaffinity， sched_setscheduler让系统设置这些参数。</p>
<pre><code class="language-cpp"> // 调度策略和调度参数
 struct sched_param schedparam;
 int schedpolicy;
</code></pre>
<h4 id="pthread-struct-的alloc和free"><a class="header" href="#pthread-struct-的alloc和free">pthread struct 的alloc和free</a></h4>
<p><code>nptl/allocatestatck.c</code> 中的<code>allocate_stack</code>和<code>__deallocate_stack</code>负责alloc和free pd struct。如果用的是系统分配的stack话， pthread有个stack list，当alloc的时候，从这个stack list中取出一个，然后在free的时候，把这个stack放回到stack list中。</p>
<p>这就导致了一个问题, pthread_t 并不适合作为线程的标识符，比如下面两个线程的pthread_t的地址是一样的(参考自Linux 多线程服务端编程: 4.3节):</p>
<pre><code class="language-cpp">int main() {
    pthread_t t1, t2;
    pthread_create(&amp;t1, NULL, threadFunc, NULL);
    printf(&quot;%lx\n&quot;, t1);
    pthread_join(t1, NULL);

    pthread_create(&amp;t2, NULL, threadFunc, NULL);
    printf(&quot;%lx\n&quot;, t2);
    pthread_join(t2, NULL);

}
</code></pre>
<h3 id="pthread_create"><a class="header" href="#pthread_create">pthread_create</a></h3>
<p>pthread create 首先分配线程的栈，并在这个栈上划出一片内存给pthread struct, 然后调syscall clone(2) 创建一个线程，创建的新的线程会从<code>START_THREAD_DEFF</code> 这个入口开始执行起来，最后线程的执行结果保存在pd-&gt;result里面， 用户可以通过pthread_attr_setstack来指定线程stack的内存，也可以直接使用系统的内存。</p>
<img src="pthread/./images/pthread-create.jpeg" />
分配stack, 使用用户提供的stack或者系统分配一个stack(pd 这个struct也存放在stack里面了)
<pre><code class="language-cpp">ALLOCATE_STACK(iattr, &amp;pd)
</code></pre>
<p><code>create_thread</code> 调用linux系统接口clone创建线程, 如果线程要指定在某个CPU上跑的话，调用sched_setaffinity设置好cpuset, 最后何止好调度策略和调度参数。</p>
<pre><code class="language-cpp">ARCH_CLONE(&amp;start_thread, STACK_VARIABLES_ARGS, clone_flags, pd, &amp;pd-&gt;tid, tp, &amp;pd-&gt;tid)

INTERNAL_SYSCALL(sched_setaffinity, err, 3, pd-&gt;tid, attr-&gt;cpusetsize, attr-&gt;cpuset)

INTERNAL_SYSCALL(sched_setscheduler, err, 3, pd-&gt;tid, pd-&gt;schedpolicy, &amp;pd-&gt;schedparam)
</code></pre>
<p>其中clone 的flags如下：</p>
<pre><code class="language-cpp">const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM
              | CLONE_SIGHAND | CLONE_THREAD
              | CLONE_SETTLS | CLONE_PARENT_SETTID
              | CLONE_CHILD_CLEARTID
              | 0);
</code></pre>
<p><code>CLONE_THREAD</code>, 标明是创建一个线程，和创建者同一个group,  同一个parent。</p>
<p><code>STACK_VARIABLES_ARGS</code>对应着上一步ALLOCATE_STACK分配好的内存地址, 这块内存会作为新的线程的stack来用。</p>
<p>clone中的的start_thread就是线程的entry_point, 这个函数定义在nptl/pthread_create.c里面 <code>START_THREAD_DEFF</code>, 这个函数就是新创建的线程的入口。</p>
<h3 id="start-thread"><a class="header" href="#start-thread">start thread</a></h3>
<p>start thread是线程的入口， 在跑用户函数之前，会设置一个jmp point, 之后等线程执行结束的时候(调用pthread_exit, 或者线程被cancel掉的时候)，会longjump 回到这个函数, 接着做线程执行完的清理工作。</p>
<p>如果线程是Deteched， 那么线程的pd结构就会被释放掉(因为pthread返回的status指针是保存在pd-&gt;result这个里面的)，否则就要等pthread_join完之后释放掉。</p>
<p>最后线程exit_thread之后，会把pd中的tid设置为0，这样就可以唤醒等待join该线程结束的线程。</p>
<img src="pthread/./images/start-thread.jpeg" />
<ol>
<li>设置好unwind buffer, do cancel的时候可以跳回来</li>
</ol>
<pre><code class="language-cpp">int not_first_call;
 not_first_call = setjmp ((struct __jmp_buf_tag* ) unwind_buf.cancel_jmp_buf);
 if (__glibc_likely (! not_first_call))
   {
     THREAD_SETMEM (pd, cleanup_jmp_buf, &amp;unwind_buf);
</code></pre>
<p>setjmp和longjmp是非局部跳转函数, 它可以在在栈上跳过若干调用帧，返回到当前函数调用路径上的某一个函数中, 若直接调用则返回0，若从longjmp调用返回则返回非0值的longjmp中的val值。之后的do_cancel可能会longjmp到这个地方。</p>
<ol start="2">
<li>调用用户提供的函数, 结果存在<code>pd-&gt;result</code>中</li>
</ol>
<pre><code class="language-cpp">#ifdef CALL_THREAD_FCT
      THREAD_SETMEM (pd, result, CALL_THREAD_FCT (pd));
#else
      THREAD_SETMEM (pd, result, pd-&gt;start_routine (pd-&gt;arg));
#endif
</code></pre>
<ol start="3">
<li>做一些清理工作，清理TLS, 标记stack为可复用状态，如果线程是detached, 则释放pd struct的内存, 否则要在pthread_join里面释放这个pb struct, 如果一个线程既不是deteched，也没有线程在pthread_join等待他，这个pb struct就不会被释放，进入一个类似于zombile的状态。</li>
</ol>
<pre><code class="language-cpp">__call_tls_dtors ();
/* Run the destructor for the thread-local data.  */
__nptl_deallocate_tsd ();
/* Clean up any state libc stored in thread-local variables.  */
__libc_thread_freeres ();
if (IS_DETACHED (pd))
    __free_tcb (pd);

// mark stack resuable
char *sp = CURRENT_STACK_FRAME;
size_t freesize = (sp - (char *) pd-&gt;stackblock) &amp; ~pagesize_m1;
assert (freesize &lt; pd-&gt;stackblock_size);
if (freesize &gt; PTHREAD_STACK_MIN)
  __madvise (pd-&gt;stackblock, freesize - PTHREAD_STACK_MIN, MADV_DONTNEED);

// other code
__exit_thread ();
</code></pre>
<h3 id="pthread_exit"><a class="header" href="#pthread_exit">pthread_exit</a></h3>
<p>猜测pthread_exit 的do_cancel的unwind会调用pthread_cleanup_push中注册的cleaup函数，最后会longjmp回到start_thread里面的setjmp那块，继续执行线程结束后的清理工作。</p>
<pre><code class="language-cpp">__pthread_exit (void* value)
{
  THREAD_SETMEM (THREAD_SELF, result, value);

  __do_cancel ();
}
</code></pre>
<p>do_cancel定义如下:</p>
<pre><code class="language-cpp">__do_cancel (void)
{
  struct pthread* self = THREAD_SELF;

  THREAD_ATOMIC_BIT_SET (self, cancelhandling, EXITING_BIT);
  __pthread_unwind ((__ pthread_unwind_buf_t*)
		    THREAD_GETMEM (self, cleanup_jmp_buf));
}
</code></pre>
<h3 id="pthread_join"><a class="header" href="#pthread_join">pthread_join</a></h3>
<p><code>pthread_join(t1, &amp;result)</code> 线程会调用lll_wait_tid等到t1执行结束，然后从t1的pd-&gt;result获取线程返回的结果, 返回给status，最后释放线程t1对应的pd sturct.</p>
<ol>
<li>检查是否有死锁, 避免等待自己，以及正在被cancel的线程，</li>
</ol>
<pre><code class="language-cpp">if ((pd == self
       || (self-&gt;joinid == pd
	   &amp;&amp; (pd-&gt;cancelhandling
	       &amp; (CANCELING_BITMASK | CANCELED_BITMASK | EXITING_BITMASK
		  | TERMINATED_BITMASK)) == 0))
      &amp;&amp; !CANCEL_ENABLED_AND_CANCELED (self-&gt;cancelhandling))
result = EDEADLK;
</code></pre>
<ol start="2">
<li>设置<code>t1-&gt;joinid = self;</code></li>
</ol>
<pre><code class="language-cpp">/* Wait for the thread to finish.  If it is already locked something
     is wrong.  There can only be one waiter.  */
  else if (__builtin_expect (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;joinid,
								   self,
								   NULL), 0))
    /* There is already somebody waiting for the thread.  */
    result = EINVAL;
</code></pre>
<ol start="3">
<li>等待t1线程执行结束, 这里的lll_wait_tid 最后会去调用linux提供的futex, 会被挂起来，一直等到t1的tid变为0。</li>
</ol>
<pre><code class="language-cpp">    /* Wait for the child.  */
    lll_wait_tid (pd-&gt;tid);
</code></pre>
<ol start="4">
<li>free t1线程的pd struct</li>
</ol>
<pre><code>pd-&gt;tid = -1;

     /* Store the return value if the caller is interested.  */
     if (thread_return != NULL)
   *thread_return = pd-&gt;result;


     /* Free the TCB.  */
     __free_tcb (pd);
</code></pre>
<h3 id="pthread_detach"><a class="header" href="#pthread_detach">pthread_detach</a></h3>
<p>标记线程为detached, 把pd的jionid改为自己。</p>
<pre><code class="language-cpp">  int result = 0;
  /* Mark the thread as detached.  */
  if (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;joinid, pd, NULL))
    {
      if (IS_DETACHED (pd))
	      result = EINVAL;
    }
  else if ((pd-&gt;cancelhandling &amp; EXITING_BITMASK) != 0)
      __free_tcb (pd);
  return result;

</code></pre>
<h3 id="pthread_cancel"><a class="header" href="#pthread_cancel">pthread_cancel</a></h3>
<p>pthread_cancel 只是把<code>pd-&gt;cancelhandling</code>的状态记为<code>CANCLEING_BITMASK|CANCELED_BITMASK</code>。</p>
<pre><code class="language-cpp">do{
    oldval = pd-&gt;cancelhandling;
    newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
    //other code

} while (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;cancelhandling, newval,
                          oldval);
</code></pre>
<p>然后在pthread_testcancel的时候，才真正的调用do_cancel去cancel thread.</p>
<pre><code class="language-cpp">//pthread_testcancel --&gt; CANCELLATION_P

if (CANCEL_ENABLED_AND_CANCELED (cancelhandling))			      \
     {									      \
   THREAD_SETMEM (self, result, PTHREAD_CANCELED);			      \
   __do_cancel ();							      \
     }			
</code></pre>
<p>或者一些会check cancel point的调用比如pthread_cond_wait里面，会去检查这个标记，</p>
<pre><code class="language-cpp">pthread_cond_wait --&gt;futex_wait_cancelable --&gt; pthread_enable_asynccancel --&gt;  __do_cancel
futex_reltimed_wait_cancelable --&gt; pthread_enable_asynccancel --&gt; __do_cancel
sem_wait_common -&gt; futex_abstimed_wait_cancelable --&gt; pthread_enable_asynccancel --&gt; __do_cancel
</code></pre>
<h3 id="singal-handle"><a class="header" href="#singal-handle">singal handle</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h2 id="glibc的pthread实现代码研读-2-线程同步"><a class="header" href="#glibc的pthread实现代码研读-2-线程同步">Glibc的pthread实现代码研读 2: 线程同步</a></h2>
<p>第二部分主要讲述pthread中的线程的同步方法包括mutex, sem, condition var, rwlock, barrier的实现，pthread使用了linux的futex来实现这些同步方法。</p>
<h3 id="futex"><a class="header" href="#futex">futex</a></h3>
<p>pthread中的locks通过linux的futex(faster user space locking)实现, lock放在process之间的共享内存中, pthread通过atomic的指令来对这个lock进行dec, inc, load and test 等操作, 如果有竞态冲突的时候获取锁失败的时候，才会去sys call 调用linux底层的do_futex, 底层把线程放到futex对应的wait队列里面, 然后挂起线程等待被唤醒。</p>
<p>由于只有竞态冲突的时候才需要syscall, 其他情况都不需要，因此节省了很多sys call，这样比较快。</p>
<img src="pthread/./glibc-pthread-images/pthread-lock-overview.jpeg" width=300px/>
<h4 id="mutex"><a class="header" href="#mutex">Mutex</a></h4>
<p>xchgl 这个是atomic操作吧，失败了回去调用do_futex, flag 是FUTEX_WAIT</p>
<pre><code>phtread_mutex_lock --&gt; LL_MUTEX_LOCK --&gt; ll_lock --&gt; lll_lock_wait|lll_lock_wait_private --&gt; xchgl

</code></pre>
<h4 id="sem"><a class="header" href="#sem">Sem</a></h4>
<h4 id="condition-var"><a class="header" href="#condition-var">Condition var</a></h4>
<h4 id="read-write-lock"><a class="header" href="#read-write-lock">Read write lock</a></h4>
<h4 id="barrier"><a class="header" href="#barrier">Barrier</a></h4>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yew"><a class="header" href="#yew">Yew</a></h1>
<!-- toc -->
<h2 id="doc-overview"><a class="header" href="#doc-overview">doc overview</a></h2>
<p><img src="yew/./dots/yew_overview.svg" alt="" /></p>
<h2 id="context"><a class="header" href="#context">Context</a></h2>
<p><img src="yew/./dots/context.svg" alt="" /></p>
<h2 id="vnode"><a class="header" href="#vnode">VNode</a></h2>
<p><img src="yew/./dots/vnode.svg" alt="" /></p>
<h2 id="scope"><a class="header" href="#scope">Scope</a></h2>
<p><img src="yew/./dots/Scope.svg" alt="" /></p>
<h2 id="scheduler"><a class="header" href="#scheduler">scheduler</a></h2>
<p><img src="yew/./dots/scheduler.svg" alt="" /></p>
<h2 id="wasm_bindgen"><a class="header" href="#wasm_bindgen">wasm_bindgen</a></h2>
<p><img src="yew/./dots/wasm_bindgen.svg" alt="" /></p>
<h2 id="callback"><a class="header" href="#callback">Callback</a></h2>
<p><img src="yew/./dots/callback.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="axum"><a class="header" href="#axum">Axum</a></h1>
<!-- toc -->
<h2 id="router"><a class="header" href="#router">Router</a></h2>
<h3 id="router数据结构"><a class="header" href="#router数据结构">Router数据结构</a></h3>
<p><img src="axum/./dots/Router.dot.svg" alt="" /></p>
<h3 id="路由注册和分发"><a class="header" href="#路由注册和分发">路由注册和分发</a></h3>
<p><img src="axum/./dots/path_to_route_id.dot.svg" alt="" /></p>
<h2 id="handlers"><a class="header" href="#handlers">handlers</a></h2>
<h3 id="trait-handler"><a class="header" href="#trait-handler">trait Handler</a></h3>
<p>In axum a &quot;handler&quot; is an async function that accepts zero or more
<a href="crate::extract">&quot;extractors&quot;</a> as arguments and returns something that
can be converted <a href="crate::response">into a response</a>.</p>
<p><img src="axum/./dots/extractor.dot.svg" alt="" /></p>
<h3 id="extract-fromrequest"><a class="header" href="#extract-fromrequest">extract: FromRequest</a></h3>
<p>FromRequest用于将requestPart解析成各种类型，然后传个handler</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait FromRequest&lt;B = crate::body::Body&gt;: Sized {
    /// If the extractor fails it'll use this &quot;rejection&quot; type. A rejection is
    /// a kind of error that can be converted into a response.
    type Rejection: IntoResponse;

    /// Perform the extraction.
    async fn from_request(req: &amp;mut RequestParts&lt;B&gt;) -&gt; Result&lt;Self, Self::Rejection&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="axum/./dots/FromRequest.dot.svg" alt="" /></p>
<p>比较常用的有Json, Query, MatchedPath</p>
<h4 id="json"><a class="header" href="#json">Json</a></h4>
<p>Json会用<code>serde_json::from_slice</code>将request body bytes解析为对应的类型，用法示例如下：</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Deserialize)]
struct CreateUser {
    email: String,
    password: String,
}

async fn create_user(extract::Json(payload): extract::Json&lt;CreateUser&gt;) {
    // payload is a `CreateUser`
}

let app = Router::new().route(&quot;/users&quot;, post(create_user));
<span class="boring">async {
</span><span class="boring">axum::Server::bind(&amp;&quot;&quot;.parse().unwrap()).serve(app.into_make_service()).await.unwrap();
</span><span class="boring">};
</span><span class="boring">}
</span></code></pre></pre>
<h4 id="query"><a class="header" href="#query">Query</a></h4>
<p>Query会使用<code>serde_urlencoded</code>将query解析为相应的param, 用法示例如下:</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn app() -&gt; Router {
    Router::new().route(&quot;/&quot;, get(handler))
}

async fn handler(Query(params): Query&lt;Params&gt;) -&gt; String {
    format!(&quot;{:?}&quot;, params)
}

/// See the tests below for which combinations of `foo` and `bar` result in
/// which deserializations.
///
/// This example only shows one possible way to do this. [`serde_with`] provides
/// another way. Use which ever method works best for you.
///
/// [`serde_with`]: https://docs.rs/serde_with/1.11.0/serde_with/rust/string_empty_as_none/index.html
#[derive(Debug, Deserialize)]
#[allow(dead_code)]
struct Params {
    #[serde(default, deserialize_with = &quot;empty_string_as_none&quot;)]
    foo: Option&lt;i32&gt;,
    bar: Option&lt;String&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="multipart-文件上传"><a class="header" href="#multipart-文件上传">Multipart: 文件上传</a></h4>
<p>使用表单获取上传文件，其中form的<code>enctype='multipart/form-data'</code>,  并
使用ContentLengthLimit 来限制文件大小。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn accept_form(
    ContentLengthLimit(mut multipart): ContentLengthLimit&lt;
        Multipart,
        {
            250 * 1024 * 1024 /* 250mb */
        },
    &gt;,
) {
    while let Some(field) = multipart.next_field().await.unwrap() {
        let name = field.name().unwrap().to_string();
        let data = field.bytes().await.unwrap();

        println!(&quot;Length of `{}` is {} bytes&quot;, name, data.len());
    }
}

async fn show_form() -&gt; Html&lt;&amp;'static str&gt; {
    Html(
        r#&quot;
        &lt;!doctype html&gt;
        &lt;html&gt;
            &lt;head&gt;&lt;/head&gt;
            &lt;body&gt;
                &lt;form action=&quot;/&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;
                    &lt;label&gt;
                        Upload file:
                        &lt;input type=&quot;file&quot; name=&quot;file&quot; multiple&gt;
                    &lt;/label&gt;

                    &lt;input type=&quot;submit&quot; value=&quot;Upload files&quot;&gt;
                &lt;/form&gt;
            &lt;/body&gt;
        &lt;/html&gt;
        &quot;#,
    )
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="towerlayer"><a class="header" href="#towerlayer">tower.Layer</a></h2>
<p>Layer 用来写中间件, Axum中提供了HandlerError中间件，可以指定一个函数，将handler的错误
转换为对应的response。ExtractorMiddleware可以将extractor转成middleware，如果extract
成功则继续执行，否则就提前返回。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Layer&lt;S&gt; {
    /// The wrapped service
    type Service;
    /// Wrap the given service with the middleware, returning a new service
    /// that has been decorated with the middleware.
    fn layer(&amp;self, inner: S) -&gt; Self::Service;
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="axum/./dots/layer.dot.svg" alt="" /></p>
<h3 id="extractormiddleware"><a class="header" href="#extractormiddleware">ExtractorMiddleware</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait::async_trait]
impl&lt;B&gt; FromRequest&lt;B&gt; for RequireAuth
where
    B: Send,
{
    type Rejection = StatusCode;

    async fn from_request(req: &amp;mut RequestParts&lt;B&gt;) -&gt; Result&lt;Self, Self::Rejection&gt; {
        if let Some(auth) = req
            .headers()
            .expect(&quot;headers already extracted&quot;)
            .get(&quot;authorization&quot;)
            .and_then(|v| v.to_str().ok())
        {
            if auth == &quot;secret&quot; {
                return Ok(Self);
            }
        }

        Err(StatusCode::UNAUTHORIZED)
    }
}

async fn handler() {}

let app = Router::new().route(
    &quot;/&quot;,
    get(handler.layer(extractor_middleware::&lt;RequireAuth&gt;())),
);
<span class="boring">}
</span></code></pre></pre>
<h3 id="handleerrorlayer"><a class="header" href="#handleerrorlayer">HandleErrorLayer</a></h3>
<p>HandleErrorLayer 可以使用闭包的函数将错误转换为相应的response，相应例子如下</p>
<p>问题：可以使用不同的HandlerErrorLayer堆在一起，每个处理自己相应类型的错误吗？
还是需要在这个地方统一来处理？</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let app = Router::new()
        .route(&quot;/todos&quot;, get(todos_index).post(todos_create))
        .route(&quot;/todos/:id&quot;, patch(todos_update).delete(todos_delete))
        // Add middleware to all routes
        .layer(
            ServiceBuilder::new()
                .layer(HandleErrorLayer::new(|error: BoxError| async move {
                    if error.is::&lt;tower::timeout::error::Elapsed&gt;() {
                        Ok(StatusCode::REQUEST_TIMEOUT)
                    } else {
                        Err((
                            StatusCode::INTERNAL_SERVER_ERROR,
                            format!(&quot;Unhandled internal error: {}&quot;, error),
                        ))
                    }
                }))
                .timeout(Duration::from_secs(10))
                .layer(TraceLayer::new_for_http())
                .layer(AddExtensionLayer::new(db))
                .into_inner(),
        );
<span class="boring">}
</span></code></pre></pre>
<h2 id="response"><a class="header" href="#response">Response</a></h2>
<h2 id="extensions"><a class="header" href="#extensions">Extensions</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A type map of protocol extensions.
///
/// `Extensions` can be used by `Request` and `Response` to store
/// extra data derived from the underlying protocol.
#[derive(Default)]
pub struct Extensions {
    // If extensions are never used, no need to carry around an empty HashMap.
    // That's 3 words. Instead, this is only 1 word.
    map: Option&lt;Box&lt;AnyMap&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>http::extentions::Extensions::get方法，根据typeid 获取对应的type.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    pub fn get&lt;T: Send + Sync + 'static&gt;(&amp;self) -&gt; Option&lt;&amp;T&gt; {
        self.map
            .as_ref()
            .and_then(|map| map.get(&amp;TypeId::of::&lt;T&gt;()))
            .and_then(|boxed| (&amp;**boxed as &amp;(dyn Any + 'static)).downcast_ref())
    }
<span class="boring">}
</span></code></pre></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<h3 id="cookies"><a class="header" href="#cookies">Cookies</a></h3>
<p><img src="axum/./dots/cookies.svg" alt="" /></p>
<h3 id="oauth"><a class="header" href="#oauth">OAuth</a></h3>
<p><img src="axum/./dots/oauth.svg" alt="" /></p>
<h3 id="multipart"><a class="header" href="#multipart">Multipart</a></h3>
<p><img src="axum/./dots/multi_parts.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hyper"><a class="header" href="#hyper">Hyper</a></h1>
<h2 id="make_service_fn"><a class="header" href="#make_service_fn">make_service_fn</a></h2>
<p><img src="axum/./dots/hyper.svg" alt="" /></p>
<h2 id="server"><a class="header" href="#server">Server</a></h2>
<p><img src="axum/./dots/hyper_server.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tokio"><a class="header" href="#tokio">tokio</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executor-1"><a class="header" href="#executor-1">Executor</a></h1>
<p>Executor中主要有<code>Executor</code>, <code>TypedExecutor</code>, <code>enter</code>, <code>DefaultExecutor</code>, <code>Park</code></p>
<ol>
<li>
<p><code>Executor</code>, <code>TypedExecutor</code>主要作用是spawn future，转换为相应的任务，然后去执行该任务，不断的poll future,直到future complete.</p>
</li>
<li>
<p><code>DefaultExecutor</code>作用，是将<code>tokio::spawn</code>的future转给当前默认的executor.</p>
</li>
<li>
<p><code>enter</code>　用于阻止在当前executor context中，再start一个executor</p>
</li>
<li>
<p><code>park</code>　是对线程<code>block/unblock</code>操作的抽象.</p>
</li>
</ol>
<p>原文如下（摘自tokio-executor/src/lib.rs)</p>
<ul>
<li>
<p>The [<code>Executor</code>] trait spawns future object onto an executor.</p>
</li>
<li>
<p>The [<code>TypedExecutor</code>] trait spawns futures of a specific type onto an
executor. This is used to be generic over executors that spawn futures
that are either <code>Send</code> or <code>!Send</code> or implement executors that apply to
specific futures.</p>
</li>
<li>
<p>[<code>enter</code>] marks that the current thread is entering an execution
context. This prevents a second executor from accidentally starting from
within the context of one that is already running.</p>
</li>
<li>
<p>[<code>DefaultExecutor</code>] spawns tasks onto the default executor for the current
context.</p>
</li>
<li>
<p>[<code>Park</code>] abstracts over blocking and unblocking the current thread.</p>
</li>
</ul>
<h2 id="executor-impl"><a class="header" href="#executor-impl">Executor impl</a></h2>
<p>实现Executor接口的主要有current thread，task executor, default executor还有thread pool的executor.</p>
<p><img src="tokio/./executor.svg" alt="executor" /></p>
<h3 id="defaultexecutor"><a class="header" href="#defaultexecutor">DefaultExecutor</a></h3>
<p>DefaultExecutor 扮演了入口的角色，会将spawn调用转发给thread local storage var的Executor;
<img src="tokio/./default-executor.svg" alt="default-executor" /></p>
<h3 id="current-thread"><a class="header" href="#current-thread">current thread</a></h3>
<p>current thread executor 是单线程的executor。task spwan和execute是在同一线程上完成的。</p>
<p>代码中Entered和Borrow的作用是干啥的不太明白，感觉这块代码有点绕.</p>
<p>Entered和Borrow定义如下:</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A `CurrentThread` instance bound to a supplied execution context.
pub struct Entered&lt;'a, P: Park&gt; {
    executor: &amp;'a mut CurrentThread&lt;P&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// This is mostly split out to make the borrow checker happy.
struct Borrow&lt;'a, U&gt; {
    id: u64,
    scheduler: &amp;'a mut Scheduler&lt;U&gt;,
    num_futures: &amp;'a atomic::AtomicUsize,
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./current-thread-executor.svg" alt="current-thread-executor" /></p>
<h3 id="thread-pool-sender"><a class="header" href="#thread-pool-sender">thread pool sender</a></h3>
<p>thread pool的sender使用future创建相应的task, 然后调用pool的<code>submit_external</code>提交任务</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    fn spawn(
        &amp;mut self,
        future: Pin&lt;Box&lt;dyn Future&lt;Output = ()&gt; + Send&gt;&gt;,
    ) -&gt; Result&lt;(), SpawnError&gt; {
        self.prepare_for_spawn()?;

        // At this point, the pool has accepted the future, so schedule it for
        // execution.

        // Create a new task for the future
        let task = Arc::new(Task::new(future));

        // Call `submit_external()` in order to place the task into the global
        // queue. This way all workers have equal chance of running this task,
        // which means IO handles will be assigned to reactors more evenly.
        self.pool.submit_external(task, &amp;self.pool);

        Ok(())
    }
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./thread_pool_sender.svg" alt="thread-pool-sender" /></p>
<h2 id="executor-setup"><a class="header" href="#executor-setup">Executor setup</a></h2>
<p>thread local var <code>EXECUTOR</code>的设置过程</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>thread_local! {
    /// Thread-local tracking the current executor
    static EXECUTOR: Cell&lt;State&gt; = Cell::new(State::Empty)
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./executor-setup.svg" alt="executor-setup" /></p>
<p>在调用<code>tokio::spawn</code>时，会通过DefaultExecutor调用相应的Thread local storage中设置好的Executor</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//tokio-executor/src/global.rs
pub fn spawn&lt;T&gt;(future: T)
where
    T: Future&lt;Output = ()&gt; + Send + 'static,
{
    DefaultExecutor::current().spawn(Box::pin(future)).unwrap()
}
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//tokio-executor/src/global.rs
impl DefaultExecutor {
    #[inline]
    fn with_current&lt;F: FnOnce(&amp;mut dyn Executor) -&gt; R, R&gt;(f: F) -&gt; Option&lt;R&gt; {
        EXECUTOR.with(
            |current_executor| match current_executor.replace(State::Active) {
                State::Ready(executor_ptr) =&gt; {
                    let executor = unsafe { &amp;mut *executor_ptr };
                    let result = f(executor);
                    current_executor.set(State::Ready(executor_ptr));
                    Some(result)
                }
                State::Empty | State::Active =&gt; None,
            },
        )
    }
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="park"><a class="header" href="#park">park</a></h1>
<p>park是对当前线程block和unblock操作的抽象, 和std的park/unpark操作来比，在线程被blocked的时候，可以去调用一些定制化的功能。</p>
<h2 id="park-impl"><a class="header" href="#park-impl">Park impl</a></h2>
<p><img src="tokio/./park.svg" alt="park" /></p>
<h3 id="reactor-park"><a class="header" href="#reactor-park">Reactor Park</a></h3>
<p>Reactor 相关数据结构如下, 
<img src="tokio/./reactor-park-struct.svg" alt="reactor-park-struct" /></p>
<p>Par接口的park/unpark操作主要依赖于mio的poll和SetReadness。
<img src="tokio/./reactor-park.svg" alt="reactor-park" /></p>
<h3 id="thread-pool-default-park"><a class="header" href="#thread-pool-default-park">Thread pool default park</a></h3>
<p>线程池的default park主要依赖于croess beam的park和unpark</p>
<p><img src="tokio/./threadpool_default_park.svg" alt="threadpool_default_park" /></p>
<h3 id="parkthread"><a class="header" href="#parkthread">ParkThread</a></h3>
<p>数据结构之间关系</p>
<p><img src="tokio/./park-thread-struct.svg" alt="park-thread-struct" /></p>
<p>接口调用关系</p>
<p><img src="tokio/./park-thread.svg" alt="park-thread" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tokio-thread-pool"><a class="header" href="#tokio-thread-pool">tokio thread pool</a></h1>
<h2 id="schedule"><a class="header" href="#schedule">schedule</a></h2>
<p>tokio 使用了crossbeam中的Queue, Stealer, Worker等来实现线程池，其中觉得有意思的地方时work stealing策略</p>
<p>每个task被分给worker的过程如下：有个pool.queue作为全局的task队列入口每次spawn task都会将task push到pool.queue中</p>
<p>worker run函数取task的逻辑如下：</p>
<ol>
<li>从自己的worker队列中去取任务.</li>
<li>如果自己队列中没任务，则从全局队列中，获取一批任务。</li>
<li>如果全局队列中也没任务，则随机的从其他的worker中steal一批任务。</li>
</ol>
<p>这样做的好处是，降低对全局队列的频繁加锁等操作，而且有steal机制，使得task可以比较均匀的被调度。</p>
<h3 id="task-spawn"><a class="header" href="#task-spawn">task spawn</a></h3>
<p>task 从spawn到最后run的过程：</p>
<p><img src="tokio/./worker-steal.svg" alt="worker-steal" /></p>
<h3 id="task-wake"><a class="header" href="#task-wake">task wake</a></h3>
<p><img src="tokio/./task-wake.svg" alt="task-wake" /></p>
<h3 id="worker-sleep"><a class="header" href="#worker-sleep">worker sleep</a></h3>
<p>worker在sleep时候，会把自己push到pool的sleep_stack上, entry中的park/unpark负责线程的sleep和wake.</p>
<p><img src="tokio/./worker-sleep.svg" alt="worker sleep" /></p>
<h3 id="worker-run"><a class="header" href="#worker-run">worker run</a></h3>
<p><img src="tokio/./worker-run.svg" alt="worker-run" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tokio-driver"><a class="header" href="#tokio-driver">tokio driver</a></h1>
<p>Driver 简单来说，就是io event事件触发后，找到相应等待的task, 然后调用预设好的回调函数.</p>
<p>tokio中事件驱动主要靠<code>mio::poll</code>, 在像mio::register中注册一个event时，会带上一个token(token是在tokio中生成的）, driver根据该token建立到SchduleIo的映射，event触发的时候，就会调用schedulIo中预先定义好的方法。
然后事件被触发的时候,mio会把这个token带过来。</p>
<h2 id="task---mio-event"><a class="header" href="#task---mio-event">task &lt;-&gt; mio event</a></h2>
<p>task和mio event通过token 建立关系，回调函数waker通过过Context包装, 传递给future的poll函数，当future需要等待某个事件时候，就会把事件和context关联起来。然后等事件被触发了，就调用context中预先设置好的waker.</p>
<p><img src="tokio/./task-token-event.svg" alt="task-token-event" /></p>
<h2 id="主要数据结构"><a class="header" href="#主要数据结构">主要数据结构</a></h2>
<p><img src="tokio/./reactor-park-struct.svg" alt="reactor-park-struct" /></p>
<p><code>reactor::inner</code>中的<code>io_dispatch</code>表，用于记录事件token到ScheduleIO的一个映射关系.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//reactor.rs
pub(super) struct Inner {
    /// The underlying system event queue.
    io: mio::Poll,

    /// ABA guard counter
    next_aba_guard: AtomicUsize,

    /// Dispatch slabs for I/O and futures events
    pub(super) io_dispatch: RwLock&lt;Slab&lt;ScheduledIo&gt;&gt;,

    /// Used to wake up the reactor from a call to `turn`
    wakeup: mio::SetReadiness,
}

<span class="boring">}
</span></code></pre></pre>
<p>ScheduledIo, 主要用于指向context </p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub(super) struct ScheduledIo {
    aba_guard: usize,
    pub(super) readiness: AtomicUsize,
    pub(super) reader: AtomicWaker,
    pub(super) writer: AtomicWaker,
}

<span class="boring">}
</span></code></pre></pre>
<p>Context 中的waker则定义了如何唤醒task, 对于threadpool 会去调用Task::Schedule方法，而对于current thread, 则会去调用Node.Notify</p>
<h2 id="context-注册过程"><a class="header" href="#context-注册过程">context 注册过程</a></h2>
<p>首先ctx会在task run时候，被创建，然后传递给future_poll, 经过层层的poll_ready 之类的，注册到<code>Reactor::inner::io_dipatch</code>表中
注册的key会在<code>Reactor::inner::add_source</code>计算出来，然后传递给mio的register函数。</p>
<p>然后mio的poll函数在事件发生时，会将该token带上，在Reactor::dispatch中根据token找到相应的contex waker, 调用对应的wake函数。</p>
<h3 id="thread-pool-中-ctx-waker的创建"><a class="header" href="#thread-pool-中-ctx-waker的创建">thread pool 中 ctx waker的创建</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//threadpool/task/mod.rs
    pub(crate) fn run(me: &amp;Arc&lt;Task&gt;, pool: &amp;Arc&lt;Pool&gt;) -&gt; Run {
    //...
            let waker = task::waker(Arc::new(Waker {
                task: me.clone(),
                pool: pool.clone(),
            }));

            let mut cx = Context::from_waker(&amp;waker);
    //...
    }
<span class="boring">}
</span></code></pre></pre>
<p>其中Waker定义如下, event经过dispatch 后, 最终会调用Task::Schedule.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// threadpool/waker.rs
impl ArcWake for Waker {
    fn wake_by_ref(me: &amp;Arc&lt;Self&gt;) {
        Task::schedule(&amp;me.task, &amp;me.pool);
    }
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="current-thread中ctx-waker的创建"><a class="header" href="#current-thread中ctx-waker的创建">current thread中ctx waker的创建</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    pub fn block_on&lt;F&gt;(&amp;mut self, mut future: F) -&gt; F::Output
    where
        F: Future,
    {
        // Safety: we shadow the original `future`, so it will never move
        // again.
        let mut future = unsafe { Pin::new_unchecked(&amp;mut future) };
        let waker = self.executor.scheduler.waker();
        let mut cx = Context::from_waker(&amp;waker);
        // ... other code
    }
<span class="boring">}
</span></code></pre></pre>
<h3 id="event-token-scheduleio"><a class="header" href="#event-token-scheduleio">event, token, scheduleIO</a></h3>
<p>tokio中通过token将event和scheduleIO关联起来</p>
<h4 id="token到scheduleio"><a class="header" href="#token到scheduleio">token到ScheduleIO</a></h4>
<p>在<code>reactor::inner::add_source</code>中, 会在<code>io_dispatch</code>表中先创建一个ScheduleIO， key为aba_guard, 使用aba_guard计算出一个token, 
最后通过调用mio.register 将token和event关联起来, 这样就建立了ScheduleIO和event之间的关系.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// tokio-net/src/driver/reactor.rs
    pub(super) fn add_source(&amp;self, source: &amp;dyn Evented) -&gt; io::Result&lt;usize&gt; {
        // Get an ABA guard value
        let aba_guard = self.next_aba_guard.fetch_add(1 &lt;&lt; TOKEN_SHIFT, Relaxed);

        let key = {
            // Block to contain the write lock
            let mut io_dispatch = self.io_dispatch.write();

            if io_dispatch.len() == MAX_SOURCES {
                return Err(io::Error::new(
                    io::ErrorKind::Other,
                    &quot;reactor at max \
                     registered I/O resources&quot;,
                ));
            }

            io_dispatch.insert(ScheduledIo {
                aba_guard,
                readiness: AtomicUsize::new(0),
                reader: AtomicWaker::new(),
                writer: AtomicWaker::new(),
            })
        };

        let token = aba_guard | key;
        debug!(&quot;adding I/O source: {}&quot;, token);

        self.io.register(
            source,
            mio::Token(token),
            mio::Ready::all(),
            mio::PollOpt::edge(),
        )?;

        Ok(key)
    }
<span class="boring">}
</span></code></pre></pre>
<h4 id="scheduledio-到context"><a class="header" href="#scheduledio-到context">ScheduledIo 到context,</a></h4>
<p>主要在Registration::inner::register中完成.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span>
<span class="boring">fn main() {
</span>    pub(super) fn register(&amp;self, token: usize, dir: Direction, w: Waker) {
        debug!(&quot;scheduling {:?} for: {}&quot;, dir, token);
        let io_dispatch = self.io_dispatch.read();
        let sched = io_dispatch.get(token).unwrap();

        let (waker, ready) = match dir {
            Direction::Read =&gt; (&amp;sched.reader, !mio::Ready::writable()),
            Direction::Write =&gt; (&amp;sched.writer, mio::Ready::writable()),
        };

        waker.register(w);

        if sched.readiness.load(SeqCst) &amp; ready.as_usize() != 0 {
            waker.wake();
        }
    }
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./task-event-detail.svg" alt="task-event-detail" /></p>
<h3 id="事件分发dispatch"><a class="header" href="#事件分发dispatch">事件分发：dispatch</a></h3>
<p><code>reactor::poll</code>调用<code>mio::poll</code>来轮询是否有事件发生，如果有事件发生，则从mio的event中取出token,</p>
<p>然后调动dispatch, 调用相应的wake函数</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//tokio-net/src/driver/reactor.rs

    #[cfg_attr(feature = &quot;tracing&quot;, tracing::instrument(level = &quot;debug&quot;))]
    fn poll(&amp;mut self, max_wait: Option&lt;Duration&gt;) -&gt; io::Result&lt;()&gt; {
        // Block waiting for an event to happen, peeling out how many events
        // happened.
        match self.inner.io.poll(&amp;mut self.events, max_wait) {
            Ok(_) =&gt; {}
            Err(e) =&gt; return Err(e),
        }

        // Process all the events that came in, dispatching appropriately

        // event count is only used for  tracing instrumentation.
        #[cfg(feature = &quot;tracing&quot;)]
        let mut events = 0;

        for event in self.events.iter() {
            #[cfg(feature = &quot;tracing&quot;)]
            {
                events += 1;
            }
            let token = event.token();
            trace!(event.readiness = ?event.readiness(), event.token = ?token);

            if token == TOKEN_WAKEUP {
                self.inner
                    .wakeup
                    .set_readiness(mio::Ready::empty())
                    .unwrap();
            } else {
                self.dispatch(token, event.readiness());
            }
        }

        trace!(message = &quot;loop process&quot;, events);

        Ok(())
    }
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    fn dispatch(&amp;self, token: mio::Token, ready: mio::Ready) {
        let aba_guard = token.0 &amp; !MAX_SOURCES;
        let token = token.0 &amp; MAX_SOURCES;

        let mut rd = None;
        let mut wr = None;

        // Create a scope to ensure that notifying the tasks stays out of the
        // lock's critical section.
        {
            let io_dispatch = self.inner.io_dispatch.read();

            let io = match io_dispatch.get(token) {
                Some(io) =&gt; io,
                None =&gt; return,
            };

            if aba_guard != io.aba_guard {
                return;
            }

            io.readiness.fetch_or(ready.as_usize(), Relaxed);

            if ready.is_writable() || platform::is_hup(ready) {
                wr = io.writer.take_waker();
            }

            if !(ready &amp; (!mio::Ready::writable())).is_empty() {
                rd = io.reader.take_waker();
            }
        }

        if let Some(w) = rd {
            w.wake();
        }

        if let Some(w) = wr {
            w.wake();
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tokio-io"><a class="header" href="#tokio-io">tokio io</a></h1>
<p>Core I/O abstractions for the Tokio stack.</p>
<p>AsyncRead/AsyncWrite use nonblock IO</p>
<p><strong>non-blocking</strong>. All non-blocking I/O objects must return an error when
bytes are unavailable instead of blocking the current thread.</p>
<p>Would block error to future Not Ready poll</p>
<h2 id="asyncread"><a class="header" href="#asyncread">AsyncRead</a></h2>
<ul>
<li><code>poll_read</code>: Attempt to read from the <code>AsyncRead</code> into <code>buf</code>.</li>
<li><code>poll_read_buf</code>: Pull some bytes from this source into the specified <code>BufMut</code>, returning how many bytes were read.</li>
</ul>
<p>AsyncReadExt An extension trait which adds utility methods to <code>AsyncRead</code> types.</p>
<p>This trait inherits from std::io::Read and indicates that an I/O object is non-blocking. All non-blocking I/O objects must return an error when bytes are unavailable instead of blocking the current thread.</p>
<p><img src="tokio/./async_read.svg" alt="async_read" /></p>
<h2 id="asyncwrite"><a class="header" href="#asyncwrite">AsyncWrite</a></h2>
<ul>
<li><code>poll_write</code>:  Attempt to write bytes from <code>buf</code> into the object.</li>
<li><code>poll_write_buf</code>: Write a <code>Buf</code> into this value, returning how many bytes were written.</li>
<li><code>poll_flush</code>: Attempt to flush the object, ensuring that any buffered data reach their destination.</li>
<li><code>poll_shutdown</code>: Initiates or attempts to shut down this writer, returning success when the I/O connection has completely shut down.</li>
</ul>
<p><img src="tokio/./async_write.svg" alt="asycn_write" /></p>
<h3 id="tcp-stream"><a class="header" href="#tcp-stream">tcp stream</a></h3>
<p>/// An I/O object representing a TCP stream connected to a remote endpoint.</p>
<p><img src="tokio/./tcp_stream_struct.svg" alt="tcp_stream_struct" /></p>
<p><img src="tokio/./tcp_stream.svg" alt="tcp_stream" /></p>
<h2 id="split"><a class="header" href="#split">Split</a></h2>
<p>Split a single value implementing <code>AsyncRead + AsyncWrite</code> into separate
<code>AsyncRead</code> and <code>AsyncWrite</code> handles. 还不是太明白这个地方为啥需要lock ?类似与rw lock？</p>
<p>将一个stream分为reader, write部分，解决需要两次mut引用问题（src, dst)</p>
<p><img src="tokio/./split.svg" alt="split" /></p>
<p>调用<code>poll_read</code>, <code>poll_write</code>都会调用<code>poll_lock</code>, 此处的<code>poll_lock</code>并不会block线程。类似于spin lock。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 类似与Spin Lock.
impl&lt;T&gt; Inner&lt;T&gt; {
    fn poll_lock(&amp;self, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Guard&lt;'_, T&gt;&gt; {
        if !self.locked.compare_and_swap(false, true, Acquire) {
            Poll::Ready(Guard { inner: self })
        } else {
            // Spin... but investigate a better strategy

            ::std::thread::yield_now();
            cx.waker().wake_by_ref();

            Poll::Pending
        }
    }
}

// 用于Mutex 
impl&lt;T&gt; Guard&lt;'_, T&gt; {
    fn stream_pin(&amp;mut self) -&gt; Pin&lt;&amp;mut T&gt; {
        // safety: the stream is pinned in `Arc` and the `Guard` ensures mutual
        // exclusion.
        unsafe { Pin::new_unchecked(&amp;mut *self.inner.stream.get()) }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="copy"><a class="header" href="#copy">Copy</a></h2>
<p>future copy实现了从reader异步write到write逻辑</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;R, W&gt; Future for Copy&lt;'_, R, W&gt;
where
    R: AsyncRead + Unpin + ?Sized,
    W: AsyncWrite + Unpin + ?Sized,
{
    type Output = io::Result&lt;u64&gt;;

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;io::Result&lt;u64&gt;&gt; {
        loop {
            // If our buffer is empty, then we need to read some data to
            // continue.
            if self.pos == self.cap &amp;&amp; !self.read_done {
                let me = &amp;mut *self;
                // 从reader中异步读取n个字节
                let n = ready!(Pin::new(&amp;mut *me.reader).poll_read(cx, &amp;mut me.buf))?;
                if n == 0 {
                    self.read_done = true;
                } else {
                    self.pos = 0;
                    self.cap = n;
                }
            }

            // If our buffer has some data, let's write it out!
            while self.pos &lt; self.cap {
                let me = &amp;mut *self;
                // 异步写n个字节到writer中
                let i = ready!(Pin::new(&amp;mut *me.writer).poll_write(cx, &amp;me.buf[me.pos..me.cap]))?;
                if i == 0 {
                    return Poll::Ready(Err(io::Error::new(
                        io::ErrorKind::WriteZero,
                        &quot;write zero byte into writer&quot;,
                    )));
                } else {
                    self.pos += i;
                    self.amt += i as u64;
                }
            }

            // If we've written al the data and we've seen EOF, flush out the
            // data and finish the transfer.
            // done with the entire transfer.
            if self.pos == self.cap &amp;&amp; self.read_done {
                let me = &amp;mut *self;
                // 最后写完了等待flush
                ready!(Pin::new(&amp;mut *me.writer).poll_flush(cx))?;
                return Poll::Ready(Ok(self.amt));
            }
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="buf-readerwritersream"><a class="header" href="#buf-readerwritersream">buf reader/writer/sream</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="codec"><a class="header" href="#codec">codec</a></h1>
<h2 id="transport"><a class="header" href="#transport">Transport</a></h2>
<p>Codec</p>
<p>This is often known as “framing”: instead of viewing your connections as consisting of just bytes in/bytes out, you view them as “frames” of application data that are received and sent. A framed stream of bytes is often referred to as a “transport”.</p>
<h2 id="encodedecode-trait"><a class="header" href="#encodedecode-trait">Encode/Decode Trait</a></h2>
<p>有点像序列化和反序列化</p>
<p>Encoder</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Encoder {
    /// The type of items consumed by the `Encoder`
    type Item;

    /// The type of encoding errors.
    ///
    /// `FramedWrite` requires `Encoder`s errors to implement `From&lt;io::Error&gt;`
    /// in the interest letting it return `Error`s directly.
    type Error: From&lt;io::Error&gt;;

    /// Encodes a frame into the buffer provided.
    ///
    /// This method will encode `item` into the byte buffer provided by `dst`.
    /// The `dst` provided is an internal buffer of the `Framed` instance and
    /// will be written out when possible.
    fn encode(&amp;mut self, item: Self::Item, dst: &amp;mut BytesMut) -&gt; Result&lt;(), Self::Error&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p>Decoder</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Decoder {
    type Item;
    type Error: From&lt;io::Error&gt;;
    fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;Self::Item&gt;, Self::Error&gt;;
    fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;Self::Item&gt;, Self::Error&gt;;
    fn framed&lt;T: AsyncRead + AsyncWrite + Sized&gt;(self, io: T) -&gt; Framed&lt;T, Self&gt;
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./frame_trait.svg" alt="frame-trait" /></p>
<h2 id="framed"><a class="header" href="#framed">framed</a></h2>
<p>frame write</p>
<p><img src="tokio/./frame_write.svg" alt="frame-write" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="channel"><a class="header" href="#channel">channel</a></h1>
<p>multi producer and single consumer for sendin values between tasks;</p>
<h2 id="data-struct"><a class="header" href="#data-struct">data struct</a></h2>
<p><img src="tokio/./channel.svg" alt="channel" /></p>
<h2 id="function-call"><a class="header" href="#function-call">function call</a></h2>
<p>函数调用</p>
<p><img src="tokio/./channel-call.svg" alt="channel-call" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="task-waker"><a class="header" href="#task-waker">task waker</a></h1>
<p><img src="tokio/./task-waker.svg" alt="task-waker" /></p>
<h2 id="atomic-waker"><a class="header" href="#atomic-waker">atomic waker</a></h2>
<p><code>AtomicWaker</code> is a multi-consumer, single-producer transfer cell. The cell
stores a <code>Waker</code> value produced by calls to <code>register</code> and many threads can
race to take the waker by calling <code>wake</code>.</p>
<p>Because of this, the task will do one of two things.</p>
<ol>
<li>
<p>Observe the application state change that Thread B is waking on. In
this case, it is OK for Thread B's wake to be lost.</p>
</li>
<li>
<p>Call register before attempting to observe the application state. Since
Thread A still holds the <code>wake</code> lock, the call to <code>register</code> will result
in the task waking itself and get scheduled again.</p>
</li>
</ol>
<p><img src="tokio/./atomic-waker-state.svg" alt="atomic-waker-state" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crossbeam"><a class="header" href="#crossbeam">crossbeam</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="skiplist"><a class="header" href="#skiplist">SkipList</a></h1>
<h2 id="skiplist-算法"><a class="header" href="#skiplist-算法">SkipList 算法</a></h2>
<p>SkipList是William Pugh 在 1990论文：Skip Lists: A Probabilistic Alternative to Balanced Trees
中提出的一个数据结构。</p>
<p>最低层level 0的为全链表，level 1层为level 0层的一半，level i层node个数为level (i-1)层的一半，越往上越稀疏。</p>
<p><img src="crossbeam/./dot/Linked_lists_with_additional_pointers.png" alt="" /></p>
<p>插入和查询</p>
<p><img src="crossbeam/./dot/skiplist_insert.png" alt="" /></p>
<h2 id="data-struct-1"><a class="header" href="#data-struct-1">data struct</a></h2>
<p><img src="crossbeam/./dot/skiplist.svg" alt="" /></p>
<h2 id="insert"><a class="header" href="#insert">insert</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rust"><a class="header" href="#rust">rust</a></h1>
<h2 id="books-futures-explained"><a class="header" href="#books-futures-explained">books futures explained</a></h2>
<p>https://cfsamson.github.io/books-futures-explained/0_background_information.html</p>
<h3 id="os-thread-green-threads-callback-based-async-task"><a class="header" href="#os-thread-green-threads-callback-based-async-task">OS Thread, Green Threads, Callback based, Async task</a></h3>
<p>Green threads use the same mechanism as an OS - creating a thread for each task, setting up a stack, saving the CPU's state, and jumping from one task(thread) to another by doing a &quot;context switch&quot;.</p>
<p>async, await, Future, Pin</p>
<p>GreenThread 有点像GO的做法.</p>
<p>GreenThread做法</p>
<ol>
<li>Run some non-blocking code.</li>
<li>Make a blocking call to some external resource.</li>
<li>CPU &quot;jumps&quot; to the &quot;main&quot; thread which schedules a different thread to run and &quot;jumps&quot; to that stack.</li>
<li>Run some non-blocking code on the new thread until a new blocking call or the task is finished.</li>
<li>CPU &quot;jumps&quot; back to the &quot;main&quot; thread, schedules a new thread which is ready to make progress, and &quot;jumps&quot; to that thread.</li>
</ol>
<p>GreenThread DrawBacks</p>
<ol>
<li>The stacks might need to grow. Solving this is not easy and will have a cost.</li>
<li>You need to save the CPU state on every switch.</li>
<li>It's not a zero cost abstraction (Rust had green threads early on and this was one of the reasons they were removed).</li>
<li>Complicated to implement correctly if you want to support many different platforms.</li>
</ol>
<p>去看下GO是怎么解决 1/2的</p>
<p>这个代码可以要仔细研究下
https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(llvm_asm, naked_functions)]
use std::ptr;

const DEFAULT_STACK_SIZE: usize = 1024 * 1024 * 2;
const MAX_THREADS: usize = 4;
static mut RUNTIME: usize = 0;

pub struct Runtime {
    threads: Vec&lt;Thread&gt;,
    current: usize,
}

#[derive(PartialEq, Eq, Debug)]
enum State {
    Available,
    Running,
    Ready,
}

struct Thread {
    id: usize,
    stack: Vec&lt;u8&gt;,
    ctx: ThreadContext,
    state: State,
    task: Option&lt;Box&lt;dyn Fn()&gt;&gt;,
}

#[derive(Debug, Default)]
#[repr(C)]
struct ThreadContext {
    rsp: u64,
    r15: u64,
    r14: u64,
    r13: u64,
    r12: u64,
    rbx: u64,
    rbp: u64,
    thread_ptr: u64,
}

impl Thread {
    fn new(id: usize) -&gt; Self {
        Thread {
            id,
            stack: vec![0_u8; DEFAULT_STACK_SIZE],
            ctx: ThreadContext::default(),
            state: State::Available,
            task: None,
        }
    }
}

impl Runtime {
    pub fn new() -&gt; Self {
        let base_thread = Thread {
            id: 0,
            stack: vec![0_u8; DEFAULT_STACK_SIZE],
            ctx: ThreadContext::default(),
            state: State::Running,
            task: None,
        };

        let mut threads = vec![base_thread];
        threads[0].ctx.thread_ptr = &amp;threads[0] as *const Thread as u64;
        let mut available_threads: Vec&lt;Thread&gt; = (1..MAX_THREADS).map(|i| Thread::new(i)).collect();
        threads.append(&amp;mut available_threads);

        Runtime {
            threads,
            current: 0,
        }
    }

    pub fn init(&amp;self) {
        unsafe {
            let r_ptr: *const Runtime = self;
            RUNTIME = r_ptr as usize;
        }
    }

    pub fn run(&amp;mut self) -&gt; ! {
        while self.t_yield() {}
        std::process::exit(0);
    }

    fn t_return(&amp;mut self) {
        if self.current != 0 {
            self.threads[self.current].state = State::Available;
            self.t_yield();
        }
    }

    fn t_yield(&amp;mut self) -&gt; bool {
        let mut pos = self.current;
        while self.threads[pos].state != State::Ready {
            pos += 1;
            if pos == self.threads.len() {
                pos = 0;
            }
            if pos == self.current {
                return false;
            }
        }

        if self.threads[self.current].state != State::Available {
            self.threads[self.current].state = State::Ready;
        }

        self.threads[pos].state = State::Running;
        let old_pos = self.current;
        self.current = pos;

        unsafe {
           let old: *mut ThreadContext = &amp;mut self.threads[old_pos].ctx;
           let new: *const ThreadContext = &amp;self.threads[pos].ctx;
           llvm_asm!(
               &quot;mov $0, %rdi
                mov $1, %rsi&quot;::&quot;r&quot;(old), &quot;r&quot;(new)
           );
           switch();
       }
        true
    }

    pub fn spawn&lt;F: Fn() + 'static&gt;(f: F){
        unsafe {
            let rt_ptr = RUNTIME as *mut Runtime;
            let available = (*rt_ptr)
                .threads
                .iter_mut()
                .find(|t| t.state == State::Available)
                .expect(&quot;no available thread.&quot;);

            let size = available.stack.len();
            let s_ptr = available.stack.as_mut_ptr().offset(size as isize);
            let s_ptr = (s_ptr as usize &amp; !15) as *mut u8;
            available.task = Some(Box::new(f));
            available.ctx.thread_ptr = available as *const Thread as u64;
            //ptr::write(s_ptr.offset((size - 8) as isize) as *mut u64, guard as u64);
            std::ptr::write(s_ptr.offset(-16) as *mut u64, guard as u64);
            std::ptr::write(s_ptr.offset(-24) as *mut u64, skip as u64);
            std::ptr::write(s_ptr.offset(-32) as *mut u64, call as u64);
            available.ctx.rsp = s_ptr.offset(-32) as u64;
            available.state = State::Ready;
        }
    }
}

fn call(thread: u64) {
    let thread = unsafe { &amp;*(thread as *const Thread) };
    if let Some(f) = &amp;thread.task {
        f();
    }
}

#[naked]
fn skip() { }

fn guard() {
    unsafe {
        let rt_ptr = RUNTIME as *mut Runtime;
        let rt = &amp;mut *rt_ptr;
        println!(&quot;THREAD {} FINISHED.&quot;, rt.threads[rt.current].id);
        rt.t_return();
    };
}

pub fn yield_thread() {
    unsafe {
        let rt_ptr = RUNTIME as *mut Runtime;
        (*rt_ptr).t_yield();
    };
}

#[naked]
#[inline(never)]
unsafe fn switch() {
    llvm_asm!(&quot;
       mov     %rsp, 0x00(%rdi)
       mov     %r15, 0x08(%rdi)
       mov     %r14, 0x10(%rdi)
       mov     %r13, 0x18(%rdi)
       mov     %r12, 0x20(%rdi)
       mov     %rbx, 0x28(%rdi)
       mov     %rbp, 0x30(%rdi)

       mov     0x00(%rsi), %rsp
       mov     0x08(%rsi), %r15
       mov     0x10(%rsi), %r14
       mov     0x18(%rsi), %r13
       mov     0x20(%rsi), %r12
       mov     0x28(%rsi), %rbx
       mov     0x30(%rsi), %rbp
       mov     0x38(%rsi), %rdi
       &quot;
   );
}
#[cfg(not(windows))]
fn main() {
    let mut runtime = Runtime::new();
    runtime.init();
    Runtime::spawn(|| {
        println!(&quot;I haven't implemented a timer in this example.&quot;);
        yield_thread();
        println!(&quot;Finally, notice how the tasks are executed concurrently.&quot;);
    });
    Runtime::spawn(|| {
        println!(&quot;But we can still nest tasks...&quot;);
        Runtime::spawn(|| {
            println!(&quot;...like this!&quot;);
        })
    });
    runtime.run();
}
</code></pre></pre>
<p>promises return a state machine which can be in one of three states: pending, fulfilled or rejected</p>
<p>promise三个状态: pending, fulfilled, rejected</p>
<pre><code class="language-javascript">function timer(ms) {
    return new Promise((resolve) =&gt; setTimeout(resolve, ms));
}

timer(200)
.then(() =&gt; timer(100))
.then(() =&gt; timer(50))
.then(() =&gt; console.log(&quot;I'm the last one&quot;));
</code></pre>
<pre><code>async function run() {
    await timer(200);
    await timer(100);
    await timer(50);
    console.log(&quot;I'm the last one&quot;);
}
</code></pre>
<p>Since promises are re-written as state machines, they also enable an even better syntax which allows us to write our last example like this:</p>
<p>state machines? 为什么 promise可以被rewrite为state machines?</p>
<h2 id="futures"><a class="header" href="#futures">Futures</a></h2>
<p>A future is a representation of some operation which will complete in the future.</p>
<p>Async in Rust uses a Poll based approach, in which an asynchronous task will have three phases.</p>
<ol>
<li>The Poll phase. A Future is polled which results in the task progressing until a point where it can no longer make progress. We often refer to the part of the runtime which polls a Future as an executor.</li>
<li>The Wait phase. An event source, most often referred to as a reactor, registers that a Future is waiting for an event to happen and makes sure that it will wake the Future when that event is ready.</li>
<li>The Wake phase. The event happens and the Future is woken up. It's now up to the executor which polled the Future in step 1 to schedule the future to be polled again and make further progress until it completes or reaches a new point where it can't make further progress and the cycle repeats.</li>
</ol>
<h3 id="leaf-future-non-leaf-future"><a class="header" href="#leaf-future-non-leaf-future">leaf future, non-leaf future</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut stream = tokio::net::TcpStream::connect(&quot;127.0.0.1:3000&quot;);
``

Operations on these resources, like a Read on a socket, will be non-blocking and return a future which we call a leaf future since it's the future which we're actually waiting on.

## non-leaf future

Non-leaf-futures are the kind of futures we as users of a runtime write ourselves using the async keyword to create a task which can be run on the executor.

```rust
let non_leaf = async {
    let mut stream = TcpStream::connect(&quot;127.0.0.1:3000&quot;).await.unwrap();// &lt;- yield
    println!(&quot;connected!&quot;);
    let result = stream.write(b&quot;hello world\n&quot;).await; // &lt;- yield
    println!(&quot;message sent!&quot;);
    ...
};
<span class="boring">}
</span></code></pre></pre>
<p>The key to these tasks is that they're able to yield control to the runtime's scheduler and then resume execution again where it left off at a later point.</p>
<p>yield 地方下次poll时候，会接着执行。 这个是怎么实现的？有点像thread context switch时候保存stack ptr,下次来的时候，接着执行了。</p>
<p>不知道rust是怎么实现的? state每次  async 对应的task结构是？
switch(state) {
case state1:
xxx code
yield: set state to state2
case state2:
xxx code
}</p>
<p>In contrast to leaf futures, these kind of futures do not themselves represent an I/O resource. 
When we poll them they will run until they get to a leaf-future which returns Pending and then yield control to the scheduler (which is a part of what we call the runtime).</p>
<p>Mental Model</p>
<p>A fully working async system in Rust can be divided into three parts:</p>
<p>Reactor
Executor
Future</p>
<p>Reactor 表示最底层事件？</p>
<p>So, how does these three parts work together? 
They do that through an object called the Waker.
The Waker is how the reactor tells the executor that a specific Future is ready to run.
Once you understand the life cycle and ownership of a Waker, you'll understand how futures work from a user's perspective.
Here is the life cycle:</p>
<p>A Waker is created by the executor. 
A common, but not required, method is to create a new Waker for each Future that is registered with the executor.</p>
<p>When a future is registered with an executor, 
it’s given a clone of the Waker object created by the executor.
Since this is a shared object (e.g. an Arc<T>), all clones actually point to the same underlying object.
Thus, anything that calls any clone of the original Waker will wake the particular Future that was registered to it.</p>
<p>The future clones the Waker and passes it to the reactor, which stores it to use later.</p>
<p>Rust 标准库关注的：接口..</p>
<p>What Rust's standard library takes care of</p>
<ol>
<li>
<p>A common interface representing an operation which will be completed in the future through the Future trait.</p>
</li>
<li>
<p>An ergonomic way of creating tasks which can be suspended and resumed through the async and await keywords.</p>
</li>
<li>
<p>A defined interface to wake up a suspended task through the Waker type.</p>
</li>
</ol>
<p>async keyward rewrites our code block to a state machine. Each await point represents a state change.</p>
<p>a waker i spassed into Future::poll, iT wil hang on to thath waker</p>
<p>until it reaches an <code>await</code> point. when it does it will call <code>poll</code> on that future and pass the waker along</p>
<p>We don't actually pass a Waker directly, we pass the waker as a aprt of an object call <code>Context</code> which might add extra 
context to the poll method in the future.</p>
<p>Reactor just caretes an object implementing the <code>Future</code> trait and returns it.</p>
<p>leaf_fut.poll(waker)</p>
<h2 id="trait-object"><a class="header" href="#trait-object">Trait object</a></h2>
<p>fat pointer</p>
<pre><pre class="playground"><code class="language-rust edition2021">use std::mem::size_of;
trait SomeTrait { }

fn main() {
    println!(&quot;======== The size of different pointers in Rust: ========&quot;);
    println!(&quot;&amp;dyn Trait:------{}&quot;, size_of::&lt;&amp;dyn SomeTrait&gt;());
    println!(&quot;&amp;[&amp;dyn Trait]:---{}&quot;, size_of::&lt;&amp;[&amp;dyn SomeTrait]&gt;());
    println!(&quot;Box&lt;Trait&gt;:------{}&quot;, size_of::&lt;Box&lt;SomeTrait&gt;&gt;());
    println!(&quot;Box&lt;Box&lt;Trait&gt;&gt;:-{}&quot;, size_of::&lt;Box&lt;Box&lt;SomeTrait&gt;&gt;&gt;());
    println!(&quot;&amp;i32:------------{}&quot;, size_of::&lt;&amp;i32&gt;());
    println!(&quot;&amp;[i32]:----------{}&quot;, size_of::&lt;&amp;[i32]&gt;());
    println!(&quot;Box&lt;i32&gt;:--------{}&quot;, size_of::&lt;Box&lt;i32&gt;&gt;());
    println!(&quot;&amp;Box&lt;i32&gt;:-------{}&quot;, size_of::&lt;&amp;Box&lt;i32&gt;&gt;());
    println!(&quot;[&amp;dyn Trait;4]:--{}&quot;, size_of::&lt;[&amp;dyn SomeTrait; 4]&gt;());
    println!(&quot;[i32;4]:---------{}&quot;, size_of::&lt;[i32; 4]&gt;());
}
</code></pre></pre>
<p>The layout for a pointer to a trait object looks like this:</p>
<p>The first 8 bytes points to the data for the trait object
The second 8 bytes points to the vtable for the trait object</p>
<p>fat pointer, vtable</p>
<pre><pre class="playground"><code class="language-rust edition2021">
use std::mem::{align_of, size_of};

// A reference to a trait object is a fat pointer: (data_ptr, vtable_ptr)
trait Test {
    fn add(&amp;self) -&gt; i32;
    fn sub(&amp;self) -&gt; i32;
    fn mul(&amp;self) -&gt; i32;
}

// This will represent our home-brewed fat pointer to a trait object
#[repr(C)]
struct FatPointer&lt;'a&gt; {
    /// A reference is a pointer to an instantiated `Data` instance
    data: &amp;'a mut Data,
    /// Since we need to pass in literal values like length and alignment it's
    /// easiest for us to convert pointers to usize-integers instead of the other way around.
    vtable: *const usize,
}

// This is the data in our trait object. It's just two numbers we want to operate on.
struct Data {
    a: i32,
    b: i32,
}

// ====== function definitions ======
fn add(s: &amp;Data) -&gt; i32 {
    s.a + s.b
}
fn sub(s: &amp;Data) -&gt; i32 {
    s.a - s.b
}
fn mul(s: &amp;Data) -&gt; i32 {
    s.a * s.b
}

fn main() {
    let mut data = Data {a: 3, b: 2};
    // vtable is like special purpose array of pointer-length types with a fixed
    // format where the three first values contains some general information like
    // a pointer to drop and the length and data alignment of `data`.
    let vtable = vec![
        0,                  // pointer to `Drop` (which we're not implementing here)
        size_of::&lt;Data&gt;(),  // length of data
        align_of::&lt;Data&gt;(), // alignment of data

        // we need to make sure we add these in the same order as defined in the Trait.
        add as usize, // function pointer - try changing the order of `add`
        sub as usize, // function pointer - and `sub` to see what happens
        mul as usize, // function pointer
    ];

    let fat_pointer = FatPointer { data: &amp;mut data, vtable: vtable.as_ptr()};
    let test = unsafe { std::mem::transmute::&lt;FatPointer, &amp;dyn Test&gt;(fat_pointer) };

    // And voalá, it's now a trait object we can call methods on
    println!(&quot;Add: 3 + 2 = {}&quot;, test.add());
    println!(&quot;Sub: 3 - 2 = {}&quot;, test.sub());
    println!(&quot;Mul: 3 * 2 = {}&quot;, test.mul());
}
</code></pre></pre>
<p>std::mem::transmute</p>
<p><a href="https://github.com/rust-lang/rfcs/blob/master/text/2033-experimental-coroutines.md">rust generator的 RFC</a></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async]
fn print_lines() -&gt; io::Result&lt;()&gt; {
    let addr = &quot;127.0.0.1:8080&quot;.parse().unwrap();
    let tcp = await!(TcpStream::connect(&amp;addr))?;
    let io = BufReader::new(tcp);

    #[async]
    for line in io.lines() {
        println!(&quot;{}&quot;, line);
    }

    Ok(())
}


fn print_lines() -&gt; impl Future&lt;Item = (), Error = io::Error&gt; {
    lazy(|| {
        let addr = &quot;127.0.0.1:8080&quot;.parse().unwrap();
        TcpStream::connect(&amp;addr).and_then(|tcp| {
            let io = BufReader::new(tcp);

            io.lines().for_each(|line| {
                println!(&quot;{}&quot;, line);
                Ok(())
            })
        })
    })
}
<span class="boring">}
</span></code></pre></pre>
<p>State machines as &quot;stackless coroutines&quot;</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn print_lines() -&gt; impl Future&lt;Item = (), Error = io::Error&gt; {
    CoroutineToFuture(|| {
        let addr = &quot;127.0.0.1:8080&quot;.parse().unwrap();
        let tcp = {
            let mut future = TcpStream::connect(&amp;addr);
            loop {
                match future.poll() {
                    Ok(Async::Ready(e)) =&gt; break Ok(e),
                    Ok(Async::NotReady) =&gt; yield, //这块的yield, 怎么记住state , 下次进来怎么resume ?
                    Err(e) =&gt; break Err(e),
                }
            }
        }?;

        let io = BufReader::new(tcp);

        let mut stream = io.lines();
        loop {
            let line = {
                match stream.poll()? {
                    Async::Ready(Some(e)) =&gt; e,
                    Async::Ready(None) =&gt; break,
                    Async::NotReady =&gt; {
                        yield;
                        continue
                    }
                }
            };
            println!(&quot;{}&quot;, line);
        }

        Ok(())
    })
}
<span class="boring">}
</span></code></pre></pre>
<p>yield 关键字:
the most prominent addition here is the usage of yield keywords. These are inserted here to inform the compiler that the coroutine should be suspended for later resumption</p>
<p>问题: Coroutine::resume是怎么实现的？</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CoroutineToFuture&lt;T&gt;(T);

impl&lt;T: Coroutine&gt; Future for CoroutineToFuture {
    type Item = T::Item;
    type Error = T::Error;

    fn poll(&amp;mut self) -&gt; Poll&lt;T::Item, T::Error&gt; {
    //不知道Coroutine::resume 这个是怎么实现的
        match Coroutine::resume(&amp;mut self.0) {
            CoroutineStatus::Return(Ok(result)) =&gt; Ok(Async::Ready(result)),
            CoroutineStatus::Return(Err(e)) =&gt; Err(e),
            CoroutineStatus::Yield =&gt; Ok(Async::NotReady),
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>设计要点</p>
<ol>
<li>No implicit memory allocation</li>
<li>Coroutines are translated to state machines internally by the compiler</li>
<li>The standard library has the traits/types necessary to support the coroutines language feature.</li>
</ol>
<blockquote>
<p>As a result, coroutines will roughly compile down to a state machine that's advanced forward as its resumed. Whenever a coroutine yields it'll leave itself in a state that can be later resumed from the yield statement.
这个是怎么实现的呢？</p>
</blockquote>
<h2 id="yield关键字"><a class="header" href="#yield关键字">yield关键字</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(generators, generator_trait)]
use std::ops::{Generator, GeneratorState};

fn main() {
    let a: i32 = 4;
    let mut gen = move || {
        println!(&quot;Hello&quot;);
        yield a * 2;
        println!(&quot;world!&quot;);
    };

    if let GeneratorState::Yielded(n) = gen.resume() {
        println!(&quot;Got value {}&quot;, n);
    }

    if let GeneratorState::Complete(()) = gen.resume() {
        ()
    };
}
</code></pre></pre>
<p>https://tmandry.gitlab.io/blog/posts/optimizing-await-1/</p>
<p>std::mem::replace 这个类似于c里面的memcp ?</p>
<p>std::mem::replace(self, GeneratorA::Exit</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="go"><a class="header" href="#go">go</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-pgm-schedule"><a class="header" href="#runtime-pgm-schedule">Runtime PGM Schedule</a></h1>
<h2 id="pgm-concept"><a class="header" href="#pgm-concept">PGM concept:</a></h2>
<pre><code class="language-go">// 摘自src/runtime/proc.go
// G - goroutine.
// M - worker thread, or machine.
// P - processor, a resource that is required to execute Go code.
//     M must have an associated P to execute Go code, however it can be
//     blocked or in a syscall w/o an associated P.
</code></pre>
<p>三者struct之间的引用关系如下：</p>
<p><img src="golang/./pgm-struct.svg" alt="pgm-struct" /></p>
<h2 id="work-stealing-scheduler"><a class="header" href="#work-stealing-scheduler">Work stealing scheduler</a></h2>
<p>Golang中的PGM采用类似于tokio的thread pool executor.  采用了worksteal的形式, 一方面降低了对global队列的锁的竞争。
另一方面每个G(go routine) 生成的go routine优先放到proc的local 队列里面，优先由同一个线程执行，比较好的增加了局部性。</p>
<p><img src="golang/./pgm-work-stealing.svg" alt="pgm-work-stealing" /></p>
<h2 id="processor创建"><a class="header" href="#processor创建">processor创建</a></h2>
<p><img src="golang/./processor.svg" alt="processor" /></p>
<h2 id="machine-worker-thread线程创建"><a class="header" href="#machine-worker-thread线程创建">machine worker thread线程创建</a></h2>
<p><img src="golang/./m-os-thread.svg" alt="m-os-thread" /></p>
<h2 id="status"><a class="header" href="#status">Status</a></h2>
<h3 id="goroutine"><a class="header" href="#goroutine">Goroutine</a></h3>
<p><img src="golang/./goroutine-status.svg" alt="goroutine-status" /></p>
<h3 id="proc"><a class="header" href="#proc">Proc</a></h3>
<p><img src="golang/./proc-status.svg" alt="proc-status" /></p>
<h2 id="sysmon"><a class="header" href="#sysmon">sysmon</a></h2>
<p><img src="golang/./sysmon.svg" alt="sysmon" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="goroutine-stack"><a class="header" href="#goroutine-stack">Goroutine Stack</a></h1>
<h2 id="goroutine-switch"><a class="header" href="#goroutine-switch">goroutine switch</a></h2>
<p><img src="golang/./goroutine-stack-switch.svg" alt="goroutine-stack-switch" /></p>
<h3 id="mcall"><a class="header" href="#mcall">mcall</a></h3>
<p>mcall 保存被切换gorutine信息，并在当前线程g0 goroutine上执行新的func</p>
<p><img src="golang/./mcall.svg" alt="mcall" /></p>
<pre><code>// func mcall(fn func(*g))
// Switch to m-&gt;g0's stack, call fn(g).
// Fn must never return. It should gogo(&amp;g-&gt;sched)
// to keep running g.
TEXT runtime·mcall(SB), NOSPLIT, $0-8
	MOVQ	fn+0(FP), DI

	get_tls(CX)
	MOVQ	g(CX), AX	// save state in g-&gt;sched
	MOVQ	0(SP), BX	// caller's PC
	MOVQ	BX, (g_sched+gobuf_pc)(AX)
	LEAQ	fn+0(FP), BX	// caller's SP
	MOVQ	BX, (g_sched+gobuf_sp)(AX)
	MOVQ	AX, (g_sched+gobuf_g)(AX)
	MOVQ	BP, (g_sched+gobuf_bp)(AX)

	// switch to m-&gt;g0 &amp; its stack, call fn
	MOVQ	g(CX), BX
	MOVQ	g_m(BX), BX
	MOVQ	m_g0(BX), SI
	CMPQ	SI, AX	// if g == m-&gt;g0 call badmcall
	JNE	3(PC)
	MOVQ	$runtime·badmcall(SB), AX
	JMP	AX
	MOVQ	SI, g(CX)	// g = m-&gt;g0
	MOVQ	(g_sched+gobuf_sp)(SI), SP	// sp = m-&gt;g0-&gt;sched.sp
	PUSHQ	AX
	MOVQ	DI, DX
	MOVQ	0(DI), DI
	CALL	DI
	POPQ	AX
	MOVQ	$runtime·badmcall2(SB), AX
	JMP	AX
	RET
</code></pre>
<h3 id="gogo"><a class="header" href="#gogo">gogo</a></h3>
<p>gogo 用来从gobuf中恢复协程执行状态，并跳转到上一次指令处继续执行</p>
<pre><code class="language-go">// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
TEXT runtime·gogo(SB), NOSPLIT, $16-8
	MOVQ	buf+0(FP), BX		// gobuf
	MOVQ	gobuf_g(BX), DX
	MOVQ	0(DX), CX		// make sure g != nil
	get_tls(CX)
	MOVQ	DX, g(CX)
	MOVQ	gobuf_sp(BX), SP	// restore SP
	MOVQ	gobuf_ret(BX), AX
	MOVQ	gobuf_ctxt(BX), DX
	MOVQ	gobuf_bp(BX), BP
	MOVQ	$0, gobuf_sp(BX)	// clear to help garbage collector
	MOVQ	$0, gobuf_ret(BX)
	MOVQ	$0, gobuf_ctxt(BX)
	MOVQ	$0, gobuf_bp(BX)
	MOVQ	gobuf_pc(BX), BX
	JMP	BX
</code></pre>
<h3 id="gosave"><a class="header" href="#gosave">gosave</a></h3>
<p>gosave感觉和cgo相关，这个代码还没怎么搞明白</p>
<p><img src="golang/./gosave.svg" alt="gosave" /></p>
<pre><code>// func gosave(buf *gobuf)
// save state in Gobuf; setjmp
TEXT runtime·gosave(SB), NOSPLIT, $0-8
	MOVQ	buf+0(FP), AX		// gobuf
	LEAQ	buf+0(FP), BX		// caller's SP
	MOVQ	BX, gobuf_sp(AX)
	MOVQ	0(SP), BX		// caller's PC
	MOVQ	BX, gobuf_pc(AX)
	MOVQ	$0, gobuf_ret(AX)
	MOVQ	BP, gobuf_bp(AX)
	// Assert ctxt is zero. See func save.
	MOVQ	gobuf_ctxt(AX), BX
	TESTQ	BX, BX
	JZ	2(PC)
	CALL	runtime·badctxt(SB)
	get_tls(CX)
	MOVQ	g(CX), BX
	MOVQ	BX, gobuf_g(AX)
	RET
</code></pre>
<h2 id="stack增长"><a class="header" href="#stack增长">Stack增长</a></h2>
<p>编译器在每个函数调用中都会插入对morestack的调用。</p>
<p>morestack会检查当前栈空间是否够用，不够用的话，会调用newstack增长空间.
newstack 会分配2倍大小的stack, copy过去, 并将指向该stack的引用指针也修改过去。</p>
<p><img src="golang/./morestack.svg" alt="morestack" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory分配"><a class="header" href="#memory分配">Memory分配</a></h1>
<h2 id="struct之间引用关系"><a class="header" href="#struct之间引用关系">struct之间引用关系</a></h2>
<p><img src="golang/./mem-struct.svg" alt="mem-struct" /></p>
<pre><code class="language-go">//src/runtime/malloc.go
//	fixalloc: a free-list allocator for fixed-size off-heap objects,
//		used to manage storage used by the allocator.
//	mheap: the malloc heap, managed at page (8192-byte) granularity.
//	mspan: a run of in-use pages managed by the mheap.
//	mcentral: collects all spans of a given size class.
//	mcache: a per-P cache of mspans with free space.
//	mstats: allocation statistics.
</code></pre>
<ol>
<li>fixalloc 用于分配mspan等固定大小的object</li>
<li>mheap 用于8KB page粒度内存管理</li>
<li>mspan: 一段连续的pages,用于分配制定specClass的object.</li>
<li>mcentral: 所有span的list</li>
<li>mcache: 线程的span cache, 优先从cache中分配, 避免每次访问heap需要lock.</li>
</ol>
<p>下图摘自<a href="golang/blog.learngoprogramming.com/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed">1</a>
比较清楚的画出了这几者之间的层级关系</p>
<p><img src="golang/./golang-mem-overview.png" alt="golang-mem-overview" /></p>
<h2 id="mspan"><a class="header" href="#mspan">mspan</a></h2>
<p>mspan的创建路径如下</p>
<p><img src="golang/./mspan-create.svg" alt="mspan-create" /></p>
<h2 id="ref"><a class="header" href="#ref">Ref</a></h2>
<ol>
<li><a href="https://blog.learngoprogramming.com/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed">A visual guide to Go Memory Allocator from scratch</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gc"><a class="header" href="#gc">GC</a></h1>
<h2 id="gcphase"><a class="header" href="#gcphase">GcPhase</a></h2>
<ol>
<li><code>_GCoff</code>:  GC not running; sweeping in background, write barrier disabled</li>
<li><code>_GCmark</code>: GC marking roots and workbufs: allocate black, write barrier ENABLED</li>
<li><code>_GCmarktermination</code>: GC mark termination: allocate black, P's help GC, write barrier ENABLED</li>
</ol>
<p>如下图所示，GC过程中开启了两次STW(stop the world),　第一次主要为parepare阶段，
第二次为Marktermination阶段:</p>
<p><img src="golang/./gcphase.svg" alt="gcphase" /></p>
<pre><code class="language-go">//go:nosplit
func setGCPhase(x uint32) {
	atomic.Store(&amp;gcphase, x)
	writeBarrier.needed = gcphase == _GCmark || gcphase == _GCmarktermination
	writeBarrier.enabled = writeBarrier.needed || writeBarrier.cgo
}
</code></pre>
<h2 id="mark-phase"><a class="header" href="#mark-phase">Mark Phase</a></h2>
<p>Golang中是如何根据指针找到对象，以及该对象所引用的对象的？答案是根据heap Arena中bitmap存储的元信息。
对于Arena中每个word, bitmap使用了两个bit，来标识该word是否是指针，以及该word是否已被扫描过。</p>
<pre><code class="language-go">type heapArena struct {
	// bitmap stores the pointer/scalar bitmap for the words in
  // this arena
	bitmap [heapArenaBitmapBytes]byte
	spans [pagesPerArena]*mspan
	pageInUse [pagesPerArena / 8]uint8
	pageMarks [pagesPerArena / 8]uint8
	zeroedBase uintptr
}
</code></pre>
<p><img src="golang/./heapbits.svg" alt="heapbits" /></p>
<p>另外每个span中有allocBits和gcmarkbits用来标记span中每个slot是否被分配。在mallocgc中会使用该信息，找到可分配的slot,
另外在gc sweep阶段根据coutAlloc()==0 来判断mspan是否是空闲的，可以被回收.</p>
<pre><code class="language-go">//go:nosplit
// 返回一个指针在heapArena中的bits位
func heapBitsForAddr(addr uintptr) (h heapBits) {
	// 2 bits per word, 4 pairs per byte, and a mask is hard coded.
	arena := arenaIndex(addr)
	ha := mheap_.arenas[arena.l1()][arena.l2()]
	// The compiler uses a load for nil checking ha, but in this
	// case we'll almost never hit that cache line again, so it
	// makes more sense to do a value check.
	if ha == nil {
		// addr is not in the heap. Return nil heapBits, which
		// we expect to crash in the caller.
		return
	}
	h.bitp = &amp;ha.bitmap[(addr/(sys.PtrSize*4))%heapArenaBitmapBytes]
	h.shift = uint32((addr / sys.PtrSize) &amp; 3)
	h.arena = uint32(arena)
	h.last = &amp;ha.bitmap[len(ha.bitmap)-1]
	return
}
</code></pre>
<pre><code class="language-go">func (s *mspan) markBitsForIndex(objIndex uintptr) markBits {
	bytep, mask := s.gcmarkBits.bitp(objIndex)
	return markBits{bytep, mask, objIndex}
}

// bitp returns a pointer to the byte containing bit n and a mask for
// selecting that bit from *bytep.
func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {
	return b.bytep(n / 8), 1 &lt;&lt; (n % 8)
}
</code></pre>
<h3 id="并发标记"><a class="header" href="#并发标记">并发标记</a></h3>
<p><img src="golang/./gcmark.svg" alt="gcmark" /></p>
<h4 id="writebarrier"><a class="header" href="#writebarrier">WriteBarrier</a></h4>
<pre><code class="language-go">	a := new(A)
	a.c = new(C)
</code></pre>
<p>混合写屏障<a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">1</a>
这里的shade就是将白色对象放入待扫描队列中(wbBuf)</p>
<pre><code>writePointer(slot, ptr):
    shade(*slot)
    if current stack is grey:
        shade(ptr)
    *slot = ptr
</code></pre>
<p>编译器注入的writeBarrier</p>
<pre><code class="language-go">	0x0059 00089 (test.go:14)	CMPL	runtime.writeBarrier(SB), $0
	0x0060 00096 (test.go:14)	JEQ	100
	0x0062 00098 (test.go:14)	JMP	115
	0x0064 00100 (test.go:14)	MOVQ	AX, (DI)
	0x0067 00103 (test.go:14)	JMP	105
	0x0069 00105 (test.go:15)	PCDATA	$0, $0
	0x0069 00105 (test.go:15)	PCDATA	$1, $0
	0x0069 00105 (test.go:15)	MOVQ	56(SP), BP
	0x006e 00110 (test.go:15)	ADDQ	$64, SP
	0x0072 00114 (test.go:15)	RET
	0x0073 00115 (test.go:14)	PCDATA	$0, $-2
	0x0073 00115 (test.go:14)	PCDATA	$1, $-2
	0x0073 00115 (test.go:14)	CALL	runtime.gcWriteBarrier(SB)
	0x0078 00120 (test.go:14)	JMP	105
</code></pre>
<h4 id="scanobject"><a class="header" href="#scanobject">scanobject</a></h4>
<p>scanobject:根据bitmap信息，判断是否是指针，是否已扫描过。
如果是指针的话，查找指针对应的object, 并加到队列里面（标记为灰色）
这样下次gcDrain会从队列中去取，接着循环的扫描。。</p>
<pre><code class="language-go">// scanobject scans the object starting at b, adding pointers to gcw.
// b must point to the beginning of a heap object or an oblet.
// scanobject consults the GC bitmap for the pointer mask and the
// spans for the size of the object.
//
//go:nowritebarrier
func scanobject(b uintptr, gcw *gcWork) {
	// Find the bits for b and the size of the object at b.
	//
	// b is either the beginning of an object, in which case this
	// is the size of the object to scan, or it points to an
	// oblet, in which case we compute the size to scan below.
	hbits := heapBitsForAddr(b)
	s := spanOfUnchecked(b)
  //...
			if s.spanclass.noscan() {
				// Bypass the whole scan.
				gcw.bytesMarked += uint64(n)
				return
			}

	var i uintptr
	for i = 0; i &lt; n; i += sys.PtrSize {
		// Find bits for this word.
		if i != 0 {
			// Avoid needless hbits.next() on last iteration.
			hbits = hbits.next()
		}
		// Load bits once. See CL 22712 and issue 16973 for discussion.
		bits := hbits.bits()
		// During checkmarking, 1-word objects store the checkmark
		// in the type bit for the one word. The only one-word objects
		// are pointers, or else they'd be merged with other non-pointer
		// data into larger allocations.
		if i != 1*sys.PtrSize &amp;&amp; bits&amp;bitScan == 0 {
			break // no more pointers in this object
		}
		if bits&amp;bitPointer == 0 {
			continue // not a pointer
		}

		// Work here is duplicated in scanblock and above.
		// If you make changes here, make changes there too.
		obj := *(*uintptr)(unsafe.Pointer(b + i))

		// At this point we have extracted the next potential pointer.
		// Quickly filter out nil and pointers back to the current object.
		if obj != 0 &amp;&amp; obj-b &gt;= n {
			// Test if obj points into the Go heap and, if so,
			// mark the object.
			//
			// Note that it's possible for findObject to
			// fail if obj points to a just-allocated heap
			// object because of a race with growing the
			// heap. In this case, we know the object was
			// just allocated and hence will be marked by
			// allocation itself.
			if obj, span, objIndex := findObject(obj, b, i); obj != 0 {
				greyobject(obj, b, i, span, gcw, objIndex)
			}
		}
	}
  //...
}
</code></pre>
<h2 id="sweep-phase"><a class="header" href="#sweep-phase">Sweep Phase</a></h2>
<p><img src="golang/./go-sweep.svg" alt="gcsweep" /></p>
<h3 id="scavenging"><a class="header" href="#scavenging">scavenging</a></h3>
<p>go1.13之后改为更智能的内存归还给os<a href="https://github.com/golang/go/issues/30333">2</a></p>
<p><img src="golang/./scavenge.svg" alt="scavege" /></p>
<h2 id="ref-1"><a class="header" href="#ref-1">Ref</a></h2>
<ol>
<li><a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">Proposal: Eliminate STW stack re-scanning</a></li>
<li><a href="https://github.com/golang/go/issues/30333">Proposal: Smarter Scavenging</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-1"><a class="header" href="#context-1">Context</a></h1>
<h3 id="context-struct之间关系"><a class="header" href="#context-struct之间关系">Context struct之间关系</a></h3>
<p><img src="golang/./context-struct.svg" alt="context-struct" /></p>
<h3 id="context-example"><a class="header" href="#context-example">Context example</a></h3>
<pre><code class="language-go">// This example demonstrates the use of a cancelable context to prevent a
// goroutine leak. By the end of the example function, the goroutine started
// by gen will return without leaking.
func ExampleWithCancel() {
	// gen generates integers in a separate goroutine and
	// sends them to the returned channel.
	// The callers of gen need to cancel the context once
	// they are done consuming generated integers not to leak
	// the internal goroutine started by gen.
	gen := func(ctx context.Context) &lt;-chan int {
		dst := make(chan int)
		n := 1
		go func() {
			for {
				select {
				case &lt;-ctx.Done():
					return // returning not to leak the goroutine
				case dst &lt;- n:
					n++
				}
			}
		}()
		return dst
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel() // cancel when we are finished consuming integers

	for n := range gen(ctx) {
		fmt.Println(n)
		if n == 5 {
			break
		}
	}
	// Output:
	// 1
	// 2
	// 3
	// 4
	// 5
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="defer-recover-panic"><a class="header" href="#defer-recover-panic">defer, recover, panic</a></h1>
<ol>
<li>每个defer语句生成的defer结构会插到队首，defer执行时从defer link list头开始执行.所以defer以LIFO顺序执行。</li>
<li>每个return语句会编译器会插入deferreturn。</li>
<li>在panic中会调用defer 链表中的函数，然后在defer中可以recover, 也可以接着panic.</li>
</ol>
<h2 id="defer"><a class="header" href="#defer">defer</a></h2>
<p><img src="golang/./defer.svg" alt="defer" /></p>
<h3 id="defer-语句"><a class="header" href="#defer-语句">defer 语句</a></h3>
<p>每个defer语句会转换成对deferproc的调用.</p>
<pre><code class="language-go">// Calls the function n using the specified call type.
// Returns the address of the return value (or nil if none).
func (s *state) call(n *Node, k callKind) *ssa.Value {
//...
		switch {
		case k == callDefer:
			call = s.newValue1A(ssa.OpStaticCall, types.TypeMem, deferproc, s.mem())
      ...
}
</code></pre>
<p>deferproc 会新建一个<code>_defer</code>结构的struct, 并插到当前goroutine的<code>_defer</code>列表队头</p>
<pre><code class="language-go">// Create a new deferred function fn with siz bytes of arguments.
// The compiler turns a defer statement into a call to this.
//go:nosplit
func deferproc(siz int32, fn *funcval) { // arguments of fn follow fn
	if getg().m.curg != getg() {
		// go code on the system stack can't defer
		throw(&quot;defer on system stack&quot;)
	}

	// the arguments of fn are in a perilous state. The stack map
	// for deferproc does not describe them. So we can't let garbage
	// collection or stack copying trigger until we've copied them out
	// to somewhere safe. The memmove below does that.
	// Until the copy completes, we can only call nosplit routines.
	sp := getcallersp()
	argp := uintptr(unsafe.Pointer(&amp;fn)) + unsafe.Sizeof(fn)
	callerpc := getcallerpc()

	d := newdefer(siz)
	if d._panic != nil {
		throw(&quot;deferproc: d.panic != nil after newdefer&quot;)
	}
	d.fn = fn
	d.pc = callerpc
	d.sp = sp
	switch siz {
	case 0:
		// Do nothing.
	case sys.PtrSize:
		*(*uintptr)(deferArgs(d)) = *(*uintptr)(unsafe.Pointer(argp))
	default:
		memmove(deferArgs(d), unsafe.Pointer(argp), uintptr(siz))
	}

	// deferproc returns 0 normally.
	// a deferred func that stops a panic
	// makes the deferproc return 1.
	// the code the compiler generates always
	// checks the return value and jumps to the
	// end of the function if deferproc returns != 0.
	return0()
	// No code can go here - the C return register has
	// been set and must not be clobbered.
}
</code></pre>
<p>其中return0的定义如下</p>
<pre><code>TEXT runtime·return0(SB), NOSPLIT, $0
	MOVL	$0, AX
	RET
</code></pre>
<p>compiler生成的代码会检查ax寄存器的值。</p>
<h3 id="defer函数的调用"><a class="header" href="#defer函数的调用">defer函数的调用</a></h3>
<p>编译器在函数的return RET指令后面加入deferreturn的调用.</p>
<pre><code>func fa() {
		defer fmt.Printf(&quot;hello&quot;)
}
</code></pre>
<pre><code class="language-go">&quot;&quot;.fa STEXT size=106 args=0x0 locals=0x48
	0x0000 00000 (test.go:7)	TEXT	&quot;&quot;.fa(SB), ABIInternal, $72-0
	0x0000 00000 (test.go:7)	MOVQ	(TLS), CX
	0x0009 00009 (test.go:7)	CMPQ	SP, 16(CX)
	0x000d 00013 (test.go:7)	JLS	99
	0x000f 00015 (test.go:7)	SUBQ	$72, SP
	0x0013 00019 (test.go:7)	MOVQ	BP, 64(SP)
	0x0018 00024 (test.go:7)	LEAQ	64(SP), BP
	0x001d 00029 (test.go:7)	FUNCDATA	$0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
	0x001d 00029 (test.go:7)	FUNCDATA	$1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
	0x001d 00029 (test.go:7)	FUNCDATA	$2, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB)
	0x001d 00029 (test.go:8)	PCDATA	$0, $0
	0x001d 00029 (test.go:8)	PCDATA	$1, $0
	0x001d 00029 (test.go:8)	MOVL	$0, &quot;&quot;..autotmp_1+8(SP)
	0x0025 00037 (test.go:8)	PCDATA	$0, $1
	0x0025 00037 (test.go:8)	LEAQ	&quot;&quot;.fa.func1·f(SB), AX
	0x002c 00044 (test.go:8)	PCDATA	$0, $0
	0x002c 00044 (test.go:8)	MOVQ	AX, &quot;&quot;..autotmp_1+32(SP)
	0x0031 00049 (test.go:8)	PCDATA	$0, $1
	0x0031 00049 (test.go:8)	LEAQ	&quot;&quot;..autotmp_1+8(SP), AX
	0x0036 00054 (test.go:8)	PCDATA	$0, $0
	0x0036 00054 (test.go:8)	MOVQ	AX, (SP)
	0x003a 00058 (test.go:8)	CALL	runtime.deferprocStack(SB)
  // 如果deferprocStack返回值不为０,则调到末尾执行deferreturn
	0x003f 00063 (test.go:8)	TESTL	AX, AX
	0x0041 00065 (test.go:8)	JNE	83
	0x0043 00067 (test.go:11)	XCHGL	AX, AX
	0x0044 00068 (test.go:11)	CALL	runtime.deferreturn(SB)
	0x0049 00073 (test.go:11)	MOVQ	64(SP), BP
	0x004e 00078 (test.go:11)	ADDQ	$72, SP
	0x0052 00082 (test.go:11)	RET
	0x0053 00083 (test.go:8)	XCHGL	AX, AX
	0x0054 00084 (test.go:8)	CALL	runtime.deferreturn(SB)
	0x0059 00089 (test.go:8)	MOVQ	64(SP), BP
	0x005e 00094 (test.go:8)	ADDQ	$72, SP
	0x0062 00098 (test.go:8)	RET
	0x0063 00099 (test.go:8)	NOP
	0x0063 00099 (test.go:7)	PCDATA	$1, $-1
	0x0063 00099 (test.go:7)	PCDATA	$0, $-1
	0x0063 00099 (test.go:7)	CALL	runtime.morestack_noctxt(SB)
	0x0068 00104 (test.go:7)	JMP	0
</code></pre>
<p>deferreturn 会调用jmpdefer，不断的执行defer link中的fn</p>
<pre><code class="language-go">// func jmpdefer(fv *funcval, argp uintptr)
// argp is a caller SP.
// called from deferreturn.
// 1. pop the caller
// 2. sub 5 bytes from the callers return
// 3. jmp to the argument
TEXT runtime·jmpdefer(SB), NOSPLIT, $0-16
	MOVQ	fv+0(FP), DX	// fn
	MOVQ	argp+8(FP), BX	// caller sp
	LEAQ	-8(BX), SP	// caller sp after CALL
	MOVQ	-8(SP), BP	// restore BP as if deferreturn returned (harmless if framepointers not in use)
	SUBQ	$5, (SP)	// return to CALL again
	MOVQ	0(DX), BX
	JMP	BX	// but first run the deferred function
</code></pre>
<h2 id="panic"><a class="header" href="#panic">panic</a></h2>
<p><img src="golang/./panic.svg" alt="panic" /></p>
<p>在panic中会调用当前goroutine的defer 函数，在这些defer函数中也可能会有panic，所有每个goroutine也有个panic的link list。</p>
<p>如果在defer中调用了recover, 那么goroutine会从derfer的sp,pc处接着执行，否则就进入fatalpanic，打印堆栈，最后<code>exit(2)</code></p>
<pre><code class="language-go">func gopanic(e interface{}) {
  //other code
	var p _panic
	p.arg = e
	p.link = gp._panic
	gp._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p)))

	atomic.Xadd(&amp;runningPanicDefers, 1)

	for {
		d := gp._defer
		if d == nil {
			break
		}

		// If defer was started by earlier panic or Goexit (and, since we're back here, that triggered a new panic),
		// take defer off list. The earlier panic or Goexit will not continue running.
		if d.started {
			if d._panic != nil {
				d._panic.aborted = true
			}
			d._panic = nil
			d.fn = nil
			gp._defer = d.link
			freedefer(d)
			continue
		}

		// Mark defer as started, but keep on list, so that traceback
		// can find and update the defer's argument frame if stack growth
		// or a garbage collection happens before reflectcall starts executing d.fn.
		d.started = true

		// Record the panic that is running the defer.
		// If there is a new panic during the deferred call, that panic
		// will find d in the list and will mark d._panic (this panic) aborted.
		d._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p)))

		p.argp = unsafe.Pointer(getargp(0))
		reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz))
		p.argp = nil

		// reflectcall did not panic. Remove d.
		if gp._defer != d {
			throw(&quot;bad defer entry in panic&quot;)
		}
		d._panic = nil
		d.fn = nil
		gp._defer = d.link

		// trigger shrinkage to test stack copy. See stack_test.go:TestStackPanic
		//GC()

		pc := d.pc
		sp := unsafe.Pointer(d.sp) // must be pointer so it gets adjusted during stack copy
		freedefer(d)
		if p.recovered {
			atomic.Xadd(&amp;runningPanicDefers, -1)

			gp._panic = p.link
			// Aborted panics are marked but remain on the g.panic list.
			// Remove them from the list.
			for gp._panic != nil &amp;&amp; gp._panic.aborted {
				gp._panic = gp._panic.link
			}
			if gp._panic == nil { // must be done with signal
				gp.sig = 0
			}
			// Pass information about recovering frame to recovery.
			gp.sigcode0 = uintptr(sp)
			gp.sigcode1 = pc
			mcall(recovery)
			throw(&quot;recovery failed&quot;) // mcall should not return
		}
	}

	// ran out of deferred calls - old-school panic now
	// Because it is unsafe to call arbitrary user code after freezing
	// the world, we call preprintpanics to invoke all necessary Error
	// and String methods to prepare the panic strings before startpanic.
	preprintpanics(gp._panic)

	fatalpanic(gp._panic) // should not return
	*(*int)(nil) = 0      // not reached
}
</code></pre>
<p>recovery, 这个地方将ret值改为了1</p>
<pre><code class="language-go">func recovery(gp *g) {
	// Info about defer passed in G struct.
	sp := gp.sigcode0
	pc := gp.sigcode1

	// d's arguments need to be in the stack.
	if sp != 0 &amp;&amp; (sp &lt; gp.stack.lo || gp.stack.hi &lt; sp) {
		print(&quot;recover: &quot;, hex(sp), &quot; not in [&quot;, hex(gp.stack.lo), &quot;, &quot;, hex(gp.stack.hi), &quot;]\n&quot;)
		throw(&quot;bad recovery&quot;)
	}

	// Make the deferproc for this d return again,
	// this time returning 1.  The calling function will
	// jump to the standard return epilogue.
	gp.sched.sp = sp
	gp.sched.pc = pc
	gp.sched.lr = 0
	gp.sched.ret = 1
	gogo(&amp;gp.sched)
}
</code></pre>
<h2 id="ref-2"><a class="header" href="#ref-2">Ref:</a></h2>
<ol>
<li>https://tiancaiamao.gitbooks.io/go-internals/content/zh/03.4.html</li>
<li>https://blog.learngoprogramming.com/gotchas-of-defer-in-go-1-8d070894cb01</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leveldb"><a class="header" href="#leveldb">leveldb</a></h1>
<p>记录一些学习leveldb源码笔记</p>
<pre><code>https://github.com/google/leveldb
</code></pre>
<h2 id="struct-and-alg"><a class="header" href="#struct-and-alg">struct and alg</a></h2>
<h3 id="skiplist-1"><a class="header" href="#skiplist-1">skiplist</a></h3>
<p>skiplist 应用的非常广泛，O(log n)的查找复杂度, O(log n)的查找复杂度, 下图是wiki上找到的示意图</p>
<p><img src="leveldb/./images/800px-Skip_list.svg.png" alt="skiplist" /></p>
<p><img src="leveldb/./images/800px-Skip_list_add_element-en.gif" alt="sliplist insert" /></p>
<h1 id="ref-3"><a class="header" href="#ref-3">ref</a></h1>
<p>[1] https://en.wikipedia.org/wiki/Skip_list</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="draft"><a class="header" href="#draft">Draft</a></h1>
<h3 id="数据结构之间引用关系"><a class="header" href="#数据结构之间引用关系">数据结构之间引用关系</a></h3>
<ol>
<li>Cache</li>
<li>Table</li>
<li>VersionSet</li>
<li>Env</li>
</ol>
<p><img src="leveldb/./dbinterface.svg" alt="dbInterface" /></p>
<h3 id="db-get"><a class="header" href="#db-get">DB Get</a></h3>
<p><img src="leveldb/./db-get.svg" alt="db-get" /></p>
<h3 id="db-put"><a class="header" href="#db-put">DB Put</a></h3>
<p><img src="leveldb/./db-put.svg" alt="db-put" /></p>
<h3 id="db-compact"><a class="header" href="#db-compact">DB Compact</a></h3>
<p><img src="leveldb/./db-compact.svg" alt="db-compact" /></p>
<h3 id="table-builder"><a class="header" href="#table-builder">Table builder</a></h3>
<p>memtable写入文件过程</p>
<p><img src="leveldb/./table-builder.svg" alt="table-builder" /></p>
<p>table format</p>
<ol>
<li>restart point的作用是啥？</li>
</ol>
<p><img src="leveldb/./table-format.svg" alt="table-format" /></p>
<h2 id="versionset"><a class="header" href="#versionset">VersionSet</a></h2>
<p><img src="leveldb/./version.svg" alt="versionset" /></p>
<h3 id="manifest文件"><a class="header" href="#manifest文件">Manifest文件</a></h3>
<h3 id="versionedit"><a class="header" href="#versionedit">VersionEdit</a></h3>
<h2 id="todo"><a class="header" href="#todo">TODO:</a></h2>
<h2 id="wal日志"><a class="header" href="#wal日志">WAL日志</a></h2>
<h2 id="iterator"><a class="header" href="#iterator">Iterator</a></h2>
<h2 id="bloom-filter"><a class="header" href="#bloom-filter">Bloom Filter</a></h2>
<h2 id="ref-4"><a class="header" href="#ref-4">Ref</a></h2>
<ol>
<li><a href="https://github.com/google/leveldb/blob/master/doc/table_format.md">table format</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/51360281">LevelDB设计与实现 - 读写流程</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="代码及模块间关系"><a class="header" href="#代码及模块间关系">代码及模块间关系</a></h1>
<p><img src="leveldb/code-struct-overview.svg" alt="code-struct-overview" /></p>
<h2 id="具体细节"><a class="header" href="#具体细节">具体细节</a></h2>
<p><img src="leveldb/./dbinterface.svg" alt="dbInterface" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leveldb-write-流程"><a class="header" href="#leveldb-write-流程">LevelDB Write 流程</a></h1>
<h3 id="数据写入流程"><a class="header" href="#数据写入流程">数据写入流程</a></h3>
<p>leveldb中数据写入流程如下:</p>
<ol>
<li>首先会将kv batch写入日志中，如果宕机了，能从日志中恢复过来，由于采用顺序写的方式，速度很快。</li>
<li>确保memtable的空间足够（没有超过一定大小限制），如果memtable没足够空间了，会新建一个memtable, 并将老的memtable转为
immtable，然后由后台压缩线程将immtable写入到level 0 文件。如果level 0 文件个数超过限制，也会触发background 压缩线程。</li>
<li>将kv batch插入memtable中, memtable的底层实现为skiplist, 插入时间复杂度为O(Log(n)),每个key,value插入都有自己的sequnceNumber.
用来控制版本号.</li>
</ol>
<p><img src="leveldb/./db-put-overview.svg" alt="db-put-overview" /></p>
<h3 id="写入细节"><a class="header" href="#写入细节">写入细节</a></h3>
<ol>
<li>由MakeRoomsForWrite来保证memtable空间足够写入新的kv，如果immtable正在等待被写到文件中，或者level0文件个数超过阈值了，则需要阻塞等待后台线程处理完毕。由<code>backgroup_work_finished_signal_</code>condvar控制。</li>
<li>多线程写入时候，有个<code>writes_</code>队列做并发控制, writes_队列也使用condvar来控制，<code>writes_</code>队列开头的writer写完后，触发condvar，下个writer线程接着写。</li>
<li>immtable由<code>CompactMemtable</code>写入level 0文件</li>
<li>后台线程压缩时候，先使用<code>PickCompaction</code>选择需要合并压缩的sstable文件，然后使用<code>DoCompactionWork</code>做归并排序合并。</li>
<li>每次写入都会更新versionSet的LastSequnceNumber，用于版本控制,Sequnce越大，表明key,value值越新。</li>
</ol>
<p><img src="leveldb/./db-put.svg" alt="db-put" /></p>
<h3 id="wal-日志写入"><a class="header" href="#wal-日志写入">WAL 日志写入</a></h3>
<p><img src="leveldb/./write_batch_internal.svg" alt="write_batch" /></p>
<h3 id="wal-日志恢复"><a class="header" href="#wal-日志恢复">WAL 日志恢复</a></h3>
<p><img src="leveldb/./wal-log-recover.svg" alt="wal-log-recover" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leveldb-read流程"><a class="header" href="#leveldb-read流程">LevelDB Read流程</a></h1>
<h3 id="数据读取流程"><a class="header" href="#数据读取流程">数据读取流程</a></h3>
<ol>
<li>根据key和options中snapshot 拼接为looupKey</li>
<li>现在MemTable中查找，然后再immutable中查找，最后到level文件中查找。</li>
<li>通过version中的<code>files_</code>可以获得当前version中所有level file的列表。</li>
<li>level0中的文件key range有重叠，所有要每个文件都搜索。</li>
<li>其他level的通过fileMeta中记录的key range，定位到相应的sstable file.</li>
<li>文件操作：先从cache中查找，是否sstable的datablock index和bloomfilter已在内存中，如果不在的话，加载这些到内存中。cache以LRU方式来更新，淘汰。</li>
<li>先从datablock index中定位到相应的datablock和bloom filter,通过bloom filter快速查看key是否不存在，避免不必要的文件操作。</li>
<li>文件操作: 读取datablock到内存中，做二分查找, 将datablock放到缓存中。</li>
</ol>
<p><img src="leveldb/./db-get-overview.svg" alt="db-get-overview" /></p>
<h3 id="读取细节"><a class="header" href="#读取细节">读取细节</a></h3>
<p>涉及到的模块说明:</p>
<ol>
<li>VersonSet负责维护version信息。</li>
<li>每个version中的<code>file_</code>数据成员，维护了每个层级的FileMetaData.</li>
<li>每个FileMetadata记录了该file的最大值和最小值，方便查找key,value时候，快速定位。</li>
<li>TableCache封装了Table和LRUCache逻辑。</li>
<li>Table封装了table加载，查找等逻辑。</li>
</ol>
<p><img src="leveldb/./db-get.svg" alt="db-get" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sstable-文件格式和读写"><a class="header" href="#sstable-文件格式和读写">SSTable 文件格式和读写</a></h1>
<h2 id="table-format"><a class="header" href="#table-format">Table format</a></h2>
<p>table文件分为Foot,metadataindex, dataIndex, metadat block, datablock这几块。</p>
<ol>
<li>Footer 48个字节，以Magic number为结尾。存储了指向metaDataIndex和DataIndex的BlockHandle（offset, size)</li>
<li>MetaBlock存储了bloomfilter 相关数据</li>
<li>DataIndexBlock 存储了每个block的lastKey，value为Datablock的blockHanle(offset和size)</li>
<li>MetaIndexBlock 中也是key,value形式，key为 <code>filter.filter_policy_name</code>，value为filterblockHandle, 当前只有bloomFilter</li>
<li>RestartPoint用于记录key shared共同前缀开始的位置。</li>
<li>每个DataBlock/IndexBlock除了原始数据，还包含了compressType(是否压缩）以及CRC32用于校验。</li>
</ol>
<p><img src="leveldb/./table-format.svg" alt="table-format" /></p>
<h2 id="table-write-流程"><a class="header" href="#table-write-流程">Table write 流程</a></h2>
<p><img src="leveldb/./table-builder.svg" alt="table-builder" /></p>
<h2 id="table-读取流程"><a class="header" href="#table-读取流程">Table 读取流程</a></h2>
<p>TableOpen中会读取文件的Footer, 读取indexBlock以及解析Metadatablock.</p>
<p><img src="leveldb/./table-read.svg" alt="Table-read" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="versionset和manifest"><a class="header" href="#versionset和manifest">Versionset和Manifest</a></h1>
<h2 id="manifest文件写入"><a class="header" href="#manifest文件写入">Manifest文件写入</a></h2>
<p>version记录了当前每个level的各个文件的FileMetadata.</p>
<p>在压缩时候，每个level的FileMetadata可能会更改, 这种修改是用VersionEdit来表示的, 每次修改会将VersionEdit Encode写入日志中, 方便崩溃时候能够从Manifest日志文件中恢复。</p>
<p><img src="leveldb/./versionset-edit.svg" alt="versionset" /></p>
<h2 id="recover"><a class="header" href="#recover">Recover</a></h2>
<p>Current文件内容记录了当前的manifest文件, 在DBOpen时候会去加载Mainfest文件，然后读取每个versionEditRecord
将它Decode为VersionEdit，然后一个个的apply，最终得到最后的version, 最后加入到VersionSet中。</p>
<p><img src="leveldb/./versionset-recover.svg" alt="versionset" /></p>
<p>遗留问题：
SequenceNumber和FilNumber这些是怎么保存的？</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compact"><a class="header" href="#compact">Compact</a></h1>
<h3 id="pickcompaction"><a class="header" href="#pickcompaction">PickCompaction</a></h3>
<p>选择要合并compact的FileMetaData</p>
<p><img src="leveldb/./pick-compaction.svg" alt="pick-compaction" /></p>
<h3 id="多路归并compact"><a class="header" href="#多路归并compact">多路归并Compact</a></h3>
<p>将选择好的FilemetaData合并，输出到level+1层。通过versionEdit更改Version.</p>
<p><img src="leveldb/./do-compaction.svg" alt="db-compact" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iterator-迭代器"><a class="header" href="#iterator-迭代器">Iterator 迭代器</a></h1>
<h2 id="iterator-继承关系"><a class="header" href="#iterator-继承关系">Iterator 继承关系</a></h2>
<p><img src="leveldb/./iterator.svg" alt="iterator" /></p>
<h2 id="blockiter"><a class="header" href="#blockiter">BlockIter</a></h2>
<p>Table中某个BlockData数据块的iter, 这里面有意思的restartPointer, restartPointer指向的record, key没有共享部分。
所以Seek时候先通过SeekToRestartPoint，找到合适的RestartPoint点，然后再使用ParseNextKey迭代遍历。</p>
<p><img src="leveldb/./block_iter.svg" alt="iterator" /></p>
<h2 id="levelfilenumiterator"><a class="header" href="#levelfilenumiterator">LevelFileNumIterator</a></h2>
<p>用于遍历一组FileMetadat中定位target所在的FileMeta Index, 在TwoLevelIterator中作为index iter使用。</p>
<p><img src="leveldb/./level_file_num_iterator.svg" alt="level_file_num_iterator" /></p>
<h2 id="twoleveliterator"><a class="header" href="#twoleveliterator">TwoLevelIterator</a></h2>
<p>双层迭代器，先通过index找到对应的block，调用<code>block_function</code>创建相应的block iterator.
增加了SkipEmptyData检查，当一个blockIter迭代完后，自动切换到下一个block iter.</p>
<p>TwoLevelIterator可以套娃:</p>
<ol>
<li>IndexBlockIter和DataBlockIter套在一起得到一个TableIterator</li>
<li>LevelFileNumIterator和TableIterator套在一次，得到某一层的Iterator.</li>
</ol>
<p><img src="leveldb/./two_level_iterator.svg" alt="two_level_iterator" /></p>
<h2 id="mergingiterator"><a class="header" href="#mergingiterator">MergingIterator</a></h2>
<p>归并N个有序的iterator.</p>
<p><img src="leveldb/./merging_iterator.svg" alt="merging_iterator" /></p>
<h2 id="dbimplnewiterator"><a class="header" href="#dbimplnewiterator">DBImpl::NewIterator</a></h2>
<ol>
<li>内存中的mm_和imm_分别作为一路iter，放到merging iter中</li>
<li>level0层由于table文件之间有overlap的，所以每个level0对应tableIterator作为一路放在merging itertor中。</li>
<li>level1 ~ levenN层 LevelFileumIterator和TableIterator通过TwoLevelIterator套在一起，得到某一层的iterator.</li>
</ol>
<p><img src="leveldb/./dbimpl-iterator.svg" alt="dbimpl-newIterator" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bloom-filter-1"><a class="header" href="#bloom-filter-1">Bloom filter</a></h1>
<h2 id="filer-policy"><a class="header" href="#filer-policy">filer policy</a></h2>
<p>leveldb中filter用于快速确定key是否不在table中, 一堆key经过一系列的hash计算后，可以得到
很小指纹数据。查询时候，可以根据这个指纹信息，快速排除key不存在的情况。</p>
<p><img src="leveldb/./bloom-filter.svg" alt="bloom-filter" /></p>
<p>计算keys对应的指纹数据：</p>
<pre><code class="language-cpp">for (int i = 0; i &lt; n; i++) {
  // Use double-hashing to generate a sequence of hash values.
  // See analysis in [Kirsch,Mitzenmacher 2006].
  uint32_t h = BloomHash(keys[i]);
  const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15);  // Rotate right 17 bits
  for (size_t j = 0; j &lt; k_; j++) {
    const uint32_t bitpos = h % bits;
    array[bitpos / 8] |= (1 &lt;&lt; (bitpos % 8));
    h += delta;
  }
</code></pre>
<p>match过程:</p>
<pre><code class="language-cpp">uint32_t h = BloomHash(key);
const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15);  // Rotate right 17 bits
for (size_t j = 0; j &lt; k; j++) {
  const uint32_t bitpos = h % bits;
  if ((array[bitpos / 8] &amp; (1 &lt;&lt; (bitpos % 8))) == 0) return false;
  h += delta;
}
return true;
</code></pre>
<h2 id="filter数据写入和读取流程"><a class="header" href="#filter数据写入和读取流程">filter数据写入和读取流程</a></h2>
<h3 id="写入流程"><a class="header" href="#写入流程">写入流程</a></h3>
<p>每个table的block数据的filter数据是写在一块的，通过一个<code>filter_offsets</code>来保存每个datablock对应的filter
在整个filter数据中的偏移和大小。</p>
<p>TableBuilder时候，每次开始新的一个datablock，都会调用filter的start new block， 然后Add Key，value时候，调用
AddKey, 创建key的指纹数据。</p>
<p>最后Table finish时候，写入filter data block数据，并且在metaindexblock中添加filter_policy_name和filter data block handle</p>
<h3 id="读取流程"><a class="header" href="#读取流程">读取流程</a></h3>
<p>每个talbe Get时候，会使用ReadFilter加载该table的所有filterdata, 然后根据blockData的offset 找到该block对应的
filter数据，并使用该数据来判断key是不是不存在。</p>
<p><img src="leveldb/./filter-policy.svg" alt="filter-policy" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rocksdb"><a class="header" href="#rocksdb">RocksDB</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="draft-1"><a class="header" href="#draft-1">Draft</a></h1>
<h2 id="class之间关系"><a class="header" href="#class之间关系">Class之间关系</a></h2>
<p><img src="rocksdb/./class-relations.svg" alt="class-relations" /></p>
<h2 id="write"><a class="header" href="#write">Write</a></h2>
<ol>
<li>最终怎么写到了memTable中。</li>
<li>WAL写的流程是什么样？</li>
</ol>
<p><img src="rocksdb/./write.svg" alt="write" /></p>
<h2 id="writebatch"><a class="header" href="#writebatch">WriteBatch</a></h2>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<h2 id="columnfamily"><a class="header" href="#columnfamily">ColumnFamily</a></h2>
<ol>
<li>Blob 中value和key是怎么对的上的？</li>
<li>数据结构之间怎么串起来的。 </li>
</ol>
<h2 id="write-thread"><a class="header" href="#write-thread">Write Thread</a></h2>
<p>Writer的状态
<img src="rocksdb/./write_thread_state.svg" alt="write thread state" /></p>
<p>write thread过程
Write group leader 负责写入WAL日志。
memtable可能由group leader写，也有可能由各个writer 并发写。</p>
<p>write thread是对写线程的抽象
<img src="rocksdb/./write_thread.svg" alt="write thread" /></p>
<p>write impl
<img src="rocksdb/./pipline_writeimpl.svg" alt="pipelined-write impl" /></p>
<h2 id="preprocesswrite"><a class="header" href="#preprocesswrite">PreprocessWrite</a></h2>
<p><img src="rocksdb/./preprocess_write.svg" alt="preprocess write" /></p>
<h2 id="后台压缩"><a class="header" href="#后台压缩">后台压缩</a></h2>
<p>MaybeScheduleFlushOrCompaction</p>
<p><img src="rocksdb/./flush_compaction.svg" alt="flush-compaction" /></p>
<p>后台线程压缩</p>
<p>compaction job之间是怎么划分的？怎么让不同线程去compact不同部分？</p>
<p><img src="rocksdb/./background-compaction.svg" alt="backgroup-compaction" /></p>
<h2 id="compaction-picker"><a class="header" href="#compaction-picker">compaction picker</a></h2>
<h2 id="level-compaction-picker"><a class="header" href="#level-compaction-picker">level compaction picker</a></h2>
<p>以下两张图摘自facebook wiki <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction">leveled-compaction</a></p>
<p><img src="rocksdb/./pre_l0_compaction.png" alt="level 0 compaction " /></p>
<p><img src="rocksdb/./pre_l1_compaction.png" alt="level 1 compaction" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="column-family"><a class="header" href="#column-family">Column Family</a></h1>
<ol>
<li>每个columnFamily有单独的Version, memtable以及imm memtable list</li>
<li>VersionStorageInfo 存储了属于该version的所有Filemetadata信息</li>
<li>读取时候，先从columnFaimly的memTable，然后imm list，然后version 中的各个level的文件</li>
<li>写时候，先写WAl日志，然后插入到memtable中，memtable在满时候，会转到imm list中, 然后由
后台线程flush到level0, 后台线程compact.</li>
</ol>
<p>rocks db中主要数据结构关系如下：</p>
<p><img src="rocksdb/./column-family-overview.svg" alt="column family overview" /></p>
<p>数据结构之间引用细节如下：</p>
<p><img src="rocksdb/./column_family.svg" alt="column family" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-ahead-log"><a class="header" href="#write-ahead-log">Write Ahead Log</a></h1>
<h3 id="writebatch-1"><a class="header" href="#writebatch-1">WriteBatch</a></h3>
<p>put/delete等操作先写入writeBatch中</p>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<p>writeBatch中Record类型如下:</p>
<pre><code class="language-cpp">// WriteBatch::rep_ :=
//    sequence: fixed64
//    count: fixed32
//    data: record[count]
</code></pre>
<pre><code class="language-cpp">enum ValueType : unsigned char {
  kTypeDeletion = 0x0,
  kTypeValue = 0x1,
  kTypeMerge = 0x2,
  kTypeLogData = 0x3,               // WAL only.
  kTypeColumnFamilyDeletion = 0x4,  // WAL only.
  kTypeColumnFamilyValue = 0x5,     // WAL only.
  kTypeColumnFamilyMerge = 0x6,     // WAL only.
  kTypeSingleDeletion = 0x7,
  kTypeColumnFamilySingleDeletion = 0x8,  // WAL only.
  kTypeBeginPrepareXID = 0x9,             // WAL only.
  kTypeEndPrepareXID = 0xA,               // WAL only.
  kTypeCommitXID = 0xB,                   // WAL only.
  kTypeRollbackXID = 0xC,                 // WAL only.
  kTypeNoop = 0xD,                        // WAL only.
  kTypeColumnFamilyRangeDeletion = 0xE,   // WAL only.
  kTypeRangeDeletion = 0xF,               // meta block
  kTypeColumnFamilyBlobIndex = 0x10,      // Blob DB only
  kTypeBlobIndex = 0x11,                  // Blob DB only
  // When the prepared record is also persisted in db, we use a different
  // record. This is to ensure that the WAL that is generated by a WritePolicy
  // is not mistakenly read by another, which would result into data
  // inconsistency.
  kTypeBeginPersistedPrepareXID = 0x12,  // WAL only.
  // Similar to kTypeBeginPersistedPrepareXID, this is to ensure that WAL
  // generated by WriteUnprepared write policy is not mistakenly read by
  // another.
  kTypeBeginUnprepareXID = 0x13,  // WAL only.
  kMaxValue = 0x7F                // Not used for storing records.
};
</code></pre>
<h3 id="memtableinserter"><a class="header" href="#memtableinserter">MemtableInserter</a></h3>
<p>MemTableInsertor 遍历writeBatch，将记录插入到memtable中,使用MemTableRep封装了skiplist和VectorRep这两种类型的memtable;</p>
<p><img src="rocksdb/./write-batch-iter.svg" alt="write batch iter" /></p>
<h3 id="writetowal"><a class="header" href="#writetowal">WriteToWAL</a></h3>
<p>日志会被分片为固定大小kBlocksize, 太小的会被填充padding,太大的会被切分为first/mid/last等分片record</p>
<p>固定大小这个有什么优势吗？</p>
<p><img src="rocksdb/./write-to-wal.svg" alt="write to wal" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rocksdb-write流程"><a class="header" href="#rocksdb-write流程">RocksDB Write流程</a></h1>
<h2 id="writebatch-2"><a class="header" href="#writebatch-2">WriteBatch</a></h2>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<h2 id="preprocesswrite-1"><a class="header" href="#preprocesswrite-1">PreprocessWrite</a></h2>
<h3 id="schedule-flush"><a class="header" href="#schedule-flush">schedule flush</a></h3>
<p>schedule flush, 将满的memtable转变为immtable, 加到<code>flush_schedule_</code>队列中
由<code>BackgrounFlush</code>将immtable刷到dish上。</p>
<p><img src="rocksdb/./schedule_flushes.svg" alt="schedule flush" /></p>
<h2 id="write-thread-1"><a class="header" href="#write-thread-1">Write thread</a></h2>
<p>Writer的状态</p>
<p><img src="rocksdb/./write_thread_state.svg" alt="write thread state" /></p>
<p>write 相关struct之间引用关系</p>
<p><img src="rocksdb/./write_struct.svg" alt="write struct" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backgroud-flush-and-compaction"><a class="header" href="#backgroud-flush-and-compaction">Backgroud flush and compaction</a></h1>
<h2 id="maybescheduleflushorcompaction"><a class="header" href="#maybescheduleflushorcompaction">MaybeScheduleFlushOrCompaction</a></h2>
<p><code>MaybeScheduleFlushOrCompaction</code>会使用线程池调度，最后在后台线程中调用<code>BackgroundFlush</code>
和<code>BackgrondCompaction</code>分别做memtable的flush和ssfile的compaction.</p>
<p><img src="rocksdb/./MaybeScheduleFlushOrCompaction.svg" alt="MaybeScheduleFlushOrCompaction" /></p>
<h2 id="后台线程调度schedule"><a class="header" href="#后台线程调度schedule">后台线程调度Schedule</a></h2>
<p><img src="rocksdb/./schedule-bgthread.svg" alt="schedule-bgtread" /></p>
<h2 id="后台线程flush"><a class="header" href="#后台线程flush">后台线程flush</a></h2>
<h3 id="生成flushrequest放入flush队列中"><a class="header" href="#生成flushrequest放入flush队列中">生成flushRequest放入flush队列中</a></h3>
<p>向<code>flush_queue_</code>中放入FlushRequest的数据流程如下:</p>
<p><img src="rocksdb/./flush_queue_put.svg" alt="flush_queue_put" /></p>
<p>具体函数调用细节如下：</p>
<p><img src="rocksdb/./flush_queue_put_detail.svg" alt="flush queue put detail" /></p>
<h3 id="后台线程处理flush队列中请求"><a class="header" href="#后台线程处理flush队列中请求">后台线程处理flush队列中请求</a></h3>
<p>后台线程执行<code>BackgroundFlush</code>从<code>flush_queue_</code>中取出FlushRequest转换为FlushJob. </p>
<p><img src="rocksdb/./flush-data-flow-overview.svg" alt="flush-data-flow-overview" /></p>
<p>cfd会被flush的条件</p>
<pre><code class="language-cpp">bool MemTableList::IsFlushPending() const {
  if ((flush_requested_ &amp;&amp; num_flush_not_started_ &gt; 0) ||
      (num_flush_not_started_ &gt;= min_write_buffer_number_to_merge_)) {
    assert(imm_flush_needed.load(std::memory_order_relaxed));
    return true;
  }
  return false;
}
</code></pre>
<p>最终调用<code>WriteLevel0Table</code> 将memtable写入磁盘中，具体调用关系如下:</p>
<p><img src="rocksdb/./background-flush.svg" alt="backgroud-flush" /></p>
<h2 id="后台线程compact"><a class="header" href="#后台线程compact">后台线程compact</a></h2>
<h3 id="cfd放入compact队列"><a class="header" href="#cfd放入compact队列">cfd放入compact队列</a></h3>
<p><img src="rocksdb/./background-compaction-put.svg" alt="background-compaction-put" /></p>
<h3 id="处理compact队列生成compactionjob"><a class="header" href="#处理compact队列生成compactionjob">处理compact队列，生成compactionJob</a></h3>
<p>后台线程会通过<code>PickCompactionFromQueue</code> 去<code>compaction_queue_</code>中取出需要compact的ColumnFamilyData,
然后调用ComlumnFamilyData的PickCompaction 选择compactio的input level, output leve, 以及input files等，</p>
<p><img src="rocksdb/./background-compaction.svg" alt="backgroup-compaction" /></p>
<h4 id="多线程并发compact"><a class="header" href="#多线程并发compact">多线程并发compact</a></h4>
<p>在compact Prepare中会将compactJob划分为不同的SubCompactionState，然后由多线程并发执行压缩</p>
<p><img src="rocksdb/./background-compaction-job.svg" alt="background-compaction-job" /></p>
<h4 id="compaction-picker-1"><a class="header" href="#compaction-picker-1">Compaction Picker</a></h4>
<p>三种compaction style</p>
<p>Level Style Compaction</p>
<p>Universal Style Compaction</p>
<p>FIFO Style Compaction</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compaction-picker-2"><a class="header" href="#compaction-picker-2">Compaction Picker</a></h1>
<h2 id="compaction生成流程"><a class="header" href="#compaction生成流程">Compaction生成流程:</a></h2>
<ol>
<li>SetupInitialFiles 选择要compaction的level和input files</li>
<li>SetupOtherL0FilesIfNeeded和SetupOtherInputsIfNeeded补充选择和input files overlap的文件</li>
<li>最后GetCompaction 生成最终的Compaction然后重新CompuateScore用于下次Compact</li>
</ol>
<p><img src="rocksdb/./level-compaction-picker-overview.svg" alt="leveled compaction pick overview" /></p>
<p>SetupInitialFiles 初始选择的优先级顺序，当前一个选择为空时候，才会去选择下一个:</p>
<p><img src="rocksdb/./SetupInitalFiles-pri.svg" alt="SetupInitialFiles-pri" /></p>
<h3 id="compactionscore"><a class="header" href="#compactionscore">CompactionScore</a></h3>
<h4 id="todo拆分为不同的子图"><a class="header" href="#todo拆分为不同的子图">TODO拆分为不同的子图</a></h4>
<p><img src="rocksdb/./compaction-score.svg" alt="compaction score" /></p>
<h3 id="详细调用图如下"><a class="header" href="#详细调用图如下">详细调用图如下</a></h3>
<p><img src="rocksdb/./level-compaction-picker.svg" alt="level-compaction-picker" /></p>
<h2 id="ref-5"><a class="header" href="#ref-5">Ref</a></h2>
<ol>
<li>leveled-compaction: https://github.com/facebook/rocksdb/wiki/Leveled-Compaction</li>
<li>choose level compaction files: https://github.com/facebook/rocksdb/wiki/Choose-Level-Compaction-Files</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="read-流程"><a class="header" href="#read-流程">read 流程</a></h1>
<h3 id="questions"><a class="header" href="#questions">Questions</a></h3>
<p>SuperVersion ？ 为啥起这个名字？</p>
<h3 id="多级index"><a class="header" href="#多级index">多级index:</a></h3>
<ol>
<li>ColumnFamily 根据Version中的<code>std::vector&lt;FileMetaData*&gt;</code> 定位到具体的Table。</li>
<li>Table根据<code>bloom filter</code>快速排出key不存在的case，如果key不存在，避免后续的磁盘操作。</li>
<li>Table根据<code>IndexBlock</code> 定位到对应的Datablock。</li>
<li>根据Datablock数据中的<code>restartPoint</code>列表二分查找，找到对应的restartPoint偏移, 进一步缩小查找区间。</li>
<li>在具体的<code>restartPoint</code>之间遍历查找具体的key</li>
</ol>
<p><img src="rocksdb/./table_read_index.svg" alt="table read index" /></p>
<h3 id="多级lru缓存"><a class="header" href="#多级lru缓存">多级LRU缓存:</a></h3>
<ol>
<li>TableCache</li>
<li>DataBlockCache</li>
<li>RowCache</li>
</ol>
<p><img src="rocksdb/./table_read_cache.svg" alt="table read cache" /></p>
<p>详细调用关系：</p>
<p><img src="rocksdb/./dbimpl_get.svg" alt="db impl get" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blob"><a class="header" href="#blob">Blob</a></h1>
<p>Questions:</p>
<ol>
<li>PinnableSlice 这个作用是啥</li>
<li>rocksdb的blob和pingcap的titan之间关系？实现逻辑？</li>
<li>Blob文件是怎么选择的</li>
</ol>
<p>Blob将key和value分来开存储。</p>
<pre><code class="language-cpp">// A wrapped database which puts values of KV pairs in a separate log
// and store location to the log in the underlying DB.
</code></pre>
<p><img src="rocksdb/./blob-db.svg" alt="blob db" /></p>
<h2 id="blob-log"><a class="header" href="#blob-log">Blob Log</a></h2>
<p>blob log format</p>
<p><img src="rocksdb/./blob-log-format.svg" alt="blob log format" /></p>
<p>blob index</p>
<p><img src="rocksdb/./blob-index.svg" alt="blob index" /></p>
<h2 id="open"><a class="header" href="#open">Open</a></h2>
<p>Blob open
<img src="rocksdb/./blob-open.svg" alt="blob open" /></p>
<h2 id="put"><a class="header" href="#put">Put</a></h2>
<p>BlobPut</p>
<p><img src="rocksdb/./blob-put.svg" alt="blob put" /></p>
<h2 id="get"><a class="header" href="#get">Get</a></h2>
<p>BlobGet
<img src="rocksdb/./blob-get.svg" alt="blob get" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transaction"><a class="header" href="#transaction">Transaction</a></h1>
<h2 id="transaction-struct"><a class="header" href="#transaction-struct">Transaction struct</a></h2>
<p><img src="rocksdb/./transaction-struct.svg" alt="transaction struct" /></p>
<h3 id="主要数据成员"><a class="header" href="#主要数据成员">主要数据成员</a></h3>
<p>rocksdb中，每个事务主要有<code>track_keys_</code>和<code>write_batch_</code>这两个数据成员，</p>
<ul>
<li><code>track_keys_</code>用于跟踪管理该事务写操作涉及的key </li>
<li><code>write_batch_</code>用于记录事务最终的写结果。</li>
</ul>
<p>所有的悲观事务(pessimistic transaction), 通过<code>txn_db_impl_</code>指针共享 PessimisticTransactionDB，
从而共享全局的TransactionLockMgr,用来统一管理key的lock。</p>
<p><img src="rocksdb/./track-key.svg" alt="track key" /></p>
<h2 id="乐观事务"><a class="header" href="#乐观事务">乐观事务</a></h2>
<p>在commit的时候才去检查key的冲突</p>
<p>一些问题：</p>
<ol>
<li>根据什么判断是否有冲突的？貌似是根据sequnceNumber，但是具体细节不太清楚</li>
<li><code>bucketed_locks_</code>的作用是啥？</li>
<li>CommitWithSerialValidate和 CommitWithParallelValidate这两者区别是啥？</li>
</ol>
<p><img src="rocksdb/./optimistic-transaction-commit.svg" alt="optimistic transaction commit" /></p>
<h2 id="悲观事务"><a class="header" href="#悲观事务">悲观事务</a></h2>
<p>分为三种？</p>
<ol>
<li>writeCommitedTxn</li>
</ol>
<p>WriteCommitted, which means that the data is written to the DB, i.e., the memtable, only after the transaction is committed</p>
<ol start="2">
<li>WritePrepared</li>
<li>WriteUnpreparedTxnDB</li>
</ol>
<p><img src="rocksdb/./pessimistic-transaction.svg" alt="pessimistic transaction" /></p>
<h2 id="参考-2"><a class="header" href="#参考-2">参考</a></h2>
<p><a href="https://github.com/facebook/rocksdb/wiki/WritePrepared-Transactions">Facebook WritePrepared</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimistic-transaction"><a class="header" href="#optimistic-transaction">Optimistic Transaction</a></h1>
<p>乐观事务在commit前，Write操作只会记录事务有哪些key, 不需要做加锁和key冲突检测，适合事务之间
write key重叠比较低的场景。</p>
<p>乐观事务在write时候，使用<code>tracked_keys</code>, 记录受影响的key以及该key的seq, </p>
<p><img src="rocksdb/./optimistic-transaction.svg" alt="optimistic transaction" /></p>
<p>在commit时候会遍历该<code>tracked_keys</code>, 对每个key查找当前db中该key的seq，然后和<code>tracked_key</code>中seq比较。
如果数据库中的seq比key的seq新，则认为发生了冲突。</p>
<p>不太理解这里面的<code>min_uncommited</code> 起了什么作用.</p>
<p><img src="rocksdb/./check-key-conflict.svg" alt="check key conflict" /></p>
<p>遍历<code>TransactionKeyMap</code>, 检查每个key的冲突</p>
<pre><code class="language-cpp">Status TransactionUtil::CheckKeysForConflicts(DBImpl* db_impl,
                                              const TransactionKeyMap&amp; key_map,
                                              bool cache_only) {
    //other code..
    //遍历迭代key_map
    for (const auto&amp; key_iter : keys) {
      const auto&amp; key = key_iter.first;
      const SequenceNumber key_seq = key_iter.second.seq;

      result = CheckKey(db_impl, sv, earliest_seq, key_seq, key, cache_only);

      if (!result.ok()) {
        break;
      }
    }

}
</code></pre>
<p>检查具体某个key的冲突</p>
<pre><code class="language-cpp">// min_uncommitted 默认值为 KMaxSequnceNumber
// snap_checker默认值为nullptr;
Status TransactionUtil::CheckKey(DBImpl* db_impl, SuperVersion* sv,
                                 SequenceNumber earliest_seq,
                                 SequenceNumber snap_seq,
                                 const std::string&amp; key, bool cache_only,
                                 ReadCallback* snap_checker,
                                 SequenceNumber min_uncommitted) {
  
  //...other code
    SequenceNumber seq = kMaxSequenceNumber;
    bool found_record_for_key = false;

    // When min_uncommitted == kMaxSequenceNumber, writes are committed in
    // sequence number order, so only keys larger than `snap_seq` can cause
    // conflict.
    // When min_uncommitted != kMaxSequenceNumber, keys lower than
    // min_uncommitted will not triggered conflicts, while keys larger than
    // min_uncommitted might create conflicts, so we need  to read them out
    // from the DB, and call callback to snap_checker to determine. So only
    // keys lower than min_uncommitted can be skipped.
    SequenceNumber lower_bound_seq =
        (min_uncommitted == kMaxSequenceNumber) ? snap_seq : min_uncommitted;

    // 去数据库中查找key的最新seq
    Status s = db_impl-&gt;GetLatestSequenceForKey(sv, key, !need_to_read_sst,
                                                lower_bound_seq, &amp;seq,
                                                &amp;found_record_for_key);

    if (!(s.ok() || s.IsNotFound() || s.IsMergeInProgress())) {
      result = s;
    } else if (found_record_for_key) {
      bool write_conflict = snap_checker == nullptr
                                ? snap_seq &lt; seq
                                : !snap_checker-&gt;IsVisible(seq);
      if (write_conflict) {
        result = Status::Busy();
      }
    }
  }
  return result;
}
</code></pre>
<p>一些问题：</p>
<ol>
<li>根据什么判断是否有冲突的？貌似是根据sequnceNumber，但是具体细节不太清楚</li>
<li><code>bucketed_locks_</code>的作用是啥？</li>
<li>CommitWithSerialValidate和 CommitWithParallelValidate这两者区别是啥？</li>
<li>key冲突检测是咋搞的</li>
<li>并行和顺序这个是怎么弄的</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transaction-lock-mgr"><a class="header" href="#transaction-lock-mgr">Transaction lock mgr</a></h1>
<p>TransactionLockMgr 用于管理悲观事务的key lock，所有的悲观事务，通过<code>txn_db_impl-&gt;lock_mgr_</code>指针共享
同一个lockmgr</p>
<h3 id="lockmap"><a class="header" href="#lockmap">LockMap</a></h3>
<p>rocksdb中对于key lock做了多种优化</p>
<ol>
<li>首先根据ColumnFamilyId, 从LockMaps获得对应的LockMap</li>
<li>使用了thread local data来缓存全局的lock maps, 避免每次查询全局的lockmaps需要加锁</li>
<li>使用<code>GetStripe</code>把key做sharding获得相应的LockStripe,降低了锁冲突, 但是在同一个stripe中的key还是有并发等待问题.</li>
</ol>
<p><img src="rocksdb/./transaction-lock-level.svg" alt="transaction lock level" /></p>
<pre><code class="language-cpp">size_t LockMap::GetStripe(const std::string&amp; key) const {
  assert(num_stripes_ &gt; 0);
  return fastrange64(GetSliceNPHash64(key), num_stripes_);
}
</code></pre>
<p><code>GetLockMap</code>封装装了从thread local cache获取lockMap逻辑</p>
<pre><code class="language-cpp">std::shared_ptr&lt;LockMap&gt; TransactionLockMgr::GetLockMap(
    uint32_t column_family_id) {

  // First check thread-local cache
  if (lock_maps_cache_-&gt;Get() == nullptr) {
    lock_maps_cache_-&gt;Reset(new LockMaps());
  }

  auto lock_maps_cache = static_cast&lt;LockMaps*&gt;(lock_maps_cache_-&gt;Get());

  //首先从thread local cache中查找
  auto lock_map_iter = lock_maps_cache-&gt;find(column_family_id);
  if (lock_map_iter != lock_maps_cache-&gt;end()) {
    // Found lock map for this column family.
    return lock_map_iter-&gt;second;
  }

  //没找到的话，使用mutex访问全局LockMaps
  // Not found in local cache, grab mutex and check shared LockMaps
  InstrumentedMutexLock l(&amp;lock_map_mutex_);

  lock_map_iter = lock_maps_.find(column_family_id);
  if (lock_map_iter == lock_maps_.end()) {
    return std::shared_ptr&lt;LockMap&gt;(nullptr);
  } else {
    //插入到thread local cache中，方便下一次访问
    // Found lock map.  Store in thread-local cache and return.
    std::shared_ptr&lt;LockMap&gt;&amp; lock_map = lock_map_iter-&gt;second;
    lock_maps_cache-&gt;insert({column_family_id, lock_map});

    return lock_map;
  }
}
</code></pre>
<h3 id="获取释放key锁"><a class="header" href="#获取释放key锁">获取/释放key锁</a></h3>
<p><img src="rocksdb/./transaction-lock-mgr.svg" alt="transaction lock mgr" /></p>
<h3 id="死锁检测"><a class="header" href="#死锁检测">死锁检测</a></h3>
<p><img src="rocksdb/./transaction-lock-mgr-deadlock-detect.svg" alt="transaction lock mgr dead lock detect" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="two-phase-commit"><a class="header" href="#two-phase-commit">Two phase commit</a></h1>
<h2 id="write-commited-txn"><a class="header" href="#write-commited-txn">Write Commited txn</a></h2>
<p>事务只有在提交之后，才会写入到db的memtable中，事务在数据库中读到的
kv都是提交之后的，这种需要在提交之前把所有的write kv操作保存在内存writeBatch中，
对于大的事务来说，内存是个瓶颈，另一方面，commit时候才集中的写入memtabe，这个延迟可能也无法忽略。</p>
<p><img src="rocksdb/./write-committed.svg" alt="write commited" /></p>
<p>WriteCommited 两阶段提交：</p>
<ul>
<li>Prepare阶段 将writebatch 写入WAL日志中,并将writeBatch中内容用<code>ktypeBeginPrepare(Xid)</code>, <code>kTypeEndPrepare(xid)</code> 括起来
由于只写到了WAL日志中,　其他事务看不到这个事务的修改</li>
<li>Commit阶段 向WAL日志写入commit 标记，比如<code>kTypeCommit(xid)</code> 并writeBatch中内容insert到memtable上，写入memtable之后，该事务的修改对其他事务就可以见了。
如果向WAL日志中写入<code>KtypeCommit(xid)</code>日志就挂了的话，下次recover时候，会重新从日志中恢复writeBatch，然后插入到memtabl中。</li>
</ul>
<p><img src="rocksdb/./two-phase-commit-write-batch.svg" alt="two-phase-commit-write-batch" /></p>
<pre><code class="language-cpp">Status WriteBatchInternal::MarkEndPrepare(WriteBatch* b, const Slice&amp; xid,
                                          bool write_after_commit,
                                          bool unprepared_batch) {
  // other code..
  // rewrite noop as begin marker
  b-&gt;rep_[12] = static_cast&lt;char&gt;(
      write_after_commit ? kTypeBeginPrepareXID
                         : (unprepared_batch ? kTypeBeginUnprepareXID
                                             : kTypeBeginPersistedPrepareXID));
  b-&gt;rep_.push_back(static_cast&lt;char&gt;(kTypeEndPrepareXID));
  PutLengthPrefixedSlice(&amp;b-&gt;rep_, xid);
  // other code..
}
</code></pre>
<h4 id="recover-1"><a class="header" href="#recover-1">Recover</a></h4>
<p>事务日志会以writeBatch为单位写入到WAL日志中，恢复时MemtableInsetor会去遍历日志中的writeBatch,
将<code>BeginPrepare</code>....<code>EndPrepare(xid)</code>之间的kv操作插入到新的writeBatch中，
在遍历到<code>Commit(xid)</code>时候，将该writeBatch插入到memtable中，完成提交。</p>
<p><img src="rocksdb/./two-phase-commit-recover.svg" alt="two phase commit recover" /></p>
<h2 id="write-prepared-txn"><a class="header" href="#write-prepared-txn">Write prepared txn</a></h2>
<p>没有commit，就把数据insert到db中，有以下几个问题需要解决:</p>
<ul>
<li>How do we identify the key/values in the DB with transactions that wrote them?</li>
<li>How do we figure if a key/value written by transaction <code>Txn_w</code> is in the read snapshot of the reading transaction <code>Txn_r</code>?</li>
<li>How do we rollback the data written by aborted transactions?</li>
</ul>
<p>在prepare阶段就插入memtalbe中.</p>
<p>CommitCache 用于判断是否提交了</p>
<p><img src="rocksdb/./two-phase-commit-write-preparedtxn.svg" alt="write unprepared" /></p>
<h2 id="write-unprepared-txn"><a class="header" href="#write-unprepared-txn">Write unprepared txn</a></h2>
<h2 id="todo-1"><a class="header" href="#todo-1">TODO:</a></h2>
<ol>
<li>write prepared txn和write unprepared txn这个具体逻辑还不是很清楚，只知道是把commit放到了一个cache里面。</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="clickhouse"><a class="header" href="#clickhouse">ClickHouse</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="server-main"><a class="header" href="#server-main">Server Main</a></h1>
<!-- toc -->
<h2 id="server-main-主流程"><a class="header" href="#server-main-主流程">Server main 主流程</a></h2>
<h3 id="主循环"><a class="header" href="#主循环">主循环</a></h3>
<p>首先监听端口号，等待客户端连接， 
和客户端建立连接后,server然后不断从conn中读取packet, 
解析sql语句为AST树，然后创建plan pipeline
最后执行plan，将result set通过网络发送给客户端.</p>
<p><img src="clickhouse/./dot/server-main.svg" alt="server-main" /></p>
<h3 id="sql-解析执行流程"><a class="header" href="#sql-解析执行流程">SQL 解析执行流程</a></h3>
<p>一条Query SQL在clickhouse中执行流程如下:</p>
<p><img src="clickhouse/./dot/execute-flow.svg" alt="execute-flow" /></p>
<h4 id="parse-sql"><a class="header" href="#parse-sql">Parse SQL</a></h4>
<p>解析SQL，解析为AST树，然后创建对应的pipeline plan.</p>
<p><img src="clickhouse/./dot/execute-query.svg" alt="execute-query" /></p>
<h4 id="selectquery"><a class="header" href="#selectquery">SelectQuery</a></h4>
<p>执行Select Query , 创建QueryPlan</p>
<p><img src="clickhouse/./dot/select-query.svg" alt="select-query" /></p>
<h4 id="queryplanstep"><a class="header" href="#queryplanstep">QueryPlanStep</a></h4>
<p><img src="clickhouse/./dot/query-plan-step.svg" alt="query-plan-step" /></p>
<h4 id="iprocessor"><a class="header" href="#iprocessor">IProcessor</a></h4>
<p>Processor is an element (low level building block) of a query execution pipeline.
It has zero or more input ports and zero or more output ports.</p>
<p>Blocks of data are transferred over ports.
Each port has fixed structure: names and types of columns and values of constants.</p>
<pre><code>src/Processors/IProcessor.h
</code></pre>
<h4 id="iprocessor-继承关系图"><a class="header" href="#iprocessor-继承关系图">IProcessor 继承关系图</a></h4>
<p>CK中Iprocessor的继承关系图</p>
<pre><code class="language-cpp">class IProcessor
{
protected:
    InputPorts inputs;
    OutputPorts outputs;
}
</code></pre>
<p><img src="clickhouse/./dot/iprocessor.svg" alt="iprocessor" /></p>
<p><img src="clickhouse/./dot/transform.svg" alt="transform" /></p>
<h3 id="executor-执行pipeline"><a class="header" href="#executor-执行pipeline">Executor: 执行pipeline</a></h3>
<h4 id="pipelineexecutor"><a class="header" href="#pipelineexecutor">PipelineExecutor</a></h4>
<p>使用线程池执行pipline</p>
<p><img src="clickhouse/./dot/pipeline-executor.svg" alt="pipeline-executor" /></p>
<h4 id="pullingpipelineexecutor"><a class="header" href="#pullingpipelineexecutor">PullingPipelineExecutor</a></h4>
<p>单线程同步执行？</p>
<pre><code class="language-cpp">/// Pulling executor for QueryPipeline. Always execute pipeline in single thread.
/// Typical usage is:
///
/// PullingPipelineExecutor executor(query_pipeline);
/// while (executor.pull(chunk))
///     ... process chunk ...
</code></pre>
<p><img src="clickhouse/./dot/pulling-pipeline-executor.svg" alt="pulling-pipeline-executor" /></p>
<h4 id="pullingasyncpipelineexecutor"><a class="header" href="#pullingasyncpipelineexecutor">PullingAsyncPipelineExecutor</a></h4>
<p>多线程异步执行</p>
<pre><code class="language-cpp">/// Asynchronous pulling executor for QueryPipeline.
/// Always creates extra thread. If query is executed in single thread, use PullingPipelineExecutor.
/// Typical usage is:
///
/// PullingAsyncPipelineExecutor executor(query_pipeline);
/// while (executor.pull(chunk, timeout))
///     ... process chunk ...
</code></pre>
<p><img src="clickhouse/./dot/pulling-async-pipeline-executor.svg" alt="pulling-async-pipeline-executor" /></p>
<h3 id="iblockinputstream"><a class="header" href="#iblockinputstream">IBlockInputStream</a></h3>
<h4 id="pipelineexecutingblockinputstream"><a class="header" href="#pipelineexecutingblockinputstream">PipelineExecutingBlockInputStream</a></h4>
<p>封装了PullingPipelineExecutor和PullingAsyncPipelineExecutor, 实现了IBlockInputStream接口</p>
<p><img src="clickhouse/./dot/pipeline-executing-block-input-stream.svg" alt="pipeline-executing-block-input-stream" /></p>
<h4 id="asynchronousblockinputstream"><a class="header" href="#asynchronousblockinputstream">AsynchronousBlockInputStream</a></h4>
<p>在另外一个线程中执行inner BlockInputStream</p>
<pre><code class="language-cpp">/** Executes another BlockInputStream in a separate thread.
  * This serves two purposes:
  * 1. Allows you to make the different stages of the query execution pipeline work in parallel.
  * 2. Allows you not to wait until the data is ready, and periodically check their readiness without blocking.
  *    This is necessary, for example, so that during the waiting period you can check if a packet
  *     has come over the network with a request to interrupt the execution of the query.
  *    It also allows you to execute multiple queries at the same time.
  */
</code></pre>
<p><img src="clickhouse/./dot/asynchronous-block-inputstream.svg" alt="asynchronous-block-inputstream" /></p>
<h3 id="blockio"><a class="header" href="#blockio">BlockIO</a></h3>
<p>block-io getInputStream，读数据时执行plan</p>
<p><img src="clickhouse/./dot/block-io.svg" alt="block-io" /></p>
<h2 id="参考资料"><a class="header" href="#参考资料">参考资料</a></h2>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1602664">Clickhouse源码导读: 网络IO</a></li>
<li><a href="http://sineyuan.github.io/post/clickhouse-source-guide/">Clickhouse源码导读</a></li>
<li><a href="https://developer.aliyun.com/article/765184">ClickHouse 源码阅读 —— SQL的前世今生</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block"><a class="header" href="#block">Block</a></h1>
<!-- toc -->
<h2 id="block-1"><a class="header" href="#block-1">Block</a></h2>
<blockquote>
<p>A Block is a container that represents a subset (chunk) of a table in memory. It is just a set of triples: (IColumn, IDataType, column name). During query execution, data is processed by Blocks. If we have a Block, we have data (in the IColumn object), we have information about its type (in IDataType) that tells us how to deal with that column, and we have the column name. It could be either the original column name from the table or some artificial name assigned for getting temporary results of calculations.</p>
</blockquote>
<p>最基本的数据处理单元, 有点类似于Pandas的dataframe, 对应的基本操作有insert/erase</p>
<pre><code class="language-cpp">/** Container for set of columns for bunch of rows in memory.
  * This is unit of data processing.
  * Also contains metadata - data types of columns and their names
  *  (either original names from a table, or generated names during temporary calculations).
  * Allows to insert, remove columns in arbitrary position, to change order of columns.
  */
</code></pre>
<p><img src="clickhouse/./dot/block.svg" alt="block" /></p>
<h2 id="blockinfo"><a class="header" href="#blockinfo">BlockInfo</a></h2>
<pre><code class="language-cpp">    /** is_overflows:
      * After running GROUP BY ... WITH TOTALS with the max_rows_to_group_by and group_by_overflow_mode = 'any' settings,
      *  a row is inserted in the separate block with aggregated values that have not passed max_rows_to_group_by.
      * If it is such a block, then is_overflows is set to true for it.
      */

    /** bucket_num:
      * When using the two-level aggregation method, data with different key groups are scattered across different buckets.
      * In this case, the bucket number is indicated here. It is used to optimize the merge for distributed aggregation.
      * Otherwise -1.
      */
</code></pre>
<h2 id="icolumn"><a class="header" href="#icolumn">IColumn</a></h2>
<p>Cow: Copy on write shared Ptr</p>
<p>ICoumn存储数据</p>
<p>icolumn和idatatype 比较类似？他们两者分别负责什么功能?</p>
<p><img src="clickhouse/./dot/icolumn.svg" alt="iclolumn" /></p>
<h2 id="idatatype"><a class="header" href="#idatatype">IDataType</a></h2>
<p>数据的序列化和反序列化</p>
<p><img src="clickhouse/./dot/idatatype.svg" alt="idatatype" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blockio-1"><a class="header" href="#blockio-1">BlockIO</a></h1>
<!-- toc -->
<p>Block的输入输出, 主要有BlockInputStream 和
BlockOutputStream, 输入输出的基本单位为Block</p>
<p>getHeader header的作用是啥？表明data的schema吗?</p>
<p><img src="clickhouse/./dot/blockio.svg" alt="blockio" /></p>
<h2 id="iblockinputstream-1"><a class="header" href="#iblockinputstream-1">IBlockInputStream</a></h2>
<blockquote>
<p>The stream interface for reading data by blocks from the database.
Relational operations are supposed to be done also as implementations of this interface.
Watches out at how the source of the blocks works.
Lets you get information for profiling: rows per second, blocks per second, megabytes per second, etc.
Allows you to stop reading data (in nested sources).</p>
</blockquote>
<p>IBlockInputStream 主要接口 read, readPrefix, readSuffix</p>
<p>这个地方的limit, quta, 以及info之类的作用是什么?</p>
<p><img src="clickhouse/./dot/iblock-inputstream-func.svg" alt="iblock-inputstream-func" /></p>
<h3 id="iblockinputstream-继承关系"><a class="header" href="#iblockinputstream-继承关系">IBlockInputStream 继承关系</a></h3>
<p><img src="clickhouse/./dot/iblockinputstream.svg" alt="iblockinputstream" /></p>
<h4 id="asynchronousblockinputstream-1"><a class="header" href="#asynchronousblockinputstream-1">AsynchronousBlockInputStream</a></h4>
<blockquote>
<p>Executes another BlockInputStream in a separate thread.
This serves two purposes:</p>
<ol>
<li>Allows you to make the different stages of the query execution pipeline work in parallel.</li>
<li>Allows you not to wait until the data is ready, and periodically check their readiness without blocking.
This is necessary, for example, so that during the waiting period you can check if a packet
has come over the network with a request to interrupt the execution of the query.
It also allows you to execute multiple queries at the same time.</li>
</ol>
</blockquote>
<p><img src="clickhouse/./dot/asynchronous-block-inputstream.svg" alt="asynchronousBlockInputStream" /></p>
<h4 id="pipelineexecutingblockinputstream-1"><a class="header" href="#pipelineexecutingblockinputstream-1">PipelineExecutingBlockInputStream</a></h4>
<blockquote>
<p>Implement IBlockInputStream from QueryPipeline.
It's a temporary wrapper.</p>
</blockquote>
<p><img src="clickhouse/./dot/pipeline-executing-block-input-stream.svg" alt="pipelineExecutingBlockInputStream" /></p>
<p>TODO:</p>
<ol>
<li>TypePromotion 模板</li>
<li>Cow 模板</li>
</ol>
<h2 id="iblockoutputstream"><a class="header" href="#iblockoutputstream">IBlockOutputStream</a></h2>
<blockquote>
<p>Interface of stream for writing data (into table, filesystem, network, terminal, etc.)</p>
</blockquote>
<p><img src="clickhouse/./dot/iblockoutputstream.svg" alt="iblockoutputstream" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage"><a class="header" href="#storage">Storage</a></h1>
<!-- toc -->
<h2 id="istorage-struct"><a class="header" href="#istorage-struct">IStorage Struct</a></h2>
<blockquote>
<p>Storage. Describes the table. Responsible for</p>
<ul>
<li>storage of the table data;</li>
<li>the definition in which files (or not in files) the data is stored;</li>
<li>data lookups and appends;</li>
<li>data storage structure (compression, etc.)</li>
<li>concurrent access to data (locks, etc.)</li>
</ul>
</blockquote>
<p><img src="clickhouse/./dot/istorage-struct.svg" alt="istorage struct" /></p>
<pre><code class="language-cpp">struct StorageInMemoryMetadata
{
    /// Columns of table with their names, types,
    /// defaults, comments, etc. All table engines have columns.
    ColumnsDescription columns;
    /// Table indices. Currently supported for MergeTree only.
    IndicesDescription secondary_indices;
    /// Table constraints. Currently supported for MergeTree only.
    ConstraintsDescription constraints;
    /// PARTITION BY expression. Currently supported for MergeTree only.
    KeyDescription partition_key;
    /// PRIMARY KEY expression. If absent, than equal to order_by_ast.
    KeyDescription primary_key;
    /// ORDER BY expression. Required field for all MergeTree tables
    /// even in old syntax MergeTree(partition_key, order_by, ...)
    KeyDescription sorting_key;
    /// SAMPLE BY expression. Supported for MergeTree only.
    KeyDescription sampling_key;
    /// Separate ttl expressions for columns
    TTLColumnsDescription column_ttls_by_name;
    /// TTL expressions for table (Move and Rows)
    TTLTableDescription table_ttl;
    /// SETTINGS expression. Supported for MergeTree, Buffer and Kafka.
    ASTPtr settings_changes;
    /// SELECT QUERY. Supported for MaterializedView and View (have to support LiveView).
    SelectQueryDescription select;
    //...
}
</code></pre>
<h2 id="istorage-interface"><a class="header" href="#istorage-interface">IStorage Interface</a></h2>
<h3 id="watchreadwrite"><a class="header" href="#watchreadwrite">watch/read/write</a></h3>
<pre><code class="language-cpp">    virtual BlockInputStreams watch(
        const Names &amp; /*column_names*/,
        const SelectQueryInfo &amp; /*query_info*/,
        const Context &amp; /*context*/,
        QueryProcessingStage::Enum &amp; /*processed_stage*/,
        size_t /*max_block_size*/,
        unsigned /*num_streams*/)
    {
        throw Exception(&quot;Method watch is not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }

    virtual Pipes read(
        const Names &amp; /*column_names*/,
        const StorageMetadataPtr &amp; /*metadata_snapshot*/,
        const SelectQueryInfo &amp; /*query_info*/,
        const Context &amp; /*context*/,
        QueryProcessingStage::Enum /*processed_stage*/,
        size_t /*max_block_size*/,
        unsigned /*num_streams*/)
    {
        throw Exception(&quot;Method read is not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }

    virtual BlockOutputStreamPtr write(
        const ASTPtr &amp; /*query*/,
        const StorageMetadataPtr &amp; /*metadata_snapshot*/,
        const Context &amp; /*context*/)
    {
        throw Exception(&quot;Method write is not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }

    virtual void drop() {}

    virtual void truncate(
        const ASTPtr &amp; /*query*/,
        const StorageMetadataPtr &amp; /* metadata_snapshot */,
        const Context &amp; /* context */,
        TableExclusiveLockHolder &amp;)
    {
        throw Exception(&quot;Truncate is not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }

</code></pre>
<h3 id="rename"><a class="header" href="#rename">rename</a></h3>
<pre><code class="language-cpp">    virtual void rename(const String &amp; /*new_path_to_table_data*/, const StorageID &amp; new_table_id)
    /**
     * Just updates names of database and table without moving any data on disk
     * Can be called directly only from DatabaseAtomic.
     */
    virtual void renameInMemory(const StorageID &amp; new_table_id);
</code></pre>
<h3 id="alter-adddrop-columns"><a class="header" href="#alter-adddrop-columns">alter: add/drop columns</a></h3>
<pre><code class="language-cpp">    /** ALTER tables in the form of column changes that do not affect the change
      * to Storage or its parameters. Executes under alter lock (lockForAlter).
      */
    virtual void alter(const AlterCommands &amp; params, const Context &amp; context, TableLockHolder &amp; alter_lock_holder);

    /** Checks that alter commands can be applied to storage. For example, columns can be modified,
      * or primary key can be changes, etc.
      */
    virtual void checkAlterIsPossible(const AlterCommands &amp; commands, const Settings &amp; settings) const;

    /** ALTER tables with regard to its partitions.
      * Should handle locks for each command on its own.
      */
    virtual void alterPartition(const ASTPtr &amp; /* query */, const StorageMetadataPtr &amp; /* metadata_snapshot */, const PartitionCommands &amp; /* commands */, const Context &amp; /* context */)
    {
        throw Exception(&quot;Partition operations are not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }
</code></pre>
<h4 id="altercommands"><a class="header" href="#altercommands">AlterCommands</a></h4>
<pre><code class="language-cpp">/// Operation from the ALTER query (except for manipulation with PART/PARTITION).
/// Adding Nested columns is not expanded to add individual columns.
struct AlterCommand
{
    /// The AST of the whole command
    ASTPtr ast;

    enum Type
    {
        ADD_COLUMN,
        DROP_COLUMN,
        MODIFY_COLUMN,
        COMMENT_COLUMN,
        MODIFY_ORDER_BY,
        ADD_INDEX,
        DROP_INDEX,
        ADD_CONSTRAINT,
        DROP_CONSTRAINT,
        MODIFY_TTL,
        MODIFY_SETTING,
        MODIFY_QUERY,
        RENAME_COLUMN,
    };
...
</code></pre>
<h3 id="mutate"><a class="header" href="#mutate">mutate</a></h3>
<pre><code class="language-cpp">    /// Mutate the table contents
    virtual void mutate(const MutationCommands &amp;, const Context &amp;)
    {
        throw Exception(&quot;Mutations are not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }

    /// Cancel a mutation.
    virtual CancellationCode killMutation(const String &amp; /*mutation_id*/)
    {
        throw Exception(&quot;Mutations are not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }
</code></pre>
<h4 id="mutationcommand"><a class="header" href="#mutationcommand">MutationCommand</a></h4>
<pre><code class="language-cpp">/// Represents set of actions which should be applied
/// to values from set of columns which statisfy predicate.
struct MutationCommand
{
    ASTPtr ast; /// The AST of the whole command

    enum Type
    {
        EMPTY,     /// Not used.
        DELETE,
        UPDATE,
        MATERIALIZE_INDEX,
        READ_COLUMN,
        DROP_COLUMN,
        DROP_INDEX,
        MATERIALIZE_TTL,
        RENAME_COLUMN,
    };

    Type type = EMPTY;
    ...
}
</code></pre>
<h3 id="optimize-backgroud-work"><a class="header" href="#optimize-backgroud-work">optimize: backgroud work</a></h3>
<pre><code class="language-cpp">    /** Perform any background work. For example, combining parts in a MergeTree type table.
      * Returns whether any work has been done.
      */
    virtual bool optimize(
        const ASTPtr &amp; /*query*/,
        const StorageMetadataPtr &amp; /*metadata_snapshot*/,
        const ASTPtr &amp; /*partition*/,
        bool /*final*/,
        bool /*deduplicate*/,
        const Context &amp; /*context*/)
    {
        throw Exception(&quot;Method optimize is not supported by storage &quot; + getName(), ErrorCodes::NOT_IMPLEMENTED);
    }
</code></pre>
<h3 id="startupshutdown"><a class="header" href="#startupshutdown">startup/shutdown</a></h3>
<pre><code class="language-cpp">    /** If the table have to do some complicated work on startup,
      *  that must be postponed after creation of table object
      *  (like launching some background threads),
      *  do it in this method.
      * You should call this method after creation of object.
      * By default, does nothing.
      * Cannot be called simultaneously by multiple threads.
      */
    virtual void startup() {}

    /** If the table have to do some complicated work when destroying an object - do it in advance.
      * For example, if the table contains any threads for background work - ask them to complete and wait for completion.
      * By default, does nothing.
      * Can be called simultaneously from different threads, even after a call to drop().
      */
    virtual void shutdown() {}
</code></pre>
<h2 id="storage-inherit"><a class="header" href="#storage-inherit">Storage Inherit</a></h2>
<p>主要分为StorageLog, MergeTree, SystemData还有类似StorageMySQL等external Data的</p>
<p><img src="clickhouse/./dot/istorage-inherit.svg" alt="istorage inherit" /></p>
<h3 id="system-storage"><a class="header" href="#system-storage">System Storage</a></h3>
<p>system storage 在clickhouse中可以通过use system, 然后 show tables 看到
可以通过表查询clickhouse的各种信息。</p>
<pre><code>&gt;use system;
&gt;show tables;
</code></pre>
<pre><code class="language-cpp"> aggregate_function_combinators │
│ asynchronous_metrics           │
│ build_options                  │
│ clusters                       │
│ collations                     │
│ columns                        │
│ contributors                   │
│ current_roles                  │
│ data_type_families             │
│ databases                      │
│ detached_parts                 │
│ dictionaries                   │
│ disks                          │
│ distribution_queue             │
│ enabled_roles                  │
│ events                         │
│ formats                        │
│ functions                      │
│ grants                         │
│ graphite_retentions            │
│ licenses                       │
│ macros                         │
│ merge_tree_settings            │
│ merges                         │
│ metric_log                     │
│ metric_log_0                   │
│ metrics                        │
│ models                         │
│ mutations                      │
│ numbers                        │
│ numbers_mt                     │
│ one                            │
│ parts                          │
│ parts_columns                  │
│ privileges                     │
│ processes                      │
│ query_log                      │
│ query_thread_log               │
│ quota_limits                   │
│ quota_usage                    │
│ quotas                         │
│ quotas_usage                   │
│ replicas                       │
│ replication_queue              │
│ role_grants                    │
│ roles                          │
│ row_policies                   │
│ settings                       │
│ settings_profile_elements      │
│ settings_profiles              │
│ stack_trace                    │
│ storage_policies               │
│ table_engines                  │
│ table_functions                │
│ tables                         │
│ trace_log                      │
│ trace_log_0                    │
│ users                          │
│ zeros                          │
│ zeros_mt                       │
└────────────
</code></pre>
<h3 id="mergetreedata"><a class="header" href="#mergetreedata">MergeTreeData</a></h3>
<h3 id="storagelog"><a class="header" href="#storagelog">StorageLog</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mergetreedata-1"><a class="header" href="#mergetreedata-1">MergeTreeData</a></h1>
<!-- toc -->
<pre><code>&gt; Data structure for *MergeTree engines.
&gt; Merge tree is used for incremental sorting of data.
&gt; The table consists of several sorted parts.
&gt; During insertion new data is sorted according to the primary key and is written to the new part.
&gt; Parts are merged in the background according to a heuristic algorithm.
&gt; For each part the index file is created containing primary key values for every n-th row.
&gt; This allows efficient selection by primary key range predicate.
</code></pre>
<p>数据是根据primary key排序的，新插入的数据，被写到一个新的part中，然后由
后台线程根据启发式算法将parts merge合并起来。</p>
<h2 id="mergetreedata-file-struct"><a class="header" href="#mergetreedata-file-struct">MergeTreeData file struct</a></h2>
<p><img src="clickhouse/./dot/merge-tree-data-file-struct.svg" alt="merge tree data file struct" /></p>
<h2 id="mergetreedata-struct"><a class="header" href="#mergetreedata-struct">MergeTreeData Struct</a></h2>
<p><img src="clickhouse/./dot/merge-tree-data-struct.svg" alt="merge tree data struct" /></p>
<h3 id="storageinmemorymetadata"><a class="header" href="#storageinmemorymetadata">StorageInMemoryMetadata</a></h3>
<h3 id="data_parts_indexes"><a class="header" href="#data_parts_indexes">data_parts_indexes</a></h3>
<p>使用了boost的multi_index 用来根据快速定位到DataPart</p>
<pre><code class="language-cpp">using DataPartsIndexes = boost::multi_index_container&lt;DataPartPtr,
    boost::multi_index::indexed_by&lt;
        /// Index by Info
        boost::multi_index::ordered_unique&lt;
            boost::multi_index::tag&lt;TagByInfo&gt;,
            boost::multi_index::global_fun&lt;const DataPartPtr &amp;, const MergeTreePartInfo &amp;, dataPartPtrToInfo&gt;
        &gt;,
        /// Index by (State, Info), is used to obtain ordered slices of parts with the same state
        boost::multi_index::ordered_unique&lt;
            boost::multi_index::tag&lt;TagByStateAndInfo&gt;,
            boost::multi_index::global_fun&lt;const DataPartPtr &amp;, DataPartStateAndInfo, dataPartPtrToStateAndInfo&gt;,
            LessStateDataPart
        &gt;
    &gt;
&gt;;
</code></pre>
<p>围绕data_parts_index的insert/erase和查询</p>
<p><img src="clickhouse/./dot/merge-tree-data-index.svg" alt="merge data tree index" /></p>
<p>MergeTreeData 的数据成员</p>
<pre><code class="language-cpp">    bool require_part_metadata;
    String relative_data_path;
    /// Current column sizes in compressed and uncompressed form.
    ColumnSizeByName column_sizes;
    /// Engine-specific methods
    BrokenPartCallback broken_part_callback;
    String log_name;
    Poco::Logger * log;
    /// Storage settings.
    /// Use get and set to receive readonly versions.
    MultiVersion&lt;MergeTreeSettings&gt; storage_settings;

    mutable std::mutex data_parts_mutex;
    DataPartsIndexes data_parts_indexes;
    DataPartsIndexes::index&lt;TagByInfo&gt;::type &amp; data_parts_by_info;
    DataPartsIndexes::index&lt;TagByStateAndInfo&gt;::type &amp; data_parts_by_state_and_info;

    MergeTreePartsMover parts_mover;
</code></pre>
<h3 id="create"><a class="header" href="#create">create</a></h3>
<pre><code class="language-cpp">    /// Create part, that already exists on filesystem.
    /// After this methods 'loadColumnsChecksumsIndexes' must be called.
    MutableDataPartPtr createPart(const String &amp; name,
        const VolumePtr &amp; volume, const String &amp; relative_path) const;
</code></pre>
<p>call create的调用链
<img src="clickhouse/./dot/merge-tree-data-create.svg" alt="merge-tree-data-create" /></p>
<h3 id="loaddataparts"><a class="header" href="#loaddataparts">loadDataParts</a></h3>
<p><img src="clickhouse/./dot/merge-data-tree-loadDataParts.svg" alt="merge-tree-data-loadDataParts" /></p>
<h2 id="mergetreedatawriter"><a class="header" href="#mergetreedatawriter">MergeTreeDataWriter</a></h2>
<h3 id="writetemppart"><a class="header" href="#writetemppart">writeTempPart</a></h3>
<p>对block数据排序，然后写到MergeTreeDataPart中，
<img src="clickhouse/./dot/merge-tree-data-writer-writeTmp.svg" alt="mergetreedatawtier-writerTempPart" /></p>
<h3 id="imergetreedatapartwriter"><a class="header" href="#imergetreedatapartwriter">IMergeTreeDataPartWriter</a></h3>
<h3 id="mergetreedatapartwriterondisk"><a class="header" href="#mergetreedatapartwriterondisk">MergeTreeDataPartWriterOnDisk</a></h3>
<h3 id="mergetreedatapartwriterondiskstream"><a class="header" href="#mergetreedatapartwriterondiskstream"><code>MergeTreeDataPartWriterOnDisk::Stream</code></a></h3>
<p>负责将数据写入到存储介质中</p>
<pre><code class="language-cpp">    /// Helper class, which holds chain of buffers to write data file with marks.
    /// It is used to write: one column, skip index or all columns (in compact format).
    struct Stream
    {
        Stream(
            const String &amp; escaped_column_name_,
            DiskPtr disk_,
            const String &amp; data_path_,
            const std::string &amp; data_file_extension_,
            const std::string &amp; marks_path_,
            const std::string &amp; marks_file_extension_,
            const CompressionCodecPtr &amp; compression_codec_,
            size_t max_compress_block_size_,
            size_t estimated_size_,
            size_t aio_threshold_);

        String escaped_column_name;
        std::string data_file_extension;
        std::string marks_file_extension;

        /// compressed -&gt; compressed_buf -&gt; plain_hashing -&gt; plain_file
        std::unique_ptr&lt;WriteBufferFromFileBase&gt; plain_file;
        HashingWriteBuffer plain_hashing;
        CompressedWriteBuffer compressed_buf;
        HashingWriteBuffer compressed;

        /// marks -&gt; marks_file
        std::unique_ptr&lt;WriteBufferFromFileBase&gt; marks_file;
        HashingWriteBuffer marks;

        void finalize();

        void sync() const;

        void addToChecksums(IMergeTreeDataPart::Checksums &amp; checksums);
    };
</code></pre>
<p>stream负责将数据写入磁盘(s3), 这里面要提到WriteBuffer</p>
<pre><code>MergetreeData -&gt; MergetTreeDataPart -&gt; partWriter -&gt; stream -&gt; writeBuffer
</code></pre>
<p><img src="clickhouse/./dot/MergeTreeDataPartWriterOnDiskStream.svg" alt="stream" /></p>
<h4 id="mergetreedatapartwritercompact"><a class="header" href="#mergetreedatapartwritercompact">MergeTreeDataPartWriterCompact</a></h4>
<p>所有的column写在一起</p>
<p><img src="clickhouse/./dot/MergeTreeDataPartWriterCompact.svg" alt="compact-write" /></p>
<h4 id="mergetreedatapartwriterwide"><a class="header" href="#mergetreedatapartwriterwide">MergeTreeDataPartWriterWide</a></h4>
<p>每个column有自己的.bin和.mrk文件</p>
<h4 id="mergetreedatapartwriterinmemory"><a class="header" href="#mergetreedatapartwriterinmemory">MergeTreeDataPartWriterInMemory</a></h4>
<h2 id="imergetreedatapart"><a class="header" href="#imergetreedatapart">IMergeTreeDataPart</a></h2>
<h3 id="state"><a class="header" href="#state">state</a></h3>
<pre><code class="language-cpp">enum class State
{
    Temporary,       /// the part is generating now, it is not in data_parts list
    PreCommitted,    /// the part is in data_parts, but not used for SELECTs
    Committed,       /// active data part, used by current and upcoming SELECTs
    Outdated,        /// not active data part, but could be used by only current SELECTs, could be deleted after SELECTs finishes
    Deleting,        /// not active data part with identity refcounter, it is deleting right now by a cleaner
    DeleteOnDestroy, /// part was moved to another disk and should be deleted in own destructor
};
</code></pre>
<h3 id="readerwriter-interface"><a class="header" href="#readerwriter-interface">reader/writer interface</a></h3>
<p>IMergeTreeDataPart 封装了getReader和getWriter分别用于part的读写</p>
<pre><code class="language-cpp">    virtual MergeTreeReaderPtr getReader(
        const NamesAndTypesList &amp; columns_,
        const StorageMetadataPtr &amp; metadata_snapshot,
        const MarkRanges &amp; mark_ranges,
        UncompressedCache * uncompressed_cache,
        MarkCache * mark_cache,
        const MergeTreeReaderSettings &amp; reader_settings_,
        const ValueSizeMap &amp; avg_value_size_hints_ = ValueSizeMap{},
        const ReadBufferFromFileBase::ProfileCallback &amp; profile_callback_ = ReadBufferFromFileBase::ProfileCallback{}) const = 0;

    virtual MergeTreeWriterPtr getWriter(
        const NamesAndTypesList &amp; columns_list,
        const StorageMetadataPtr &amp; metadata_snapshot,
        const std::vector&lt;MergeTreeIndexPtr&gt; &amp; indices_to_recalc,
        const CompressionCodecPtr &amp; default_codec_,
        const MergeTreeWriterSettings &amp; writer_settings,
        const MergeTreeIndexGranularity &amp; computed_index_granularity = {}) const = 0;
</code></pre>
<h3 id="mergetreedatapartwide"><a class="header" href="#mergetreedatapartwide">MergeTreeDataPartWide</a></h3>
<pre><code class="language-cpp">/** In wide format data of each column is stored in one or several (for complex types) files.
  * Every data file is followed by marks file.
  * Can be used in tables with both adaptive and non-adaptive granularity.
  * This is the regular format of parts for MergeTree and suitable for big parts, as it's the most efficient.
  * Data part would be created in wide format if it's uncompressed size in bytes or number of rows would exceed
  * thresholds `min_bytes_for_wide_part` and `min_rows_for_wide_part`.
  */ 
</code></pre>
<h3 id="mergetreedatapartcompact"><a class="header" href="#mergetreedatapartcompact">MergeTreeDataPartCompact</a></h3>
<h3 id="mergetreedatapartinmemory"><a class="header" href="#mergetreedatapartinmemory">MergeTreeDataPartInMemory</a></h3>
<h3 id="mergetreedatapartwriterwide-1"><a class="header" href="#mergetreedatapartwriterwide-1">MergeTreeDataPartWriterWide</a></h3>
<p>没有看明白，哪个地方是写数据到disk里中的。</p>
<p><img src="clickhouse/./dot/MergeTreeDataPartWriterWide_write.svg" alt="write" /></p>
<h2 id="其他杂项todo"><a class="header" href="#其他杂项todo">其他杂项TODO</a></h2>
<h3 id="writebuffer"><a class="header" href="#writebuffer">WriteBuffer</a></h3>
<p>write buffer派生</p>
<p><img src="clickhouse/./dot/writebuffer.svg" alt="write buffer" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storagemergetree"><a class="header" href="#storagemergetree">StorageMergeTree</a></h1>
<!-- toc -->
<p>什么是MergeTree？原理是啥？有啥优缺点</p>
<blockquote>
<p>MergeTree存储结构需要对用户写入的数据做排序然后进行有序存储，数据有序存储带来两大核心优势：</p>
</blockquote>
<h2 id="struct"><a class="header" href="#struct">struct</a></h2>
<h2 id="read"><a class="header" href="#read">read</a></h2>
<p><img src="clickhouse/./dot/storage-merge-tree-read.svg" alt="storage-merge-tree-read" /></p>
<h2 id="write-1"><a class="header" href="#write-1">write</a></h2>
<p>write 返回一个<code>MergeTreeBlockOutputStream</code></p>
<p><img src="clickhouse/./dot/storage-merge-tree-write.svg" alt="storage merge tree write" /></p>
<h3 id="writetemppart-1"><a class="header" href="#writetemppart-1">WriteTempPart</a></h3>
<p><img src="clickhouse/./dot/merge-tree-data-writer-writeTmp.svg" alt="write tmp" /></p>
<h2 id="mutate-1"><a class="header" href="#mutate-1">mutate</a></h2>
<p><img src="clickhouse/./dot/storage-merge-tree-mutate.svg" alt="storage-merge-tree-mutate" /></p>
<h2 id="mergemutatetask"><a class="header" href="#mergemutatetask">mergeMutateTask</a></h2>
<p><img src="clickhouse/./dot/merge-mute-task.svg" alt="mergeMuteTask" /></p>
<h3 id="finalizemutatedpart"><a class="header" href="#finalizemutatedpart">finalizeMutatedPart</a></h3>
<p>Initialize and write to disk new part fields like checksums, columns,</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tidb"><a class="header" href="#tidb">TiDB</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tidb-学习资料整理"><a class="header" href="#tidb-学习资料整理">TiDB 学习资料整理</a></h1>
<h2 id="参考资料-1"><a class="header" href="#参考资料-1">参考资料</a></h2>
<p>本文主要摘自pingcap 如下几篇blog, 从整体上介绍了tidb/tikv的设计架构，以及为什么要这么设计，为了解决什么问题。
看完后能对tidb有个整体的认识。</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/25142743">TiDB 架构的演进和开发哲学</a></li>
<li><a href="https://pingcap.com/blog-cn/10-questions-tidb-structure/">十问 TiDB ：关于架构设计的一些思考</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-internal-1/">三篇文章了解 TiDB 技术内幕 - 说存储</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-internal-3/">三篇文章了解 TiDB 技术内幕 - 谈调度</a></li>
</ol>
<h2 id="tidb-整体架构"><a class="header" href="#tidb-整体架构">TiDb 整体架构</a></h2>
<p><img src="tidb/./tidb-arch-2.png" alt="tidb-arch2" /></p>
<p>TiDB包含三大核心组件，TiDB/TiKV/PD, 组件之间通过GRPC通信, 各自功能如下：<a href="https://pingcap.com/blog-cn/tidb-operator-introduction/">TiDB Operator，让 TiDB 成为真正的 Cloud-Native 数据库</a></p>
<ol>
<li>TiDB Server：主要负责 SQL 的解析器和优化器，它相当于计算执行层，同时也负责客户端接入和交互。</li>
<li>TiKV Server：是一套分布式的 Key-Value 存储引擎，它承担整个数据库的存储层，数据的水平扩展和多副本高可用特性都是在这一层实现。</li>
<li>PD Server：相当于分布式数据库的大脑，一方面负责收集和维护数据在各个 TiKV 节点的分布情况，另一方面 PD 承担调度器的角色，根据数据分布状况以及各个存储节点的负载来采取合适的调度策略，维持整个系统的平衡与稳定。</li>
</ol>
<p>TiDB/TiKV 背后对应的论文基础<a href="https://pingcap.com/blog-cn/how-do-we-build-tidb/">How do we build TiDB</a>, Google Spanner/F1, Raft.</p>
<p><img src="tidb/./tidb-arch-overview.png" alt="tidb-arc-overview" /></p>
<p><img src="tidb/./tikv-overview.png" alt="tikv-overview" /></p>
<h3 id="tidb-1"><a class="header" href="#tidb-1">tidb</a></h3>
<p><img src="tidb/./tidb-arch.png" alt="tidb" /></p>
<p>tidb开发选择从上往下开发，无缝兼容MYSQL协议。talk is cheap, show me the test，使用了大量的测试用例来保证正确性。</p>
<p><img src="tidb/./tidb-sql.png" alt="tidb-sql" /></p>
<h3 id="关系模型到-key-value-模型的映射"><a class="header" href="#关系模型到-key-value-模型的映射">关系模型到 Key-Value 模型的映射</a></h3>
<p><a href="https://pingcap.com/blog-cn/tidb-internal-2/#%E4%B8%89%E7%AF%87%E6%96%87%E7%AB%A0%E4%BA%86%E8%A7%A3-tidb-%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95---%E8%AF%B4%E8%AE%A1%E7%AE%97">三篇文章了解 TiDB 技术内幕 - 说计算</a></p>
<pre><code class="language-SQL">CREATE TABLE User {
	ID int,
	Name varchar(20),
	Role varchar(20),
	Age int,
	PRIMARY KEY (ID),
	Key idxAge (age)
};
</code></pre>
<p>每行数据按照如下规则进行编码成 Key-Value pair：</p>
<pre><code>Key: tablePrefix{tableID}_recordPrefixSep{rowID}
Value: [col1, col2, col3, col4]
</code></pre>
<p>其中 Key 的 tablePrefix/recordPrefixSep 都是特定的字符串常量，用于在 KV 空间内区分其他数据。
对于 Index 数据，会按照如下规则编码成 Key-Value pair：</p>
<pre><code>Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue
Value: rowID
</code></pre>
<p>注意上述编码规则中的 Key 里面的各种 xxPrefix 都是字符串常量，作用都是区分命名空间，以免不同类型的数据之间相互冲突，定义如下：</p>
<pre><code>var(
	tablePrefix     = []byte{'t'}
	recordPrefixSep = []byte(&quot;_r&quot;)
	indexPrefixSep  = []byte(&quot;_i&quot;)
)
</code></pre>
<h3 id="tikv"><a class="header" href="#tikv">tikv</a></h3>
<p><img src="tidb/./tidb-tikv.png" alt="tidb-tikv" /></p>
<p>TiKV 利用 Raft 来做数据复制，每个数据变更都会落地为一条 Raft 日志，通过 Raft 的日志复制功能，将数据安全可靠地同步到 Group 的多数节点中。
通过单机的 RocksDB，我们可以将数据快速地存储在磁盘上；通过 Raft，我们可以将数据复制到多台机器上，以防单机失效。数据的写入是通过 Raft 这一层的接口写入，而不是直接写 RocksDB。通过实现 Raft，我们拥有了一个分布式的 KV，现在再也不用担心某台机器挂掉了。</p>
<p><img src="tidb/./tikv-raft.png" alt="tikv-raft" /></p>
<p><img src="tidb/./tikv-region.png" alt="tikv-region" /></p>
<p>MVCC</p>
<p>很多数据库都会实现多版本控制（MVCC），TiKV 也不例外。设想这样的场景，两个 Client 同时去修改一个 Key 的 Value，如果没有 MVCC，就需要对数据上锁，在分布式场景下，可能会带来性能以及死锁问题。 TiKV 的 MVCC 实现是通过在 Key 后面添加 Version 来实现，简单来说，没有 MVCC 之前，可以把 TiKV 看做这样的：</p>
<h3 id="pd"><a class="header" href="#pd">pd</a></h3>
<p>下面问题值得仔细思考。</p>
<ol>
<li>如何保证同一个 Region 的多个 Replica 分布在不同的节点上？更进一步，如果在一台机器上启动多个 TiKV 实例，会有什么问题？</li>
<li>TiKV 集群进行跨机房部署用于容灾的时候，如何保证一个机房掉线，不会丢失 Raft Group 的多个 Replica？</li>
<li>添加一个节点进入 TiKV 集群之后，如何将集群中其他节点上的数据搬过来?</li>
<li>当一个节点掉线时，会出现什么问题？整个集群需要做什么事情？如果节点只是短暂掉线（重启服务），那么如何处理？如果节点是长时间掉线（磁盘故障，数据全部丢失），需要如何处理？</li>
<li>假设集群需要每个 Raft Group 有 N 个副本，那么对于单个 Raft Group 来说，Replica 数量可能会不够多（例如节点掉线，失去副本），也可能会 过于多（例如掉线的节点又回复正常，自动加入集群）。那么如何调节 Replica 个数？</li>
<li>读/写都是通过 Leader 进行，如果 Leader 只集中在少量节点上，会对集群有什么影响？</li>
<li>并不是所有的 Region 都被频繁的访问，可能访问热点只在少数几个 Region，这个时候我们需要做什么？</li>
<li>集群在做负载均衡的时候，往往需要搬迁数据，这种数据的迁移会不会占用大量的网络带宽、磁盘 IO 以及 CPU？进而影响在线服务？</li>
</ol>
<p>作为一个分布式高可用存储系统，必须满足的需求，包括四种：</p>
<ol>
<li>副本数量不能多也不能少</li>
<li>副本需要分布在不同的机器上</li>
<li>新加节点后，可以将其他节点上的副本迁移过来</li>
<li>节点下线后，需要将该节点的数据迁移走</li>
</ol>
<p>作为一个良好的分布式系统，需要优化的地方，包括：</p>
<ol>
<li>维持整个集群的 Leader 分布均匀</li>
<li>维持每个节点的储存容量均匀</li>
<li>维持访问热点分布均匀</li>
<li>控制 Balance 的速度，避免影响在线服务</li>
<li>管理节点状态，包括手动上线/下线节点，以及自动下线失效节点</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tidb-server-main-loop"><a class="header" href="#tidb-server-main-loop">TiDB Server Main Loop</a></h1>
<!-- toc -->
<p>跟着官方的<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-3/">tidb源码阅读博客</a>，看了TiDB main函数，大致了解了一个SQL的处理过程</p>
<h2 id="conn-session"><a class="header" href="#conn-session">conn session</a></h2>
<p>下图显示了TiDB中Accept一个mysql连接的处理流程，对于每个新的conn, TiDB会启动一个goroutine来处理这个conn, 并按照Mysql协议，处理不同的mysql cmd。
每个conn在server端会有个对应的session.</p>
<p>对于Query语句，会session.Execute生成一个执行器，返回一个resultSet, 最后调用<code>writeResultset</code>, 从ResultSet.Next中获取结果，然后将结果返回给客户端。</p>
<p><img src="tidb/./dot/tidb-server-main.svg" alt="tidb server main" /></p>
<p>一个sql语句执行过程中经过以下几个过程:</p>
<ol>
<li><code>ParseSQL</code> 将SQL语句解析为stmt ast tree</li>
<li><code>Compile</code> 将stmt ast tree 转换为physical plan</li>
<li><code>BuildExecutor</code> 创建executor</li>
<li><code>resultSet.Next</code> 驱动executor执行</li>
</ol>
<p><img src="tidb/./dot/sql-to-resultset.svg" alt="sql-to-resultset" /></p>
<h2 id="parsesql"><a class="header" href="#parsesql">ParseSQL</a></h2>
<h3 id="stmtnodes"><a class="header" href="#stmtnodes">StmtNodes</a></h3>
<p>StmtNode 接口定义</p>
<pre><code class="language-go">// Node is the basic element of the AST.
// Interfaces embed Node should have 'Node' name suffix.
type Node interface {
	// Restore returns the sql text from ast tree
	Restore(ctx *format.RestoreCtx) error
	// Accept accepts Visitor to visit itself.
	// The returned node should replace original node.
	// ok returns false to stop visiting.
	//
	// Implementation of this method should first call visitor.Enter,
	// assign the returned node to its method receiver, if skipChildren returns true,
	// children should be skipped. Otherwise, call its children in particular order that
	// later elements depends on former elements. Finally, return visitor.Leave.
	Accept(v Visitor) (node Node, ok bool)
	// Text returns the original text of the element.
	Text() string
	// SetText sets original text to the Node.
	SetText(text string)
}

// StmtNode represents statement node.
// Name of implementations should have 'Stmt' suffix.
type StmtNode interface {
	Node
	statement()
}
</code></pre>
<p>stmtNode实现种类和继承关系</p>
<p><img src="tidb/./dot/stmt-nodes.svg" alt="stmt-nodes" /></p>
<h2 id="compile"><a class="header" href="#compile">Compile</a></h2>
<p>Compile中首先使用<code>planbuilder</code>，将stmt ast 树转换为logical plan, 
然后<code>logicalOptimize</code>做基于规则的逻辑优化，<code>physicalOptimize</code>会根据
cost选择最佳的physical plan. 最后<code>postOptimize</code>还会做一波优化。</p>
<p><img src="tidb/./dot/sql-plan.svg" alt="sql-plan" /></p>
<h3 id="logical-plan-optimize"><a class="header" href="#logical-plan-optimize">logical plan optimize</a></h3>
<p>逻辑优化(Rule-based-Optimization, 简称RBO)，主要依据关系代数的等价交换规则做一些逻辑变换。</p>
<pre><code class="language-go">var optRuleList = []logicalOptRule{
	&amp;gcSubstituter{},
	&amp;columnPruner{},
	&amp;buildKeySolver{},
	&amp;decorrelateSolver{},
	&amp;aggregationEliminator{},
	&amp;projectionEliminator{},
	&amp;maxMinEliminator{},
	&amp;ppdSolver{},
	&amp;outerJoinEliminator{},
	&amp;partitionProcessor{},
	&amp;aggregationPushDownSolver{},
	&amp;pushDownTopNOptimizer{},
	&amp;joinReOrderSolver{},
	&amp;columnPruner{}, // column pruning again at last, note it will mess up the results of buildKeySolver
}
</code></pre>
<h3 id="physical-optimization"><a class="header" href="#physical-optimization">Physical Optimization</a></h3>
<p>物理优化，主要通过对查询的数据读取、表连接方式、表连接顺序、排序等技术进行优化。
基于代价的优化（CBO), 依赖于统计信息的准确性与及时性，执行计划会及时的根据数据变换做对应的调整</p>
<p>主要实现在函数<code>findBestTask</code>中，每个logical plan都实现了这个findBestTask接口, 具体实现在<code>planner/core/find_best_task.go</code>中
其中<code>baseLogicalPlan.findBestTask</code> 为封装的基类函数
在attach2Task中会更新task的cost</p>
<h4 id="findbesttask"><a class="header" href="#findbesttask">findBestTask</a></h4>
<p><img src="tidb/./dot/find-best-task.svg" alt="" /></p>
<pre><code class="language-go">	// findBestTask converts the logical plan to the physical plan. It's a new interface.
	// It is called recursively from the parent to the children to create the result physical plan.
	// Some logical plans will convert the children to the physical plans in different ways, and return the one
	// With the lowest cost and how many plans are found in this function.
	// planCounter is a counter for planner to force a plan.
	// If planCounter &gt; 0, the clock_th plan generated in this function will be returned.
	// If planCounter = 0, the plan generated in this function will not be considered.
	// If planCounter = -1, then we will not force plan.
	findBestTask(prop *property.PhysicalProperty, planCounter *PlanCounterTp) (task, int64, error)

	// attach2Task makes the current physical plan as the father of task's physicalPlan and updates the cost of
	// current task. If the child's task is cop task, some operator may close this task and return a new rootTask.
	attach2Task(...task) task
</code></pre>
<p>findBestTask最后的输出为task, 可以使用explain查看最后生成的task</p>
<pre><code class="language-sql">&gt;create table t (id varchar(31), name varchar(50), age int, key id_idx(id));
&gt;explain select name, age from t where age &gt; 18;
+-----------------------+---------+-----------+---------------+--------------------------------+
| id                    | estRows | task      | access object | operator info                  |
+-----------------------+---------+-----------+---------------+--------------------------------+
| Projection_4          | 0.00    | root      |               | tests.t.name, tests.t.age      |
| └─TableReader_7       | 0.00    | root      |               | data:Selection_6               |
|   └─Selection_6       | 0.00    | cop[tikv] |               | eq(tests.t.id, &quot;pingcap&quot;)      |
|     └─TableFullScan_5 | 1.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo |
+-----------------------+---------+-----------+---------------+--------------------------------+
</code></pre>
<h3 id="task"><a class="header" href="#task">task</a></h3>
<p>cop task 是指被下推到 KV 端分布式执行的计算任务，root task 是指在 TiDB 端单点执行的计算任务。</p>
<pre><code class="language-go">type task interface {
	count() float64
	addCost(cost float64)
	cost() float64
	copy() task
	plan() PhysicalPlan
	invalid() bool
}
</code></pre>
<h4 id="roottask"><a class="header" href="#roottask">rootTask</a></h4>
<pre><code class="language-go">// rootTask is the final sink node of a plan graph. It should be a single goroutine on tidb.
type rootTask struct {
	p   PhysicalPlan
	cst float64
}
</code></pre>
<h4 id="coptask"><a class="header" href="#coptask">copTask</a></h4>
<pre><code class="language-go">// copTask is a task that runs in a distributed kv store.
// TODO: In future, we should split copTask to indexTask and tableTask.
type copTask struct {
	indexPlan PhysicalPlan
	tablePlan PhysicalPlan
	cst       float64
	// indexPlanFinished means we have finished index plan.
	indexPlanFinished bool
	// keepOrder indicates if the plan scans data by order.
	keepOrder bool
	// doubleReadNeedProj means an extra prune is needed because
	// in double read case, it may output one more column for handle(row id).
	doubleReadNeedProj bool

	extraHandleCol   *expression.Column
	commonHandleCols []*expression.Column
	// tblColHists stores the original stats of DataSource, it is used to get
	// average row width when computing network cost.
	tblColHists *statistics.HistColl
	// tblCols stores the original columns of DataSource before being pruned, it
	// is used to compute average row width when computing scan cost.
	tblCols           []*expression.Column
	idxMergePartPlans []PhysicalPlan
	// rootTaskConds stores select conditions containing virtual columns.
	// These conditions can't push to TiKV, so we have to add a selection for rootTask
	rootTaskConds []expression.Expression

	// For table partition.
	partitionInfo PartitionInfo
}
</code></pre>
<h3 id="execstmt"><a class="header" href="#execstmt">ExecStmt</a></h3>
<p>Compile后最后返回的数据结构为<code>ExecStmt</code>, 接下来会使用<code>buildExecutor</code>把ExecSmt
转变为executor.</p>
<pre><code class="language-go">// ExecStmt implements the sqlexec.Statement interface, it builds a planner.Plan to an sqlexec.Statement.
type ExecStmt struct {
	// GoCtx stores parent go context.Context for a stmt.
	GoCtx context.Context
	// InfoSchema stores a reference to the schema information.
	InfoSchema infoschema.InfoSchema
	// Plan stores a reference to the final physical plan.
	Plan plannercore.Plan
	// Text represents the origin query text.
	Text string

	StmtNode ast.StmtNode

	Ctx sessionctx.Context

	// LowerPriority represents whether to lower the execution priority of a query.
	LowerPriority     bool
	isPreparedStmt    bool
	isSelectForUpdate bool
	retryCount        uint
	retryStartTime    time.Time

	// OutputNames will be set if using cached plan
	OutputNames []*types.FieldName
	PsStmt      *plannercore.CachedPrepareStmt
}
</code></pre>
<h2 id="executor-2"><a class="header" href="#executor-2">Executor</a></h2>
<p>根据physical plan生成相应的executor, Executor interface如下, 使用了Volcano模型，接口用起来和迭代器差不多，采用Open-Next-Close套路来使用。</p>
<ul>
<li><code>Open</code>: 做一些初始化工作</li>
<li><code>Next</code>: 执行具体操作 </li>
<li><code>Close</code>: 做一些清理操作</li>
</ul>
<pre><code class="language-go">// Executor is the physical implementation of a algebra operator.
//
// In TiDB, all algebra operators are implemented as iterators, i.e., they
// support a simple Open-Next-Close protocol. See this paper for more details:
//
// &quot;Volcano-An Extensible and Parallel Query Evaluation System&quot;
//
// Different from Volcano's execution model, a &quot;Next&quot; function call in TiDB will
// return a batch of rows, other than a single row in Volcano.
// NOTE: Executors must call &quot;chk.Reset()&quot; before appending their results to it.
type Executor interface {
	base() *baseExecutor
	Open(context.Context) error
	Next(ctx context.Context, req *chunk.Chunk) error
	Close() error
	Schema() *expression.Schema
}
</code></pre>
<p><img src="tidb/./dot/sql-executor.svg" alt="sql-executor" /></p>
<h2 id="recordset"><a class="header" href="#recordset">RecordSet</a></h2>
<p>executor的next方法，将由recordset的next来驱动不断地执行。</p>
<p>下图摘自<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-3/">2</a>
executor本身</p>
<p><img src="tidb/./dot/executor-next.png" alt="" /></p>
<h3 id="handlenodelay"><a class="header" href="#handlenodelay">handleNoDelay</a></h3>
<p>Insert 这种不需要返回数据的语句，只需要把语句执行完成即可。这类语句也是通过 Next 驱动执行，驱动点在构造 recordSet 结构之前</p>
<p><img src="tidb/./dot/sql-nodelay-next.svg" alt="sql-nodelay-next" /></p>
<h3 id="recordset-driver"><a class="header" href="#recordset-driver">RecordSet driver</a></h3>
<p><img src="tidb/./dot/sql-recordset-driver.svg" alt="sql-recordset-driver" /></p>
<pre><code class="language-go">// RecordSet is an abstract result set interface to help get data from Plan.
type RecordSet interface {
	// Fields gets result fields.
	Fields() []*ast.ResultField

	// Next reads records into chunk.
	Next(ctx context.Context, req *chunk.Chunk) error

	// NewChunk create a chunk.
	NewChunk() *chunk.Chunk

	// Close closes the underlying iterator, call Next after Close will
	// restart the iteration.
	Close() error
}
</code></pre>
<p>RecordSet Next方法接口的实现.</p>
<pre><code class="language-go">// Next use uses recordSet's executor to get next available chunk for later usage.
// If chunk does not contain any rows, then we update last query found rows in session variable as current found rows.
// The reason we need update is that chunk with 0 rows indicating we already finished current query, we need prepare for
// next query.
// If stmt is not nil and chunk with some rows inside, we simply update last query found rows by the number of row in chunk.
func (a *recordSet) Next(ctx context.Context, req *chunk.Chunk) error {
	err := Next(ctx, a.executor, req)
	if err != nil {
		a.lastErr = err
		return err
	}
	numRows := req.NumRows()
	if numRows == 0 {
		if a.stmt != nil {
			a.stmt.Ctx.GetSessionVars().LastFoundRows = a.stmt.Ctx.GetSessionVars().StmtCtx.FoundRows()
		}
		return nil
	}
	if a.stmt != nil {
		a.stmt.Ctx.GetSessionVars().StmtCtx.AddFoundRows(uint64(numRows))
	}
	return nil
}
</code></pre>
<p>在writeResult时候不断调用RecordSet的Next方法，去驱动调用executor的Next;</p>
<pre><code class="language-go">// writeChunks writes data from a Chunk, which filled data by a ResultSet, into a connection.
// binary specifies the way to dump data. It throws any error while dumping data.
// serverStatus, a flag bit represents server information
func (cc *clientConn) writeChunks(ctx context.Context, rs ResultSet, binary bool, serverStatus uint16) error {
	data := cc.alloc.AllocWithLen(4, 1024)
	req := rs.NewChunk()
  //...
	for {
		// Here server.tidbResultSet implements Next method.
		err := rs.Next(ctx, req)
    /...
		rowCount := req.NumRows()
    //...
		for i := 0; i &lt; rowCount; i++ {
			data = data[0:4]
      /...
			if err = cc.writePacket(data); err != nil {
				return err
			}
      //...
    }
  }
	return cc.writeEOF(serverStatus)
}
</code></pre>
<h2 id="ref-6"><a class="header" href="#ref-6">Ref</a></h2>
<ol>
<li><a href="https://www.bookstack.cn/read/pingcap-docs-cn/sql-understanding-the-query-execution-plan.md">理解TIDB执行计划</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-3/">TiDB 源码阅读系列文章（三）SQL 的一生</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="insert-语句"><a class="header" href="#insert-语句">Insert 语句</a></h1>
<!-- toc -->
<h2 id="insertstmt"><a class="header" href="#insertstmt">InsertStmt</a></h2>
<h3 id="parse"><a class="header" href="#parse">Parse</a></h3>
<p>parser.y 将insert语句解析为InsertStmt</p>
<pre><code class="language-yacc">TableName:
	Identifier
	{
		$$ = &amp;ast.TableName{Name: model.NewCIStr($1)}
	}
|	Identifier '.' Identifier
	{
		$$ = &amp;ast.TableName{Schema: model.NewCIStr($1), Name: model.NewCIStr($3)}
	}

InsertIntoStmt:
	&quot;INSERT&quot; TableOptimizerHints PriorityOpt IgnoreOptional IntoOpt TableName PartitionNameListOpt InsertValues OnDuplicateKeyUpdate
	{
		x := $8.(*ast.InsertStmt)
		x.Priority = $3.(mysql.PriorityEnum)
		x.IgnoreErr = $4.(bool)
		// Wraps many layers here so that it can be processed the same way as select statement.
		ts := &amp;ast.TableSource{Source: $6.(*ast.TableName)}
		x.Table = &amp;ast.TableRefsClause{TableRefs: &amp;ast.Join{Left: ts}}
		if $9 != nil {
			x.OnDuplicate = $9.([]*ast.Assignment)
		}
		if $2 != nil {
			x.TableHints = $2.([]*ast.TableOptimizerHint)
		}
		x.PartitionNames = $7.([]model.CIStr)
		$$ = x
	}
</code></pre>
<p>解析后的InsertStmt结构如下</p>
<pre><code class="language-go">// InsertStmt is a statement to insert new rows into an existing table.
// See https://dev.mysql.com/doc/refman/5.7/en/insert.html
type InsertStmt struct {
	dmlNode

	IsReplace   bool
	IgnoreErr   bool
	Table       *TableRefsClause
	Columns     []*ColumnName
	Lists       [][]ExprNode
	Setlist     []*Assignment
	Priority    mysql.PriorityEnum
	OnDuplicate []*Assignment
	Select      ResultSetNode
	// TableHints represents the table level Optimizer Hint for join type.
	TableHints     []*TableOptimizerHint
	PartitionNames []model.CIStr
}
</code></pre>
<h3 id="preprocess"><a class="header" href="#preprocess">Preprocess</a></h3>
<p>使用preprocess函数补全table的info信息, 找到要插入table.</p>
<p><img src="tidb/./dot/preprocess-tableName.svg" alt="" /></p>
<p>schema 在tidb server内存中信息如下：</p>
<p><img src="tidb/./dot/model.svg" alt="model" /></p>
<h3 id="planbuilderbuildinsert"><a class="header" href="#planbuilderbuildinsert">PlanBuilder.buildInsert</a></h3>
<p><img src="tidb/./dot/insert-stmt.svg" alt="insert stmt" /></p>
<h2 id="insertexec"><a class="header" href="#insertexec">InsertExec</a></h2>
<p>数据结构定义如下</p>
<p><img src="tidb/./dot/insert-exec.svg" alt="insert exec" /></p>
<pre><code class="language-go">// InsertExec represents an insert executor.
type InsertExec struct {
	*InsertValues
	OnDuplicate    []*expression.Assignment
	evalBuffer4Dup chunk.MutRow
	curInsertVals  chunk.MutRow
	row4Update     []types.Datum

	Priority mysql.PriorityEnum
}

// InsertValues is the data to insert.
type InsertValues struct {
	baseExecutor

	rowCount       uint64
	curBatchCnt    uint64
	maxRowsInBatch uint64
	lastInsertID   uint64

	SelectExec Executor

	Table   table.Table
	Columns []*ast.ColumnName
	Lists   [][]expression.Expression
	SetList []*expression.Assignment

	GenExprs []expression.Expression

	insertColumns []*table.Column

	// colDefaultVals is used to store casted default value.
	// Because not every insert statement needs colDefaultVals, so we will init the buffer lazily.
	colDefaultVals  []defaultVal
	evalBuffer      chunk.MutRow
	evalBufferTypes []*types.FieldType

	allAssignmentsAreConstant bool

	hasRefCols     bool
	hasExtraHandle bool

	// Fill the autoID lazily to datum. This is used for being compatible with JDBC using getGeneratedKeys().
	// `insert|replace values` can guarantee consecutive autoID in a batch.
	// Other statements like `insert select from` don't guarantee consecutive autoID.
	// https://dev.mysql.com/doc/refman/8.0/en/innodb-auto-increment-handling.html
	lazyFillAutoID bool
	memTracker     *memory.Tracker

	stats *InsertRuntimeStat
}

type baseExecutor struct {
	ctx           sessionctx.Context
	id            int
	schema        *expression.Schema // output schema
	initCap       int
	maxChunkSize  int
	children      []Executor
	retFieldTypes []*types.FieldType
	runtimeStats  *execdetails.BasicRuntimeStats
}
</code></pre>
<h3 id="next"><a class="header" href="#next">Next</a></h3>
<p>最终会调用Table.AddRecord 接口向表中插入记录</p>
<p><img src="tidb/./dot/insert-exec-next.svg" alt="insert exec next" /></p>
<h2 id="table-addrecord"><a class="header" href="#table-addrecord">Table AddRecord</a></h2>
<h3 id="allochandleids"><a class="header" href="#allochandleids">allocHandleIDs</a></h3>
<h3 id="encode-keyvalue"><a class="header" href="#encode-keyvalue">encode key/value</a></h3>
<p>pingcap的博客<a href="https://pingcap.com/blog-cn/tidb-internal-2/">三篇文章了解 TiDB 技术内幕 - 说计算</a>,
中介绍了row/index的key value编码方式：</p>
<p>TiDB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID（如果表有整数型的 Primary Key，那么会用 Primary Key 的值当做 RowID），其中 TableID 在整个集群内唯一，IndexID/RowID 在表内唯一，这些 ID 都是 int64 类型。</p>
<p>row key/value 编码如下:</p>
<pre><code>Key: tablePrefix{tableID}_recordPrefixSep{rowID}
Value: [col1, col2, col3, col4]
</code></pre>
<p>index的key编码如下:</p>
<pre><code>Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue
Value: rowID
</code></pre>
<p><img src="tidb/./dot/table_addrecord.svg" alt="table ad record" /></p>
<h2 id="kv"><a class="header" href="#kv">KV</a></h2>
<h3 id="transaction-1"><a class="header" href="#transaction-1">Transaction</a></h3>
<p>最终commit时候，首先将MemBuffer转为mutation,
最后提交到tikv。</p>
<p><img src="tidb/./dot/txn-commit.svg" alt="txn commit" /></p>
<p>commit 的被调用流程如下，
<img src="tidb/./dot/txn-commit-bt.svg" alt="txn commit bt" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="select-语句"><a class="header" href="#select-语句">Select 语句</a></h1>
<!-- toc -->
<h2 id="selectstmt"><a class="header" href="#selectstmt">SelectStmt</a></h2>
<pre><code class="language-go">type SelectStmt struct {
	dmlNode
	resultSetNode

	// SelectStmtOpts wraps around select hints and switches.
	*SelectStmtOpts
	// Distinct represents whether the select has distinct option.
	Distinct bool
	// From is the from clause of the query.
	From *TableRefsClause
	// Where is the where clause in select statement.
	Where ExprNode
	// Fields is the select expression list.
	Fields *FieldList
	// GroupBy is the group by expression list.
	GroupBy *GroupByClause
	// Having is the having condition.
	Having *HavingClause
	// WindowSpecs is the window specification list.
	WindowSpecs []WindowSpec
	// OrderBy is the ordering expression list.
	OrderBy *OrderByClause
	// Limit is the limit clause.
	Limit *Limit
	// LockInfo is the lock type
	LockInfo *SelectLockInfo
	// TableHints represents the table level Optimizer Hint for join type
	TableHints []*TableOptimizerHint
	// IsInBraces indicates whether it's a stmt in brace.
	IsInBraces bool
	// QueryBlockOffset indicates the order of this SelectStmt if counted from left to right in the sql text.
	QueryBlockOffset int
	// SelectIntoOpt is the select-into option.
	SelectIntoOpt *SelectIntoOption
	// AfterSetOperator indicates the SelectStmt after which type of set operator
	AfterSetOperator *SetOprType
	// Kind refer to three kind of statement: SelectStmt, TableStmt and ValuesStmt
	Kind SelectStmtKind
	// Lists is filled only when Kind == SelectStmtKindValues
	Lists []*RowExpr
}
</code></pre>
<h2 id="logicalplan"><a class="header" href="#logicalplan">LogicalPlan</a></h2>
<p><img src="tidb/./dot/build_select_plan.svg" alt="build select plan" /></p>
<h2 id="physicalplan"><a class="header" href="#physicalplan">PhysicalPlan</a></h2>
<h3 id="datasourcefindbesttask"><a class="header" href="#datasourcefindbesttask">DataSource.findBestTask</a></h3>
<p><img src="tidb/./dot/datasource-findbesttask.svg" alt="data source findBestTask" /></p>
<h3 id="logicaljoinexhaustphysicalplans"><a class="header" href="#logicaljoinexhaustphysicalplans">LogicalJoin.exhaustPhysicalPlans</a></h3>
<p><img src="tidb/./dot/logicaljoin_exhaustPhysicalPlans.svg" alt="logicaljoin exhaustPhysicalPlans" /></p>
<h2 id="executor-3"><a class="header" href="#executor-3">Executor</a></h2>
<h3 id="selectionexec"><a class="header" href="#selectionexec">SelectionExec</a></h3>
<pre><code>// SelectionExec represents a filter executor.
type SelectionExec struct {
	baseExecutor

	batched     bool
	filters     []expression.Expression
	selected    []bool
	inputIter   *chunk.Iterator4Chunk
	inputRow    chunk.Row
	childResult *chunk.Chunk

	memTracker *memory.Tracker
}
</code></pre>
<p>生成的计划</p>
<pre><code class="language-sql">explain select name, age from t where id = 'pingcap';

+-----------------------+---------+-----------+---------------+--------------------------------+
| id                    | estRows | task      | access object | operator info                  |
+-----------------------+---------+-----------+---------------+--------------------------------+
| Projection_4          | 0.00    | root      |               | tests.t.name, tests.t.age      |
| └─TableReader_7       | 0.00    | root      |               | data:Selection_6               |
|   └─Selection_6       | 0.00    | cop[tikv] |               | eq(tests.t.id, &quot;pingcap&quot;)      |
|     └─TableFullScan_5 | 3.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo |
+-----------------------+---------+-----------+---------------+--------------------------------+
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tidb-基本数据类型"><a class="header" href="#tidb-基本数据类型">TiDB 基本数据类型</a></h1>
<!-- toc -->
<h2 id="datum"><a class="header" href="#datum">Datum</a></h2>
<pre><code class="language-go">// Datum is a data box holds different kind of data.
// It has better performance and is easier to use than `interface{}`.
type Datum struct {
	k         byte        // datum kind.
	decimal   uint16      // decimal can hold uint16 values.
	length    uint32      // length can hold uint32 values.
	i         int64       // i can hold int64 uint64 float64 values.
	collation string      // collation hold the collation information for string value.
	b         []byte      // b can hold string or []byte values.
	x         interface{} // x hold all other types.
}

const (
	KindNull          byte = 0
	KindInt64         byte = 1
	KindUint64        byte = 2
	KindFloat32       byte = 3
	KindFloat64       byte = 4
	KindString        byte = 5
	KindBytes         byte = 6
	KindBinaryLiteral byte = 7 // Used for BIT / HEX literals.
	KindMysqlDecimal  byte = 8
	KindMysqlDuration byte = 9
	KindMysqlEnum     byte = 10
	KindMysqlBit      byte = 11 // Used for BIT table column values.
	KindMysqlSet      byte = 12
	KindMysqlTime     byte = 13
	KindInterface     byte = 14
	KindMinNotNull    byte = 15
	KindMaxValue      byte = 16
	KindRaw           byte = 17
	KindMysqlJSON     byte = 18
)
</code></pre>
<h2 id="chunk"><a class="header" href="#chunk">Chunk</a></h2>
<pre><code class="language-go">// Chunk stores multiple rows of data in Apache Arrow format.
// See https://arrow.apache.org/docs/format/Columnar.html#physical-memory-layout
// Values are appended in compact format and can be directly accessed without decoding.
// When the chunk is done processing, we can reuse the allocated memory by resetting it.
type Chunk struct {
	// sel indicates which rows are selected.
	// If it is nil, all rows are selected.
	sel []int

	columns []*Column
	// numVirtualRows indicates the number of virtual rows, which have zero Column.
	// It is used only when this Chunk doesn't hold any data, i.e. &quot;len(columns)==0&quot;.
	numVirtualRows int
	// capacity indicates the max number of rows this chunk can hold.
	// TODO: replace all usages of capacity to requiredRows and remove this field
	capacity int

	// requiredRows indicates how many rows the parent executor want.
	requiredRows int
}
</code></pre>
<h3 id="column"><a class="header" href="#column">Column</a></h3>
<p>Column offsets[i]表示第i个元素在data中的偏移, data和elmBuf分别作用是啥？
elmBuf用来临时append value,将value转为[]byte，然后再append到data上
fixed sized的offsfet数组就不用了，可以直接算出来，这样就省下了offset这个数组.</p>
<pre><code class="language-go">// Column stores one column of data in Apache Arrow format.
// See https://arrow.apache.org/docs/memory_layout.html
type Column struct {
	length     int
	nullBitmap []byte // bit 0 is null, 1 is not null
	offsets    []int64
	data       []byte
	elemBuf    []byte
}
</code></pre>
<h3 id="row"><a class="header" href="#row">Row</a></h3>
<pre><code class="language-go">// Row represents a row of data, can be used to access values.
type Row struct {
	c   *Chunk
	idx int
}
</code></pre>
<h2 id="range"><a class="header" href="#range">Range</a></h2>
<pre><code class="language-go">// Range represents a range generated in physical plan building phase.
type Range struct {
	LowVal  []types.Datum
	HighVal []types.Datum

	LowExclude  bool // Low value is exclusive.
	HighExclude bool // High value is exclusive.
}
</code></pre>
<h2 id="rowcontainer"><a class="header" href="#rowcontainer">RowContainer</a></h2>
<pre><code class="language-go">// RowContainer provides a place for many rows, so many that we might want to spill them into disk.
type RowContainer struct {
	m struct {
		// RWMutex guarantees spill and get operator for rowContainer is mutually exclusive.
		sync.RWMutex
		// records stores the chunks in memory.
		records *List
		// recordsInDisk stores the chunks in disk.
		recordsInDisk *ListInDisk
		// spillError stores the error when spilling.
		spillError error
	}

	fieldType []*types.FieldType
	chunkSize int
	numRow    int

	memTracker  *memory.Tracker
	diskTracker *disk.Tracker
	actionSpill *SpillDiskAction
}
</code></pre>
<p><img src="tidb/./dot/chunk_RowContainer.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="expression"><a class="header" href="#expression">expression</a></h1>
<h2 id="expression-1"><a class="header" href="#expression-1">Expression</a></h2>
<p><img src="tidb/./dot/expression.svg" alt="" /></p>
<h2 id="builtinfunc"><a class="header" href="#builtinfunc">builtinFunc</a></h2>
<p><img src="tidb/./dot/builtinFunc.svg" alt="" /></p>
<h2 id="column-1"><a class="header" href="#column-1">Column</a></h2>
<h2 id="schema"><a class="header" href="#schema">Schema</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ddl"><a class="header" href="#ddl">DDL</a></h1>
<p>本文主要描述TiDB在分布式场景下支持无锁schema变更。</p>
<p>Schema 信息会存储在TiKV中，每个TiDB server内存中也会有个Schema信息。</p>
<!-- toc -->
<h2 id="schema-1"><a class="header" href="#schema-1">Schema</a></h2>
<h3 id="schema-in-tikv"><a class="header" href="#schema-in-tikv">Schema in TiKV</a></h3>
<p>Schema在kv中的存储形式如下</p>
<pre><code class="language-go">//meta/meta.go // Meta structure:
//	NextGlobalID -&gt; int64
//	SchemaVersion -&gt; int64
//	DBs -&gt; {
//		DB:1 -&gt; db meta data []byte
//		DB:2 -&gt; db meta data []byte
//	}
//	DB:1 -&gt; {
//		Table:1 -&gt; table meta data []byte
//		Table:2 -&gt; table meta data []byte
//		TID:1 -&gt; int64
//		TID:2 -&gt; int64
//	}
//
</code></pre>
<p>TiDB <code>meta/meta.go</code>模块封装了对存储在TiKV中schema进行的操作，在ddl owner节点在
<code>runDDLJobs</code>时候，会调用meta的方法, 来修改schema。
TiDB <code>loadSchemaInLoop</code> 中也会用到meta方法来加载schema.</p>
<p>模块层次之间调用如下图所示:</p>
<p><img src="tidb/./dot/schema-meta.svg" alt="schema mata" /></p>
<pre><code class="language-go">// Meta is for handling meta information in a transaction.
type Meta struct {
	txn        *structure.TxStructure
	StartTS    uint64 // StartTS is the txn's start TS.
	jobListKey JobListKeyType
}

// TxStructure supports some simple data structures like string, hash, list, etc... and
// you can use these in a transaction.
type TxStructure struct {
	reader     kv.Retriever
	readWriter kv.RetrieverMutator
	prefix     []byte
}

// RetrieverMutator is the interface that groups Retriever and Mutator interfaces.
type RetrieverMutator interface {
	Retriever
	Mutator
}

// Getter is the interface for the Get method.
type Getter interface {
	// Get gets the value for key k from kv store.
	// If corresponding kv pair does not exist, it returns nil and ErrNotExist.
	Get(ctx context.Context, k Key) ([]byte, error)
}
// Retriever is the interface wraps the basic Get and Seek methods.
type Retriever interface {
	Getter
	// Iter creates an Iterator positioned on the first entry that k &lt;= entry's key.
	// If such entry is not found, it returns an invalid Iterator with no error.
	// It yields only keys that &lt; upperBound. If upperBound is nil, it means the upperBound is unbounded.
	// The Iterator must be Closed after use.
	Iter(k Key, upperBound Key) (Iterator, error)

	// IterReverse creates a reversed Iterator positioned on the first entry which key is less than k.
	// The returned iterator will iterate from greater key to smaller key.
	// If k is nil, the returned iterator will be positioned at the last key.
	// TODO: Add lower bound limit
	IterReverse(k Key) (Iterator, error)
}

// Mutator is the interface wraps the basic Set and Delete methods.
type Mutator interface {
	// Set sets the value for key k as v into kv store.
	// v must NOT be nil or empty, otherwise it returns ErrCannotSetNilValue.
	Set(k Key, v []byte) error
	// Delete removes the entry for key k from kv store.
	Delete(k Key) error
}
</code></pre>
<h3 id="schema-in-tidb"><a class="header" href="#schema-in-tidb">Schema in TiDB</a></h3>
<p>TiDB 使用Schema来将关系数据库中的table/index等映射到TiKV的kv存储中。
Schema本身也是以kv的形式保存在TiKV中的。 TiDB是无状态的，而且在TiDB内存 
中也加载这一份Schema, 在TiDB server中infoSchema在内存中结构如下</p>
<p><img src="tidb/./dot/model.svg" alt="model" /></p>
<h3 id="schema-modification"><a class="header" href="#schema-modification">Schema Modification</a></h3>
<p>TiDB ddl 请求处理请求流程如下图所示(摘自<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a>)
<img src="tidb/./dot/ddl-flow.png" alt="ddl flow" /></p>
<p>每个tidb server都会起一个ddl worker，但只有一个节点的
ddl worker会被选为owner。</p>
<p>owner节点的ddl worker 从ddl job queue 中取job
执行job, 调用<code>Meta.go</code> 中定义的<code>CreateDatabase</code>等接口
修改存储在TiKV中的schema。</p>
<p>其他TiDB server收到ddl 请求，只用把这个请求转ddl job 放入ddl job queue中
即可。</p>
<p><img src="tidb/./dot/ddl-schema-flow.svg" alt="ddl-schema-flow" /></p>
<p>问题：每个tidb server是怎么更新自己内存中的schema 信息的？
怎么知道内存中的schema已经过期了的？</p>
<p>owner 节点的ddl worker handleDDLJobQueue 主要调用关系如下图所示：</p>
<p><img src="tidb/./dot/ddl_worker.svg" alt="ddl worker" /></p>
<h4 id="owner-选举"><a class="header" href="#owner-选举">Owner 选举</a></h4>
<p><img src="tidb/./dot/campaign-owner.svg" alt="owner campaign" /></p>
<h3 id="tidb-load-schema"><a class="header" href="#tidb-load-schema">TiDB load schema</a></h3>
<p>TiDB每隔lease/2 就会去Tikv中去reload schema, 首先会检查版本号，如果tikv中版本号和TiDB
中版本一致的话，就不用继续加载了。否则，<code>tryLoadSchemaDiffs</code>先尝试加载schemaDiff, 如果不行的话，
调用<code>fetchAllSchemasWithTables</code>会加载所有的schema</p>
<p><img src="tidb/./dot/schema-load.svg" alt="schema-load" /></p>
<h2 id="online-schema-change"><a class="header" href="#online-schema-change">Online Schema Change</a></h2>
<h3 id="schema-state"><a class="header" href="#schema-state">Schema state</a></h3>
<p><img src="tidb/./dot/schema-state.svg" alt="schema state" /></p>
<h3 id="ddl-job"><a class="header" href="#ddl-job">DDL Job</a></h3>
<p>TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 owner 节点的 worker 串行执行的。</p>
<ul>
<li>worker：每个节点都有一个 worker 用来处理 DDL 操作。</li>
<li>owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。</li>
</ul>
<p>以上内容摘自<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">4</a></p>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a></li>
<li><a href="https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md">TiDB 的异步 schema 变更实现</a></li>
<li><a href="http://zimulala.github.io/2017/12/24/optimize/">TiDB 异步Schema 变更优化</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-存储"><a class="header" href="#schema-存储">Schema 存储</a></h1>
<!-- toc -->
<h2 id="schema-存储格式"><a class="header" href="#schema-存储格式">Schema 存储格式</a></h2>
<p>Schema在kv中的存储形式如下</p>
<pre><code class="language-go">//meta/meta.go // Meta structure:
//	NextGlobalID -&gt; int64
//	SchemaVersion -&gt; int64
//	DBs -&gt; {
//		DB:1 -&gt; db meta data []byte
//		DB:2 -&gt; db meta data []byte
//	}
//	DB:1 -&gt; {
//		Table:1 -&gt; table meta data []byte
//		Table:2 -&gt; table meta data []byte
//		TID:1 -&gt; int64
//		TID:2 -&gt; int64
//	}
//
</code></pre>
<h2 id="meta"><a class="header" href="#meta">meta</a></h2>
<p>TiDB <code>meta/meta.go</code>模块封装了对存储在TiKV中schema进行的操作</p>
<ol>
<li>ddl owner节点在 <code>runDDLJobs</code>时候，会调用meta的方法修改schema。</li>
<li>TiDB <code>loadSchemaInLoop</code> 中调用meta方法来加载schema.</li>
</ol>
<p>模块层次之间调用如下图所示:</p>
<p><img src="tidb/./dot/schema-meta.svg" alt="schema mata" /></p>
<pre><code class="language-go">// Meta is for handling meta information in a transaction.
type Meta struct {
	txn        *structure.TxStructure
	StartTS    uint64 // StartTS is the txn's start TS.
	jobListKey JobListKeyType
}

// TxStructure supports some simple data structures like string, hash, list, etc... and
// you can use these in a transaction.
type TxStructure struct {
	reader     kv.Retriever
	readWriter kv.RetrieverMutator
	prefix     []byte
}

// RetrieverMutator is the interface that groups Retriever and Mutator interfaces.
type RetrieverMutator interface {
	Retriever
	Mutator
}

// Getter is the interface for the Get method.
type Getter interface {
	// Get gets the value for key k from kv store.
	// If corresponding kv pair does not exist, it returns nil and ErrNotExist.
	Get(ctx context.Context, k Key) ([]byte, error)
}
// Retriever is the interface wraps the basic Get and Seek methods.
type Retriever interface {
	Getter
	// Iter creates an Iterator positioned on the first entry that k &lt;= entry's key.
	// If such entry is not found, it returns an invalid Iterator with no error.
	// It yields only keys that &lt; upperBound. If upperBound is nil, it means the upperBound is unbounded.
	// The Iterator must be Closed after use.
	Iter(k Key, upperBound Key) (Iterator, error)

	// IterReverse creates a reversed Iterator positioned on the first entry which key is less than k.
	// The returned iterator will iterate from greater key to smaller key.
	// If k is nil, the returned iterator will be positioned at the last key.
	// TODO: Add lower bound limit
	IterReverse(k Key) (Iterator, error)
}

// Mutator is the interface wraps the basic Set and Delete methods.
type Mutator interface {
	// Set sets the value for key k as v into kv store.
	// v must NOT be nil or empty, otherwise it returns ErrCannotSetNilValue.
	Set(k Key, v []byte) error
	// Delete removes the entry for key k from kv store.
	Delete(k Key) error
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="load-schema"><a class="header" href="#load-schema">Load Schema</a></h1>
<!-- toc -->
<h2 id="schema-cache"><a class="header" href="#schema-cache">Schema Cache</a></h2>
<p>TiDB 使用Schema来将关系数据库中的table/index等映射到TiKV的kv存储中。</p>
<p>TiDB是无状态的，在TiDB内存中也加载这一份Schema, 在TiDB server中infoSchema在内存中结构如下</p>
<p><img src="tidb/./dot/model.svg" alt="model" /></p>
<h2 id="schema-load"><a class="header" href="#schema-load">Schema Load</a></h2>
<p>TiDB每隔lease/2 会去Tikv中去reload schema,</p>
<ol>
<li>首先会检查版本号，如果tikv中版本号和TiDB 中版本一致的话，就不用继续加载了, 否则进入下一步</li>
<li><code>tryLoadSchemaDiffs</code>先尝试加载schemaDiff, 如果失败，进入下一步</li>
<li>调用<code>fetchAllSchemasWithTables</code>会加载所有的schema</li>
</ol>
<p><img src="tidb/./dot/schema-load.svg" alt="schema-load" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-modification-1"><a class="header" href="#schema-modification-1">Schema Modification</a></h1>
<!-- toc -->
<h2 id="ddl-处理流程"><a class="header" href="#ddl-处理流程">DDL 处理流程</a></h2>
<p>TiDB ddl 请求处理请求流程如下图所示(摘自<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a>)</p>
<p><img src="tidb/./dot/ddl-flow.png" alt="ddl flow" /></p>
<p>每个tidb server都会起一个ddl worker，但只有一个节点的
ddl worker会被选为owner。</p>
<h2 id="owner-选举-1"><a class="header" href="#owner-选举-1">Owner 选举</a></h2>
<p>TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 owner 节点的 worker 串行执行的。</p>
<ul>
<li>worker：每个节点都有一个 worker 用来处理 DDL 操作。</li>
<li>owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。</li>
</ul>
<p><img src="tidb/./dot/campaign-owner.svg" alt="owner campaign" /></p>
<h2 id="handleddljobqueue"><a class="header" href="#handleddljobqueue">handleDDLJobQueue</a></h2>
<p><img src="tidb/./dot/ddl-schema-flow.svg" alt="ddl-schema-flow" /></p>
<p>owner节点的ddl worker 从ddl job queue 中取job
执行job, 调用<code>Meta.go</code> 中定义的<code>CreateDatabase</code>等接口
修改存储在TiKV中的schema。</p>
<p>其他TiDB server收到ddl 请求，只用把这个请求转ddl job 放入ddl job queue中
即可。</p>
<p>owner 节点的ddl worker handleDDLJobQueue 主要调用关系如下图所示：</p>
<p><img src="tidb/./dot/ddl_worker.svg" alt="ddl worker" /></p>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a></li>
<li><a href="https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md">TiDB 的异步 schema 变更实现</a></li>
<li><a href="http://zimulala.github.io/2017/12/24/optimize/">TiDB 异步Schema 变更优化</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="online-schema-change-1"><a class="header" href="#online-schema-change-1">Online Schema Change</a></h1>
<h2 id="schema-state-1"><a class="header" href="#schema-state-1">Schema state</a></h2>
<p><img src="tidb/./dot/schema-state.svg" alt="schema state" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="统计信息"><a class="header" href="#统计信息">统计信息</a></h1>
<!-- toc -->
<h2 id="概念"><a class="header" href="#概念">概念</a></h2>
<p>在 TiDB 中，我们维护的统计信息包括表的总行数，列的等深直方图，Count-Min Sketch，Null 值的个数，平均长度，不同值的数目等等
用于快速估算代价。</p>
<h3 id="等深直方图"><a class="header" href="#等深直方图">等深直方图</a></h3>
<p>相比于等宽直方图，等深直方图在最坏情况下也可以很好的保证误差
等深直方图，就是落入每个桶里的值数量尽量相等。</p>
<h3 id="cmsketch"><a class="header" href="#cmsketch">CMSketch</a></h3>
<p>Count-Min Sketch 是一种可以处理等值查询，Join 大小估计等的数据结构，并且可以提供很强的准确性保证。自 2003 年在文献 An improved data stream summary: The count-min sketch and its applications 中提出以来，由于其创建和使用的简单性获得了广泛的使用。</p>
<h3 id="fmsketch"><a class="header" href="#fmsketch">FMSketch</a></h3>
<h2 id="tidb中实现"><a class="header" href="#tidb中实现">TiDB中实现</a></h2>
<h3 id="histogram"><a class="header" href="#histogram">Histogram</a></h3>
<p>一个Histogram对应一个column或者index的统计信息。</p>
<pre><code class="language-go">// Histogram represents statistics for a column or index.
type Histogram struct {
	ID        int64 // Column ID.
	NDV       int64 // Number of distinct values.
	NullCount int64 // Number of null values.
	// LastUpdateVersion is the version that this histogram updated last time.
	LastUpdateVersion uint64

	Tp *types.FieldType

	// Histogram elements.
	//
	// A bucket bound is the smallest and greatest values stored in the bucket. The lower and upper bound
	// are stored in one column.
	//
	// A bucket count is the number of items stored in all previous buckets and the current bucket.
	// Bucket counts are always in increasing order.
	//
	// A bucket repeat is the number of repeats of the bucket value, it can be used to find popular values.
	Bounds  *chunk.Chunk
	Buckets []Bucket

	// Used for estimating fraction of the interval [lower, upper] that lies within the [lower, value].
	// For some types like `Int`, we do not build it because we can get them directly from `Bounds`.
	scalars []scalar
	// TotColSize is the total column size for the histogram.
	// For unfixed-len types, it includes LEN and BYTE.
	TotColSize int64

	// Correlation is the statistical correlation between physical row ordering and logical ordering of
	// the column values. This ranges from -1 to +1, and it is only valid for Column histogram, not for
	// Index histogram.
	Correlation float64
}

// Bucket store the bucket count and repeat.
type Bucket struct {
	Count  int64
	Repeat int64
}

type scalar struct {
	lower        float64
	upper        float64
	commonPfxLen int // commonPfxLen is the common prefix length of the lower bound and upper bound when the value type is KindString or KindBytes.
}
</code></pre>
<h3 id="生成统计信息"><a class="header" href="#生成统计信息">生成统计信息</a></h3>
<h4 id="analyzeexec"><a class="header" href="#analyzeexec">AnalyzeExec</a></h4>
<p>在执行 analyze 语句的时候，TiDB 会将 analyze 请求下推到每一个 Region 上，然后将每一个 Region 的结果合并起来。</p>
<p>Analyze 语句</p>
<p><img src="tidb/./dot/AnalyzeExec.svg" alt="" /></p>
<h4 id="analyzecolumnspushdown"><a class="header" href="#analyzecolumnspushdown">analyzeColumnsPushdown</a></h4>
<p><img src="tidb/./dot/analyzeColumnsPushdown.svg" alt="" /></p>
<h4 id="analyzeindexpushdown"><a class="header" href="#analyzeindexpushdown">analyzeIndexPushdown</a></h4>
<p><img src="tidb/./dot/analyzeIndexPushdown.svg" alt="" /></p>
<h4 id="queryfeedback"><a class="header" href="#queryfeedback">QueryFeedback</a></h4>
<h5 id="收集queryfeedback"><a class="header" href="#收集queryfeedback">收集QueryFeedback</a></h5>
<p>Datasource对应的一些Executor: <code>TableReaderExecutor</code>, <code>IndexReaderExecutor</code>, 
<code>IndexLookupExecutor</code>, <code>IndexMergeReaderExecutor</code>
执行时候会生成一些feedback信息 </p>
<pre><code class="language-go">// Feedback represents the total scan count in range [lower, upper).
type Feedback struct {
	Lower  *types.Datum
	Upper  *types.Datum
	Count  int64
	Repeat int64
}
// QueryFeedback is used to represent the query feedback info. It contains the query's scan ranges and number of rows
// in each range.
type QueryFeedback struct {
	PhysicalID int64
	Hist       *Histogram
	Tp         int
	Feedback   []Feedback
	Expected   int64 // Expected is the Expected scan count of corresponding query.
	actual     int64 // actual is the actual scan count of corresponding query.
	Valid      bool  // Valid represents the whether this query feedback is still Valid.
	desc       bool  // desc represents the corresponding query is desc scan.
}
</code></pre>
<p><img src="tidb/./dot/query_feedback_collect.svg" alt="" /></p>
<h6 id="tablesrangestokvranges"><a class="header" href="#tablesrangestokvranges">TablesRangesToKVRanges</a></h6>
<pre><code class="language-go">// TablesRangesToKVRanges converts table ranges to &quot;KeyRange&quot;.
func TablesRangesToKVRanges(tids []int64, ranges []*ranger.Range, fb *statistics.QueryFeedback) []kv.KeyRange {
	if fb == nil || fb.Hist == nil {
		return tableRangesToKVRangesWithoutSplit(tids, ranges)
	}
	krs := make([]kv.KeyRange, 0, len(ranges))
	feedbackRanges := make([]*ranger.Range, 0, len(ranges))
	for _, ran := range ranges {
		low := codec.EncodeInt(nil, ran.LowVal[0].GetInt64())
		high := codec.EncodeInt(nil, ran.HighVal[0].GetInt64())
		if ran.LowExclude {
			low = kv.Key(low).PrefixNext()
		}
		// If this range is split by histogram, then the high val will equal to one bucket's upper bound,
		// since we need to guarantee each range falls inside the exactly one bucket, `PrefixNext` will make the
		// high value greater than upper bound, so we store the range here.
		r := &amp;ranger.Range{LowVal: []types.Datum{types.NewBytesDatum(low)},
			HighVal: []types.Datum{types.NewBytesDatum(high)}}
		feedbackRanges = append(feedbackRanges, r)

		if !ran.HighExclude {
			high = kv.Key(high).PrefixNext()
		}
		for _, tid := range tids {
			startKey := tablecodec.EncodeRowKey(tid, low)
			endKey := tablecodec.EncodeRowKey(tid, high)
			krs = append(krs, kv.KeyRange{StartKey: startKey, EndKey: endKey})
		}
	}
	fb.StoreRanges(feedbackRanges)
	return krs
}
</code></pre>
<p>这些信息会先插入到一个QueryFeedbackMap的一个队列中，
后面的<code>updateStatsWorker</code> 定期apply 这些feedback到自己的cache中。以及将这些
feedback apply到<code>mysql.stats_*</code>中</p>
<p><img src="tidb/./dot/query_feedback_map.svg" alt="" /></p>
<h5 id="apply-feedback-locally"><a class="header" href="#apply-feedback-locally">apply feedback locally</a></h5>
<p><img src="tidb/./dot/QueryFeedback.svg" alt="" /></p>
<h4 id="apply-feedback"><a class="header" href="#apply-feedback">apply feedback</a></h4>
<p>每个TiDB会将本地搜集到的feedback插到<code>mysql.stats_feedback</code>中，然后
由owner将表<code>mysql.stats_feedback</code>插入
<code>mysql.stats_histograms</code>, <code>msyql.stats_buckets</code>等表。</p>
<p><img src="tidb/./dot/QueryFeedback-global.svg" alt="" /></p>
<h5 id="updatehistogram"><a class="header" href="#updatehistogram">UpdateHistogram</a></h5>
<p>没怎么看明白这块算法。</p>
<p><img src="tidb/./dot/UpdateHistogram.svg" alt="" /></p>
<h3 id="使用统计信息"><a class="header" href="#使用统计信息">使用统计信息</a></h3>
<h4 id="加载统计信息"><a class="header" href="#加载统计信息">加载统计信息</a></h4>
<p>从mysql.stats_*表中加载信息。</p>
<p>每个TiDB server有个goroutine 周期性的更新stat信息
Handle can update stats info periodically.</p>
<p>在TiDB启动时候，会启动一个goroutine, loadStatsWorker</p>
<p><img src="tidb/./dot/loadStatsWorker.svg" alt="" /></p>
<p>Update, 更新statsCache</p>
<p><img src="tidb/./dot/stat_handle.svg" alt="" /></p>
<p>加载载table的Histogram和CMSketch tableStatsFromStorage</p>
<p><img src="tidb/./dot/tableStatsFromStorage.svg" alt="" /></p>
<h4 id="selectivity"><a class="header" href="#selectivity">Selectivity</a></h4>
<h4 id="statsnode"><a class="header" href="#statsnode">StatsNode</a></h4>
<pre><code class="language-go">// StatsNode is used for calculating selectivity.
type StatsNode struct {
	Tp int
	ID int64
	// mask is a bit pattern whose ith bit will indicate whether the ith expression is covered by this index/column.
	mask int64
	// Ranges contains all the Ranges we got.
	Ranges []*ranger.Range
	// Selectivity indicates the Selectivity of this column/index.
	Selectivity float64
	// numCols is the number of columns contained in the index or column(which is always 1).
	numCols int
	// partCover indicates whether the bit in the mask is for a full cover or partial cover. It is only true
	// when the condition is a DNF expression on index, and the expression is not totally extracted as access condition.
	partCover bool
}
</code></pre>
<pre><code class="language-go">// Selectivity is a function calculate the selectivity of the expressions.
// The definition of selectivity is (row count after filter / row count before filter).
// And exprs must be CNF now, in other words, `exprs[0] and exprs[1] and ... and exprs[len - 1]` should be held when you call this.
// Currently the time complexity is o(n^2).
</code></pre>
<p>Selectivity: </p>
<ol>
<li>计算表达式的ranges: ExtractColumnsFromExpressions</li>
</ol>
<p>questions:</p>
<ol>
<li>correlated column 是什么意思？</li>
<li>maskCovered作用是什么</li>
<li>statsNode的作用是什么</li>
</ol>
<p><img src="tidb/./dot/selectivity.svg" alt="" /></p>
<h2 id="参考-3"><a class="header" href="#参考-3">参考</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-12/">TiDB 源码阅读系列文章（十二）统计信息(上)</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-14/">TiDB 源码阅读系列文章（十四）统计信息（下)</a></li>
<li><a href="https://asktug.com/t/topic/37691">TiDB统计信息原理简介与实践</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="基本概念"><a class="header" href="#基本概念">基本概念</a></h1>
<!-- toc -->
<p>在 TiDB 中，我们维护的统计信息包括表的总行数，列的等深直方图，Count-Min Sketch，Null 值的个数，平均长度，不同值的数目等等
用于快速估算代价。</p>
<h2 id="等深直方图-1"><a class="header" href="#等深直方图-1">等深直方图</a></h2>
<p>相比于等宽直方图，等深直方图在最坏情况下也可以很好的保证误差
等深直方图，就是落入每个桶里的值数量尽量相等。</p>
<h2 id="cmsketch-1"><a class="header" href="#cmsketch-1">CMSketch</a></h2>
<p>Count-Min Sketch 是一种可以处理等值查询，Join 大小估计等的数据结构，并且可以提供很强的准确性保证。自 2003 年在文献 An improved data stream summary: The count-min sketch and its applications 中提出以来，由于其创建和使用的简单性获得了广泛的使用。</p>
<h2 id="fmsketch-1"><a class="header" href="#fmsketch-1">FMSketch</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stats-tables"><a class="header" href="#stats-tables">stats tables</a></h1>
<!-- toc -->
<h2 id="统计信息存储"><a class="header" href="#统计信息存储">统计信息存储</a></h2>
<p>在TiDB中统计信息会存在几个表中</p>
<ul>
<li><code>mysql.stats_meta</code>: 统计信息元信息</li>
<li><code>mysql.stats_histograms</code>: 统计信息直方图</li>
<li><code>mysql.stats_buckets</code> : 统计信息桶</li>
<li><code>mysql.stats_extended</code></li>
<li><code>mysql.stats_feedback</code> : 收集的stats feedback， 会被定期apply到上面的表中</li>
</ul>
<pre><code class="language-sql">	// CreateStatsMetaTable stores the meta of table statistics.
	CreateStatsMetaTable = `CREATE TABLE IF NOT EXISTS mysql.stats_meta (
		version 		BIGINT(64) UNSIGNED NOT NULL,
		table_id 		BIGINT(64) NOT NULL,
		modify_count	BIGINT(64) NOT NULL DEFAULT 0,
		count 			BIGINT(64) UNSIGNED NOT NULL DEFAULT 0,
		INDEX idx_ver(version),
		UNIQUE INDEX tbl(table_id)
	);`

	// CreateStatsColsTable stores the statistics of table columns.
	CreateStatsColsTable = `CREATE TABLE IF NOT EXISTS mysql.stats_histograms (
		table_id 			BIGINT(64) NOT NULL,
		is_index 			TINYINT(2) NOT NULL,
		hist_id 			BIGINT(64) NOT NULL,
		distinct_count 		BIGINT(64) NOT NULL,
		null_count 			BIGINT(64) NOT NULL DEFAULT 0,
		tot_col_size 		BIGINT(64) NOT NULL DEFAULT 0,
		modify_count 		BIGINT(64) NOT NULL DEFAULT 0,
		version 			BIGINT(64) UNSIGNED NOT NULL DEFAULT 0,
		cm_sketch 			BLOB,
		stats_ver 			BIGINT(64) NOT NULL DEFAULT 0,
		flag 				BIGINT(64) NOT NULL DEFAULT 0,
		correlation 		DOUBLE NOT NULL DEFAULT 0,
		last_analyze_pos 	BLOB DEFAULT NULL,
		UNIQUE INDEX tbl(table_id, is_index, hist_id)
	);`

	// CreateStatsBucketsTable stores the histogram info for every table columns.
	CreateStatsBucketsTable = `CREATE TABLE IF NOT EXISTS mysql.stats_buckets (
		table_id 	BIGINT(64) NOT NULL,
		is_index 	TINYINT(2) NOT NULL,
		hist_id 	BIGINT(64) NOT NULL,
		bucket_id 	BIGINT(64) NOT NULL,
		count 		BIGINT(64) NOT NULL,
		repeats 	BIGINT(64) NOT NULL,
		upper_bound BLOB NOT NULL,
		lower_bound BLOB ,
		UNIQUE INDEX tbl(table_id, is_index, hist_id, bucket_id)
	);`

	// CreateStatsFeedbackTable stores the feedback info which is used to update stats.
	CreateStatsFeedbackTable = `CREATE TABLE IF NOT EXISTS mysql.stats_feedback (
		table_id 	BIGINT(64) NOT NULL,
		is_index 	TINYINT(2) NOT NULL,
		hist_id 	BIGINT(64) NOT NULL,
		feedback 	BLOB NOT NULL,
		INDEX hist(table_id, is_index, hist_id)

	// CreateStatsExtended stores the registered extended statistics.
	CreateStatsExtended = `CREATE TABLE IF NOT EXISTS mysql.stats_extended (
		stats_name varchar(32) NOT NULL,
		db varchar(32) NOT NULL,
		type tinyint(4) NOT NULL,
		table_id bigint(64) NOT NULL,
		column_ids varchar(32) NOT NULL,
		scalar_stats double DEFAULT NULL,
		blob_stats blob DEFAULT NULL,
		version bigint(64) unsigned NOT NULL,
		status tinyint(4) NOT NULL,
		PRIMARY KEY(stats_name, db),
		KEY idx_1 (table_id, status, version),
		KEY idx_2 (status, version)
	);`

	// CreateStatsTopNTable stores topn data of a cmsketch with top n.
	CreateStatsTopNTable = `CREATE TABLE IF NOT EXISTS mysql.stats_top_n (
		table_id 	BIGINT(64) NOT NULL,
		is_index 	TINYINT(2) NOT NULL,
		hist_id 	BIGINT(64) NOT NULL,
		value 		LONGBLOB,
		count 		BIGINT(64) UNSIGNED NOT NULL,
		INDEX tbl(table_id, is_index, hist_id)
	);`
</code></pre>
<h2 id="创建-stat-tables"><a class="header" href="#创建-stat-tables">创建 stat tables</a></h2>
<p>这些SQL将由ddl worker owner在启动的时候执行, 创建相应的Table</p>
<p><img src="tidb/./dot/ddl_worker_create_tables.svg" alt="" /></p>
<h2 id="更新缓存加载-stats-table"><a class="header" href="#更新缓存加载-stats-table">更新/缓存/加载 stats table</a></h2>
<p>每个TiDB启动后，会调用UpdateTableStatsLoop，分别使用一个goroutine执行如下任务:</p>
<ol>
<li><code>autoAnalzeWorker</code> 定时触发autoAnaly worker, 根据一定规则触发执行<code>analyze table xxx</code>, 执行AnlyzeExec,会后将结果写入<code>mysql.stats_*</code>中。</li>
<li><code>loadStatsWorker</code> 把<code>mysql.stat_*</code>信息定期加载到本地缓存中。</li>
<li><code>updateStatsWorker</code> 将本地收集的feedback apply到自己的本地缓存上，并写入<code>mysql.stats_feedback</code>中，如果该节点是owner, 会将<code>mysql.stats_feedback</code>表中信息apply到 <code>mysql.stat_*</code>表中。</li>
</ol>
<p><img src="tidb/./dot/crud_mysql_stats.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analyze"><a class="header" href="#analyze">Analyze</a></h1>
<!-- toc -->
<h2 id="analyzeexec-1"><a class="header" href="#analyzeexec-1">AnalyzeExec</a></h2>
<p>在执行 analyze 语句的时候，TiDB 会将 analyze 请求下推到每一个 Region 上，然后将每一个 Region 的结果合并起来。</p>
<p>Analyze 语句</p>
<p><img src="tidb/./dot/AnalyzeExec.svg" alt="" /></p>
<h2 id="analyzecolumnspushdown-1"><a class="header" href="#analyzecolumnspushdown-1">analyzeColumnsPushdown</a></h2>
<p><img src="tidb/./dot/analyzeColumnsPushdown.svg" alt="" /></p>
<h2 id="analyzeindexpushdown-1"><a class="header" href="#analyzeindexpushdown-1">analyzeIndexPushdown</a></h2>
<p><img src="tidb/./dot/analyzeIndexPushdown.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="queryfeedback-1"><a class="header" href="#queryfeedback-1">QueryFeedback</a></h1>
<!-- toc -->
<h2 id="收集queryfeedback-1"><a class="header" href="#收集queryfeedback-1">收集QueryFeedback</a></h2>
<p>Datasource对应的一些Executor: <code>TableReaderExecutor</code>, <code>IndexReaderExecutor</code>, 
<code>IndexLookupExecutor</code>, <code>IndexMergeReaderExecutor</code>
执行时候会生成一些feedback信息 </p>
<pre><code class="language-go">// Feedback represents the total scan count in range [lower, upper).
type Feedback struct {
	Lower  *types.Datum
	Upper  *types.Datum
	Count  int64
	Repeat int64
}
// QueryFeedback is used to represent the query feedback info. It contains the query's scan ranges and number of rows
// in each range.
type QueryFeedback struct {
	PhysicalID int64
	Hist       *Histogram
	Tp         int
	Feedback   []Feedback
	Expected   int64 // Expected is the Expected scan count of corresponding query.
	actual     int64 // actual is the actual scan count of corresponding query.
	Valid      bool  // Valid represents the whether this query feedback is still Valid.
	desc       bool  // desc represents the corresponding query is desc scan.
}
</code></pre>
<p><img src="tidb/./dot/query_feedback_collect.svg" alt="" /></p>
<h2 id="tablesrangestokvranges-1"><a class="header" href="#tablesrangestokvranges-1">TablesRangesToKVRanges</a></h2>
<pre><code class="language-go">// TablesRangesToKVRanges converts table ranges to &quot;KeyRange&quot;.
func TablesRangesToKVRanges(tids []int64, ranges []*ranger.Range, fb *statistics.QueryFeedback) []kv.KeyRange {
	if fb == nil || fb.Hist == nil {
		return tableRangesToKVRangesWithoutSplit(tids, ranges)
	}
	krs := make([]kv.KeyRange, 0, len(ranges))
	feedbackRanges := make([]*ranger.Range, 0, len(ranges))
	for _, ran := range ranges {
		low := codec.EncodeInt(nil, ran.LowVal[0].GetInt64())
		high := codec.EncodeInt(nil, ran.HighVal[0].GetInt64())
		if ran.LowExclude {
			low = kv.Key(low).PrefixNext()
		}
		// If this range is split by histogram, then the high val will equal to one bucket's upper bound,
		// since we need to guarantee each range falls inside the exactly one bucket, `PrefixNext` will make the
		// high value greater than upper bound, so we store the range here.
		r := &amp;ranger.Range{LowVal: []types.Datum{types.NewBytesDatum(low)},
			HighVal: []types.Datum{types.NewBytesDatum(high)}}
		feedbackRanges = append(feedbackRanges, r)

		if !ran.HighExclude {
			high = kv.Key(high).PrefixNext()
		}
		for _, tid := range tids {
			startKey := tablecodec.EncodeRowKey(tid, low)
			endKey := tablecodec.EncodeRowKey(tid, high)
			krs = append(krs, kv.KeyRange{StartKey: startKey, EndKey: endKey})
		}
	}
	fb.StoreRanges(feedbackRanges)
	return krs
}
</code></pre>
<p>这些信息会先插入到一个QueryFeedbackMap的一个队列中，
后面的<code>updateStatsWorker</code> 定期apply 这些feedback到自己的cache中。以及将这些
feedback apply到<code>mysql.stats_*</code>中</p>
<p><img src="tidb/./dot/query_feedback_map.svg" alt="" /></p>
<h2 id="apply-feedback-locally-1"><a class="header" href="#apply-feedback-locally-1">apply feedback locally</a></h2>
<p><img src="tidb/./dot/QueryFeedback.svg" alt="" /></p>
<h2 id="apply-feedback-1"><a class="header" href="#apply-feedback-1">apply feedback</a></h2>
<p>每个TiDB会将本地搜集到的feedback插到<code>mysql.stats_feedback</code>中，然后
由owner将表<code>mysql.stats_feedback</code>插入
<code>mysql.stats_histograms</code>, <code>msyql.stats_buckets</code>等表。</p>
<p><img src="tidb/./dot/QueryFeedback-global.svg" alt="" /></p>
<h2 id="updatehistogram-1"><a class="header" href="#updatehistogram-1">UpdateHistogram</a></h2>
<p>没怎么看明白这块算法。</p>
<p><img src="tidb/./dot/UpdateHistogram.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="统计信息使用场景"><a class="header" href="#统计信息使用场景">统计信息使用场景</a></h1>
<!-- toc -->
<h2 id="加载统计信息-1"><a class="header" href="#加载统计信息-1">加载统计信息</a></h2>
<p>从mysql.stats_*表中加载信息。</p>
<p>每个TiDB server有个goroutine 周期性的更新stat信息
Handle can update stats info periodically.</p>
<p>在TiDB启动时候，会启动一个goroutine, loadStatsWorker</p>
<p><img src="tidb/./dot/loadStatsWorker.svg" alt="" /></p>
<p>Update, 更新statsCache</p>
<p><img src="tidb/./dot/stat_handle.svg" alt="" /></p>
<p>加载载table的Histogram和CMSketch tableStatsFromStorage</p>
<p><img src="tidb/./dot/tableStatsFromStorage.svg" alt="" /></p>
<h2 id="selectivity-1"><a class="header" href="#selectivity-1">Selectivity</a></h2>
<h3 id="statsnode-1"><a class="header" href="#statsnode-1">StatsNode</a></h3>
<pre><code class="language-go">// StatsNode is used for calculating selectivity.
type StatsNode struct {
	Tp int
	ID int64
	// mask is a bit pattern whose ith bit will indicate whether the ith expression is covered by this index/column.
	mask int64
	// Ranges contains all the Ranges we got.
	Ranges []*ranger.Range
	// Selectivity indicates the Selectivity of this column/index.
	Selectivity float64
	// numCols is the number of columns contained in the index or column(which is always 1).
	numCols int
	// partCover indicates whether the bit in the mask is for a full cover or partial cover. It is only true
	// when the condition is a DNF expression on index, and the expression is not totally extracted as access condition.
	partCover bool
}
</code></pre>
<pre><code class="language-go">// Selectivity is a function calculate the selectivity of the expressions.
// The definition of selectivity is (row count after filter / row count before filter).
// And exprs must be CNF now, in other words, `exprs[0] and exprs[1] and ... and exprs[len - 1]` should be held when you call this.
// Currently the time complexity is o(n^2).
</code></pre>
<p>Selectivity: </p>
<ol>
<li>计算表达式的ranges: ExtractColumnsFromExpressions</li>
</ol>
<p>questions:</p>
<ol>
<li>correlated column 是什么意思？</li>
<li>maskCovered作用是什么</li>
<li>statsNode的作用是什么</li>
</ol>
<p><img src="tidb/./dot/selectivity.svg" alt="" /></p>
<h2 id="参考-4"><a class="header" href="#参考-4">参考</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-12/">TiDB 源码阅读系列文章（十二）统计信息(上)</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-14/">TiDB 源码阅读系列文章（十四）统计信息（下)</a></li>
<li><a href="https://asktug.com/t/topic/37691">TiDB统计信息原理简介与实践</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logicalplan-optimize"><a class="header" href="#logicalplan-optimize">LogicalPlan Optimize</a></h1>
<!-- toc -->
<h2 id="logicalplan-1"><a class="header" href="#logicalplan-1">LogicalPlan</a></h2>
<p>列举下都有哪些logical plan</p>
<p><img src="tidb/./dot/logical-plan.svg" alt="logical plan" /></p>
<h4 id="schema-2"><a class="header" href="#schema-2">Schema</a></h4>
<pre><code class="language-go">// Schema stands for the row schema and unique key information get from input.
type Schema struct {
	Columns []*Column
	Keys    []KeyInfo
}
</code></pre>
<h4 id="expression-2"><a class="header" href="#expression-2">Expression</a></h4>
<p><img src="tidb/./dot/expression.svg" alt="expression" /></p>
<h3 id="datasource"><a class="header" href="#datasource">DataSource</a></h3>
<pre><code class="language-go">type DataSource struct {
	logicalSchemaProducer

	astIndexHints []*ast.IndexHint
	IndexHints    []indexHintInfo
	table         table.Table
	tableInfo     *model.TableInfo
	Columns       []*model.ColumnInfo
	DBName        model.CIStr

	TableAsName *model.CIStr
	// indexMergeHints are the hint for indexmerge.
	indexMergeHints []indexHintInfo
	// pushedDownConds are the conditions that will be pushed down to coprocessor.
	pushedDownConds []expression.Expression
	// allConds contains all the filters on this table. For now it's maintained
	// in predicate push down and used only in partition pruning.
	allConds []expression.Expression

	statisticTable *statistics.Table
	tableStats     *property.StatsInfo

	// possibleAccessPaths stores all the possible access path for physical plan, including table scan.
	possibleAccessPaths []*util.AccessPath

	// The data source may be a partition, rather than a real table.
	isPartition     bool
	physicalTableID int64
	partitionNames  []model.CIStr

	// handleCol represents the handle column for the datasource, either the
	// int primary key column or extra handle column.
	//handleCol *expression.Column
	handleCols HandleCols
	// TblCols contains the original columns of table before being pruned, and it
	// is used for estimating table scan cost.
	TblCols []*expression.Column
	// commonHandleCols and commonHandleLens save the info of primary key which is the clustered index.
	commonHandleCols []*expression.Column
	commonHandleLens []int
	// TblColHists contains the Histogram of all original table columns,
	// it is converted from statisticTable, and used for IO/network cost estimating.
	TblColHists *statistics.HistColl
	// preferStoreType means the DataSource is enforced to which storage.
	preferStoreType int
	// preferPartitions store the map, the key represents store type, the value represents the partition name list.
	preferPartitions map[int][]model.CIStr
}
</code></pre>
<ol>
<li>DataSource 这个就是数据源，也就是表，select * from t 里面的 t。</li>
<li>Selection 选择，例如 select xxx from t where xx = 5 里面的 where 过滤条件。</li>
<li>Projection 投影， select c from t 里面的取 c 列是投影操作。</li>
<li>Join 连接， select xx from t1, t2 where t1.c = t2.c 就是把 t1 t2 两个表做 Join。</li>
</ol>
<p>哪些Logical plan 可能有多个child ?
感觉也就join/union/interset</p>
<h2 id="优化规则"><a class="header" href="#优化规则">优化规则</a></h2>
<p>基于规则的优化
logicalOptimize</p>
<pre><code class="language-go">var optRuleList = []logicalOptRule{
    &amp;gcSubstituter{},
    &amp;columnPruner{},
    &amp;buildKeySolver{},
    &amp;decorrelateSolver{},
    &amp;aggregationEliminator{},
    &amp;projectionEliminator{},
    &amp;maxMinEliminator{},
    &amp;ppdSolver{},
    &amp;outerJoinEliminator{},
    &amp;partitionProcessor{},
    &amp;aggregationPushDownSolver{},
    &amp;pushDownTopNOptimizer{},
    &amp;joinReOrderSolver{},
    &amp;columnPruner{}, // column pruning again at last, note it will mess up the results of buildKeySolver
}
</code></pre>
<h3 id="列裁剪"><a class="header" href="#列裁剪">列裁剪</a></h3>
<p>列裁剪的思想是这样的：对于用不上的列，没有必要读取它们的数据，无谓的浪费 IO 资源。比如说表 t 里面有 a b c d 四列。</p>
<h3 id="构建节点属性buildkeysolver"><a class="header" href="#构建节点属性buildkeysolver">构建节点属性(buildKeySolver)</a></h3>
<p>LogicalPlan的<code>BuildKeyInfo</code>和MaxOneRow接口</p>
<pre><code class="language-go">	// BuildKeyInfo will collect the information of unique keys into schema.
	// Because this method is also used in cascades planner, we cannot use
	// things like `p.schema` or `p.children` inside it. We should use the `selfSchema`
	// and `childSchema` instead.
	BuildKeyInfo(selfSchema *expression.Schema, childSchema []*expression.Schema)

	// MaxOneRow means whether this operator only returns max one row.
	MaxOneRow() bool

	// Get the schema.
	Schema() *expression.Schema
</code></pre>
<p>buildKeySolver 会构建MaxOneRow属性和unique key属性，在后面的优化器, 比如聚合消除中会用到。</p>
<p><img src="tidb/./dot/buildKeySolver.svg" alt="buildKeySolver" /></p>
<h3 id="decorrelatesolver"><a class="header" href="#decorrelatesolver">decorrelateSolver</a></h3>
<p>decorrelateSolver tries to convert apply plan to join plan.</p>
<p><img src="tidb/./dot/decorrelateSolver.svg" alt="decorrelateSolver" /></p>
<h3 id="最大最小消除maxmineliminator"><a class="header" href="#最大最小消除maxmineliminator">最大最小消除(maxMinEliminator)</a></h3>
<p>最大最小消除，会对 Min/Max 语句进行改写。</p>
<p><img src="tidb/./dot/max-min-eliminator.svg" alt="max-min-eliminator" /></p>
<h3 id="投影消除-projectioneliminator"><a class="header" href="#投影消除-projectioneliminator">投影消除 (projectionEliminator)</a></h3>
<p>影消除可以把不必要的 Projection 算子消除掉</p>
<p><img src="tidb/./dot/projection-eliminator.svg" alt="project eliminator" /></p>
<h3 id="谓词下推ppdsolver"><a class="header" href="#谓词下推ppdsolver">谓词下推(ppdSolver)</a></h3>
<p>谓词下推将查询语句中的过滤表达式计算尽可能下推到距离数据源最近的地方，以尽早完成数据的过滤，进而显著地减少数据传输或计算的开销。</p>
<p><code>PredicatePushDown</code>是LogicalPlan的一个接口， predicates 表示要添加的过滤条件。函数返回值是无法下推的条件，以及生成的新 plan。</p>
<pre><code>	// PredicatePushDown pushes down the predicates in the where/on/having clauses as deeply as possible.
	// It will accept a predicate that is an expression slice, and return the expressions that can't be pushed.
	// Because it might change the root if the having clause exists, we need to return a plan that represents a new root.
	PredicatePushDown([]expression.Expression) ([]expression.Expression, LogicalPlan)
</code></pre>
<p><code>baseLogicalPlan</code>实现了公共基本实现，调用child的PredictPushDown, child返回的不能下推的expression会新创建一个SelectPlan.</p>
<pre><code class="language-go">// PredicatePushDown implements LogicalPlan interface.
func (p *baseLogicalPlan) PredicatePushDown(predicates []expression.Expression) ([]expression.Expression, LogicalPlan) {
	if len(p.children) == 0 {
		return predicates, p.self
	}
	child := p.children[0]
	rest, newChild := child.PredicatePushDown(predicates)
	addSelection(p.self, newChild, rest, 0)
	return nil, p.self
}
</code></pre>
<p>假设 t1 和 t2 都是 100 条数据。如果把 t1 和 t2 两个表做笛卡尔积了再过滤，我们要处理 10000 条数据，而如果能先做过滤条件，那么数据量就会大量减少。谓词下推会尽量把过滤条件，推到靠近叶子节点，从而减少数据访问，节省计算开销。这就是谓词下推的作用。</p>
<p>谓词下推不能推过 MaxOneRow 和 Limit 节点。因为先 Limit N 行，然后再做 Selection 操作，跟先做 Selection 操作，再 Limit N 行得到的结果是不一样的。比如数据是 1 到 100，先 Limit 10 再 Select 大于 5，得到的是 5 到 10，而先做 Selection 再做 Limit 得到的是 5 到 15。MaxOneRow 也是同理，跟 Limit 1 效果一样。</p>
<p>DataSource是叶子节点, 会直接把过滤条件加入到 CopTask 里面。最后会通过 coprocessor 推给 TiKV 去做。</p>
<p><img src="tidb/./dot/pddSolver.svg" alt="ppdSolver" /></p>
<h3 id="聚合消除aggregationeliminator"><a class="header" href="#聚合消除aggregationeliminator">聚合消除(aggregationEliminator)</a></h3>
<p>聚合合消除会检查 SQL 查询中 Group By 语句所使用的列是否具有唯一性属性. </p>
<p>如果满足，则会将执行计划中相应的 LogicalAggregation 算子替换为 LogicalProjection 算子。</p>
<p>这里的逻辑是当聚合函数按照具有唯一性属性的一列或多列分组时，下层算子输出的每一行都是一个单独的分组. 
这时就可以将聚合函数展开成具体的参数列或者包含参数列的普通函数表达式。</p>
<pre><code class="language-go">// tryToEliminateAggregation will eliminate aggregation grouped by unique key.
// e.g. select min(b) from t group by a. If a is a unique key, then this sql is equal to `select b from t group by a`.
// For count(expr), sum(expr), avg(expr), count(distinct expr, [expr...]) we may need to rewrite the expr. Details are shown below.
// If we can eliminate agg successful, we return a projection. Else we return a nil pointer.
</code></pre>
<p><img src="tidb/./dot/aggregation-eliminator.svg" alt="aggregation-eliminator" /></p>
<h3 id="外连接消除outerjoineliminator"><a class="header" href="#外连接消除outerjoineliminator">外连接消除(outerJoinEliminator)</a></h3>
<p>这里外连接消除指的是将整个连接操作从查询中移除。
外连接消除需要满足一定条件：</p>
<ul>
<li>条件 1 : LogicalJoin 的父亲算子只会用到 LogicalJoin 的 outer plan 所输出的列</li>
<li>条件 2 :
<ul>
<li>条件 2.1 : LogicalJoin 中的 join key 在 inner plan 的输出结果中满足唯一性属性</li>
<li>条件 2.2 : LogicalJoin 的父亲算子会对输入的记录去重</li>
</ul>
</li>
</ul>
<pre><code class="language-go">// tryToEliminateOuterJoin will eliminate outer join plan base on the following rules
// 1. outer join elimination: For example left outer join, if the parent only use the
//    columns from left table and the join key of right table(the inner table) is a unique
//    key of the right table. the left outer join can be eliminated.
// 2. outer join elimination with duplicate agnostic aggregate functions: For example left outer join.
//    If the parent only use the columns from left table with 'distinct' label. The left outer join can
//    be eliminated.
</code></pre>
<p><img src="tidb/./dot/outerJoinEliminator.svg" alt="" /></p>
<h3 id="子查询优化去相关"><a class="header" href="#子查询优化去相关">子查询优化/去相关</a></h3>
<pre><code class="language-sql">-- 非相关子查询
select * from t1 where t1.a &gt; (select t2.a from t2 limit 1);
-- 相关子查询
select * from t1 where t1.a &gt; (select t2.a from t2 where t2.b &gt; t1.b limit 1);
</code></pre>
<h4 id="子查询展开"><a class="header" href="#子查询展开">子查询展开</a></h4>
<p>即直接执行子查询获得结果，再利用这个结果改写原本包含子查询的表达式</p>
<h4 id="子查询转为-join"><a class="header" href="#子查询转为-join">子查询转为 Join</a></h4>
<pre><code class="language-sql">-- 包含IN(subquery)的查询
select * from t1 where t1.a in (select t2.a from t2);

-- 改写为inner join
select t1.* from t1 inner join (select distinct(t2.a) as a from t2) as sub on t1.a = sub.a;

-- 如果 t2.a 满足唯一性属性，根据上面介绍的聚合消除规则，查询会被进一步改写成：
select t1.* from t1 inner join t2 on t1.a = t2.a;
</code></pre>
<p>展开子查询需要一次性将 t2 的全部数据从 TiKV 返回到 TiDB 中缓存，并作为 t1 扫描的过滤条件；如果将子查询转化为 inner join 的 inner plan ，我们可以更灵活地对 t2 选择访问方式，比如我们可以对 join 选择 IndexLookUpJoin 实现方式</p>
<h3 id="aggpushdown"><a class="header" href="#aggpushdown">aggPushDown</a></h3>
<p><img src="tidb/./dot/aggPushDown.svg" alt="aggPushDown" /></p>
<h3 id="partitionprocessor"><a class="header" href="#partitionprocessor">partitionProcessor</a></h3>
<pre><code class="language-go">
// partitionProcessor rewrites the ast for table partition.
//
// create table t (id int) partition by range (id)
//   (partition p1 values less than (10),
//    partition p2 values less than (20),
//    partition p3 values less than (30))
//
// select * from t is equal to
// select * from (union all
//      select * from p1 where id &lt; 10
//      select * from p2 where id &lt; 20
//      select * from p3 where id &lt; 30)
//

</code></pre>
<p><img src="tidb/./dot/partitionProcessor.svg" alt="partition processor" /></p>
<h3 id="pushdowntopnoptimizer"><a class="header" href="#pushdowntopnoptimizer">pushDownTopNOptimizer</a></h3>
<p>TODO: 解释这块主要干了啥, TOPN 怎么push下去了的。</p>
<pre><code class="language-go">	// pushDownTopN will push down the topN or limit operator during logical optimization.
	pushDownTopN(topN *LogicalTopN) LogicalPlan
</code></pre>
<p><img src="tidb/./dot/pushDownTopNOptimizer.svg" alt="pushDownTopNOptimizer" /></p>
<h3 id="joinreordersolver"><a class="header" href="#joinreordersolver">joinReOrderSolver</a></h3>
<p><img src="tidb/./dot/joinReOrderSolver.svg" alt="joinReOrderSolver" /></p>
<h4 id="greedysolver"><a class="header" href="#greedysolver">greedySolver</a></h4>
<pre><code class="language-go">// solve reorders the join nodes in the group based on a greedy algorithm.
//
// For each node having a join equal condition with the current join tree in
// the group, calculate the cumulative join cost of that node and the join
// tree, choose the node with the smallest cumulative cost to join with the
// current join tree.
//
// cumulative join cost = CumCount(lhs) + CumCount(rhs) + RowCount(join)
//   For base node, its CumCount equals to the sum of the count of its subtree.
//   See baseNodeCumCost for more details.
// TODO: this formula can be changed to real physical cost in future.
//
// For the nodes and join trees which don't have a join equal condition to
// connect them, we make a bushy join tree to do the cartesian joins finally.
</code></pre>
<h4 id="joinreorderdpsolver"><a class="header" href="#joinreorderdpsolver">joinReorderDPSolver</a></h4>
<h2 id="参考文献-3"><a class="header" href="#参考文献-3">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-7/">TiDB 源码阅读系列文章（七）基于规则的优化</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-21/">TiDB 源码阅读系列文章（二十一）基于规则的优化 II</a></li>
<li><a href="https://docs.pingcap.com/zh/tidb/stable/subquery-optimization">TiDB 文档子查询相关的优化</a></li>
<li><a href="https://www.cockroachlabs.com/blog/join-ordering-pt1/">An Introduction to Join Ordering</a></li>
<li><a href="https://docs.pingcap.com/tidb/stable/join-reorder">Introduction to Join Reorder</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="physical-optimize"><a class="header" href="#physical-optimize">Physical Optimize</a></h1>
<!-- toc -->
<h2 id="task-1"><a class="header" href="#task-1">task</a></h2>
<pre><code class="language-go">// task is a new version of `PhysicalPlanInfo`. It stores cost information for a task.
// A task may be CopTask, RootTask, MPPTask or a ParallelTask.
type task interface {
	count() float64
	addCost(cost float64)
	cost() float64
	copy() task
	plan() PhysicalPlan
	invalid() bool
}
</code></pre>
<p>task分两种, roottask在TiDB端执行</p>
<ul>
<li><code>rootTask</code> is the final sink node of a plan graph. It should be a single goroutine on tidb.</li>
<li><code>copTask</code> is a task that runs in a distributed kv store.</li>
</ul>
<ol>
<li>task是怎么执行的呢？</li>
<li>coptask和rootTask的执行在哪而体现的呢？</li>
</ol>
<h2 id="physical-physicalproperty"><a class="header" href="#physical-physicalproperty">Physical PhysicalProperty</a></h2>
<p><img src="tidb/./dot/physical-property.svg" alt="" /></p>
<pre><code class="language-go">// It contains the orders and the task types.
type PhysicalProperty struct {
	Items []Item

	// TaskTp means the type of task that an operator requires.
	//
	// It needs to be specified because two different tasks can't be compared
	// with cost directly. e.g. If a copTask takes less cost than a rootTask,
	// we can't sure that we must choose the former one. Because the copTask
	// must be finished and increase its cost in sometime, but we can't make
	// sure the finishing time. So the best way to let the comparison fair is
	// to add TaskType to required property.
	TaskTp TaskType

	// ExpectedCnt means this operator may be closed after fetching ExpectedCnt
	// records.
	ExpectedCnt float64

	// hashcode stores the hash code of a PhysicalProperty, will be lazily
	// calculated when function &quot;HashCode()&quot; being called.
	hashcode []byte

	// whether need to enforce property.
	Enforced bool
}
</code></pre>
<h4 id="tasktype"><a class="header" href="#tasktype">taskType</a></h4>
<pre><code class="language-go">// TaskType is the type of execution task.
type TaskType int

const (
	// RootTaskType stands for the tasks that executed in the TiDB layer.
	RootTaskType TaskType = iota

	// CopSingleReadTaskType stands for the a TableScan or IndexScan tasks
	// executed in the coprocessor layer.
	CopSingleReadTaskType

	// CopDoubleReadTaskType stands for the a IndexLookup tasks executed in the
	// coprocessor layer.
	CopDoubleReadTaskType

	// CopTiFlashLocalReadTaskType stands for flash coprocessor that read data locally,
	// and only a part of the data is read in one cop task, if the current task type is
	// CopTiFlashLocalReadTaskType, all its children prop's task type is CopTiFlashLocalReadTaskType
	CopTiFlashLocalReadTaskType

	// CopTiFlashGlobalReadTaskType stands for flash coprocessor that read data globally
	// and all the data of given table will be read in one cop task, if the current task
	// type is CopTiFlashGlobalReadTaskType, all its children prop's task type is
	// CopTiFlashGlobalReadTaskType
	CopTiFlashGlobalReadTaskType
)
</code></pre>
<h2 id="findbesttask-1"><a class="header" href="#findbesttask-1">findBestTask</a></h2>
<pre><code class="language-go">type LogicalPlan interface {
	// findBestTask converts the logical plan to the physical plan. It's a new interface.
	// It is called recursively from the parent to the children to create the result physical plan.
	// Some logical plans will convert the children to the physical plans in different ways, and return the one
	// With the lowest cost and how many plans are found in this function.
	// planCounter is a counter for planner to force a plan.
	// If planCounter &gt; 0, the clock_th plan generated in this function will be returned.
	// If planCounter = 0, the plan generated in this function will not be considered.
	// If planCounter = -1, then we will not force plan.
	findBestTask(prop *property.PhysicalProperty, planCounter *PlanCounterTp) (task, int64, error)
  //..
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="datasource-1"><a class="header" href="#datasource-1">DataSource</a></h1>
<!-- toc -->
<h2 id="logical-plan"><a class="header" href="#logical-plan">Logical Plan</a></h2>
<p>DataSource在query plan tree中是叶子节点，表示数据来源，在Logical Plan Optimize中，查询过滤条件
会尽量向叶子节点下推。</p>
<p>下推的过滤条件conds，首先被用于分区剪枝(根据partion expr相关的cond剪枝)，
然后primary key相关的cond会被抽出来，转换成为TiKV层Range。</p>
<p>最后其他无法抽离出来的cond被下推到TiKV层, 由TiKV层coprocessor来处理。</p>
<h3 id="struct-datasource"><a class="header" href="#struct-datasource">struct DataSource</a></h3>
<p><code>DataSource</code> 中的<code>tableInfo</code>字段包含了table的一些元信息，比如
tableId, indices之类的。</p>
<p><code>possibleAccessPaths</code>表示该DataSource的所有
可能访问路径，比如TableScan 或者扫描某个index等。</p>
<p><code>TblColHists</code> 用来Estimate符合条件的RowCount,  从而估算对应physical plan的cost.</p>
<p><img src="tidb/./dot/datasource.svg" alt="" /></p>
<pre><code class="language-go">// DataSource represents a tableScan without condition push down.
type DataSource struct {
	logicalSchemaProducer

	astIndexHints []*ast.IndexHint
	IndexHints    []indexHintInfo
	table         table.Table
	tableInfo     *model.TableInfo
	Columns       []*model.ColumnInfo
	DBName        model.CIStr

	TableAsName *model.CIStr
	// indexMergeHints are the hint for indexmerge.
	indexMergeHints []indexHintInfo
	// pushedDownConds are the conditions that will be pushed down to coprocessor.
	pushedDownConds []expression.Expression
	// allConds contains all the filters on this table. For now it's maintained
	// in predicate push down and used only in partition pruning.
	allConds []expression.Expression

	statisticTable *statistics.Table
	tableStats     *property.StatsInfo

	// possibleAccessPaths stores all the possible access path for physical plan, including table scan.
	possibleAccessPaths []*util.AccessPath

	// The data source may be a partition, rather than a real table.
	isPartition     bool
	physicalTableID int64
	partitionNames  []model.CIStr

	// handleCol represents the handle column for the datasource, either the
	// int primary key column or extra handle column.
	//handleCol *expression.Column
	handleCols HandleCols
	// TblCols contains the original columns of table before being pruned, and it
	// is used for estimating table scan cost.
	TblCols []*expression.Column
	// commonHandleCols and commonHandleLens save the info of primary key which is the clustered index.
	commonHandleCols []*expression.Column
	commonHandleLens []int
	// TblColHists contains the Histogram of all original table columns,
	// it is converted from statisticTable, and used for IO/network cost estimating.
	TblColHists *statistics.HistColl
	// preferStoreType means the DataSource is enforced to which storage.
	preferStoreType int
	// preferPartitions store the map, the key represents store type, the value represents the partition name list.
	preferPartitions map[int][]model.CIStr
}
</code></pre>
<h3 id="accesspath"><a class="header" href="#accesspath">AccessPath</a></h3>
<p>AccessPath 表示我们访问一个table路径，是基于单索引，还是使用多索引, 或者去扫描整个表，其定义如下,
在逻辑优化阶段的paritionProcessor中会生成DataSource的所有possiableAccessPath</p>
<pre><code class="language-go">type AccessPath struct {
	Index          *model.IndexInfo
	FullIdxCols    []*expression.Column
	FullIdxColLens []int
	IdxCols        []*expression.Column
	IdxColLens     []int
	Ranges         []*ranger.Range
	// CountAfterAccess is the row count after we apply range seek and before we use other filter to filter data.
	// For index merge path, CountAfterAccess is the row count after partial paths and before we apply table filters.
	CountAfterAccess float64
	// CountAfterIndex is the row count after we apply filters on index and before we apply the table filters.
	CountAfterIndex float64
	AccessConds     []expression.Expression
	EqCondCount     int
	EqOrInCondCount int
	IndexFilters    []expression.Expression
	TableFilters    []expression.Expression
	// PartialIndexPaths store all index access paths.
	// If there are extra filters, store them in TableFilters.
	PartialIndexPaths []*AccessPath

	StoreType kv.StoreType

	IsDNFCond bool

	// IsTiFlashGlobalRead indicates whether this path is a remote read path for tiflash
	IsTiFlashGlobalRead bool

	// IsIntHandlePath indicates whether this path is table path.
	IsIntHandlePath    bool
	IsCommonHandlePath bool
	// Forced means this path is generated by `use/force index()`.
	Forced bool
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="builddatasource"><a class="header" href="#builddatasource">buildDataSource</a></h1>
<!-- toc -->
<!-- ![](./dot/buildDataSource.svg) -->
<h2 id="tablebyname"><a class="header" href="#tablebyname">TableByName</a></h2>
<p>根据tableName，找到对应的tableInfo</p>
<p><img src="tidb/./dot/TableByName.svg" alt="" /></p>
<h2 id="schema-3"><a class="header" href="#schema-3">Schema</a></h2>
<p><img src="tidb/./dot/buildDataSource-schema.svg" alt="" /></p>
<h2 id="handlecols"><a class="header" href="#handlecols">handleCols</a></h2>
<p><img src="tidb/./dot/buildDataSource-handleCols.svg" alt="" /></p>
<h2 id="getpossibleaccesspaths"><a class="header" href="#getpossibleaccesspaths">getPossibleAccessPaths</a></h2>
<p>遍历table的Indices, 生成对应的AccessPath</p>
<p><img src="tidb/./dot/buildDataSource-getPossibleAccessPath.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="range-索引范围计算"><a class="header" href="#range-索引范围计算">range: 索引范围计算</a></h1>
<!-- toc -->
<h2 id="主要作用"><a class="header" href="#主要作用">主要作用</a></h2>
<p>从查询条件中，抽出指定columns(主要是主键，索引, partition）相关filter，转换为Range,
用来做RangeScan查询, 这样TiKV只用扫描对应Key Range的数据。</p>
<p>比如下面的SQL查询(摘自参考1），会把primary key a 上filter抽离出来: <code>range: (1,5), (8,10)</code></p>
<pre><code class="language-sql">CREATE TABLE t (a int primary key, b int, c int);
select * from t where ((a &gt; 1 and a &lt; 5 and b &gt; 2) or (a &gt; 8 and a &lt; 10 and c &gt; 3)) and d = 5;
</code></pre>
<pre><code class="language-sql">explain select * from t where ((a &gt; 1 and a &lt; 5 and b &gt; 2) or (a &gt; 8 and a &lt; 10 and c &gt; 3) and (a &gt; 100)) ;


TableReader_7        | 2.00    | root      |               | data:Selection_6                                                                                                                    |
| └─Selection_6        | 2.00    | cop[tikv] |               | or(and(and(gt(tests.t.a, 1), lt(tests.t.a, 5)), gt(tests.t.b, 2)), and(and(gt(tests.t.a, 8), lt(tests.t.a, 10)), gt(tests.t.c, 3))) |
|   └─TableRangeScan_5 | 6.00    | cop[tikv] | table:t       | range:(1,5), (8,10), keep order:false, stats:pseudo
</code></pre>
<h2 id="单列索引"><a class="header" href="#单列索引">单列索引</a></h2>
<h3 id="detachcondsforcolumn"><a class="header" href="#detachcondsforcolumn">DetachCondsForColumn</a></h3>
<p>在成本估算和填充AccessPath一些信息时会调用该函数。主要是针对主键和单例索引。</p>
<p><img src="tidb/./dot/DetachCondsForColumn-caller.svg" alt="" /></p>
<pre><code class="language-go">// DetachCondsForColumn detaches access conditions for specified column from other filter conditions.
func DetachCondsForColumn(sctx sessionctx.Context, conds []expression.Expression, col *expression.Column) (accessConditions, otherConditions []expression.Expression) {
	checker := &amp;conditionChecker{
		colUniqueID: col.UniqueID,
		length:      types.UnspecifiedLength,
	}
	return detachColumnCNFConditions(sctx, conds, checker)
}
</code></pre>
<p><img src="tidb/./dot/DetachCondsForColumn.svg" alt="" /></p>
<h3 id="conditionchecker"><a class="header" href="#conditionchecker">conditionChecker</a></h3>
<p>没怎么看明白conditionChecker到底是干嘛的。</p>
<p><img src="tidb/./dot/conditionChecker.svg" alt="" /></p>
<h2 id="多列索引"><a class="header" href="#多列索引">多列索引</a></h2>
<h3 id="rangedetacher"><a class="header" href="#rangedetacher">rangeDetacher</a></h3>
<p><img src="tidb/./dot/datasource-allconds-range2.svg" alt="" /></p>
<h4 id="detachcondandbuildrangeforindex"><a class="header" href="#detachcondandbuildrangeforindex">DetachCondAndBuildRangeForIndex</a></h4>
<p><img src="tidb/./dot/DetachCondAndBuildRangeForIndex.svg" alt="" /></p>
<h4 id="detachdnfcondandbuildrangeforindex"><a class="header" href="#detachdnfcondandbuildrangeforindex">detachDNFCondAndBuildRangeForIndex</a></h4>
<p><img src="tidb/./dot/datasource-allconds-range.svg" alt="" /></p>
<h4 id="detachcnfcondandbuildrangeforindex"><a class="header" href="#detachcnfcondandbuildrangeforindex">detachCNFCondAndBuildRangeForIndex</a></h4>
<blockquote>
<p>AND 表达式中，只有当之前的列均为点查的情况下，才会考虑下一个列。</p>
</blockquote>
<blockquote>
<p>e.g. 对于索引 (a, b, c)，有条件 a &gt; 1 and b = 1，那么会被选中的只有 a &gt; 1。对于条件 a in (1, 2, 3) and b &gt; 1，两个条件均会被选到用来计算 range。</p>
</blockquote>
<p><img src="tidb/./dot/detachCNFCondAndBuildRangeForIndex.svg" alt="" /></p>
<h4 id="extracteqandincondition"><a class="header" href="#extracteqandincondition">ExtractEqAndInCondition</a></h4>
<pre><code class="language-go">// ExtractEqAndInCondition will split the given condition into three parts by the information of index columns and their lengths.
// accesses: The condition will be used to build range.
// filters: filters is the part that some access conditions need to be evaluate again since it's only the prefix part of char column.
// newConditions: We'll simplify the given conditions if there're multiple in conditions or eq conditions on the same column.
//   e.g. if there're a in (1, 2, 3) and a in (2, 3, 4). This two will be combined to a in (2, 3) and pushed to newConditions.
// bool: indicate whether there's nil range when merging eq and in conditions.
func ExtractEqAndInCondition(sctx sessionctx.Context, conditions []expression.Expression,
</code></pre>
<p><img src="tidb/./dot/ExtractEqAndInCondition.svg" alt="" /></p>
<h2 id="range-build-计算逻辑区间"><a class="header" href="#range-build-计算逻辑区间">range build: 计算逻辑区间</a></h2>
<p>计算一个expression对应的range</p>
<pre><code class="language-go">func (r *builder) build(expr expression.Expression) []point {
	switch x := expr.(type) {
	case *expression.Column:
		return r.buildFromColumn(x)
	case *expression.ScalarFunction:
		return r.buildFromScalarFunc(x)
	case *expression.Constant:
		return r.buildFromConstant(x)
	}

	return fullRange
}
</code></pre>
<p><img src="tidb/./dot/range_builder.svg" alt="" /></p>
<h3 id="point"><a class="header" href="#point">point</a></h3>
<blockquote>
<p>每个 point 代表区间的一个端点，其中的 excl 表示端点为开区间的端点还是闭区间的端点。start 表示这个端点是左端点还是右端点。</p>
</blockquote>
<pre><code class="language-go">// Point is the end point of range interval.
type point struct {
	value types.Datum
	excl  bool // exclude
	start bool
}
</code></pre>
<h2 id="flattendnfconditionsflattencnfconditions"><a class="header" href="#flattendnfconditionsflattencnfconditions">FlattenDNFConditions/FlattenCNFConditions</a></h2>
<p>extract DNF/CNF expression's leaf item</p>
<pre><code class="language-go">// FlattenDNFConditions extracts DNF expression's leaf item.
// e.g. or(or(a=1, a=2), or(a=3, a=4)), we'll get [a=1, a=2, a=3, a=4].
func FlattenDNFConditions(DNFCondition *ScalarFunction) []Expression {
	return extractBinaryOpItems(DNFCondition, ast.LogicOr)
}

// FlattenCNFConditions extracts CNF expression's leaf item.
// e.g. and(and(a&gt;1, a&gt;2), and(a&gt;3, a&gt;4)), we'll get [a&gt;1, a&gt;2, a&gt;3, a&gt;4].
func FlattenCNFConditions(CNFCondition *ScalarFunction) []Expression {
	return extractBinaryOpItems(CNFCondition, ast.LogicAnd)
}
</code></pre>
<h2 id="参考-5"><a class="header" href="#参考-5">参考</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-13/">TiDB 源码阅读系列文章（十三）索引范围计算简介</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tableindex存储编码"><a class="header" href="#tableindex存储编码">table/index存储编码</a></h1>
<!-- toc -->
<h2 id="rowkey"><a class="header" href="#rowkey">RowKey</a></h2>
<p>Rowkey形式如下</p>
<pre><code>{tablePrefix}{tableID}{recordPrefixSep}{handle.Encode()}
</code></pre>
<p>prefix 常量值如下:</p>
<pre><code class="language-go">var (
	tablePrefix     = []byte{'t'}
	recordPrefixSep = []byte(&quot;_r&quot;)
	indexPrefixSep  = []byte(&quot;_i&quot;)
	metaPrefix      = []byte{'m'}
)
</code></pre>
<p><img src="tidb/./dot/tablecodec.svg" alt="" /></p>
<h3 id="handle"><a class="header" href="#handle">Handle</a></h3>
<p>Handle is the ID of a row, 主要类型有：CommonHandle, IntHandle,  PartitionHandle</p>
<p>CommonHandle 指非整形的handle 比如string等, IntHandle是int64的handle
PartitionHandle 增加了一个PartitionID 字段</p>
<pre><code class="language-go">// Handle is the ID of a row.
type Handle interface {
	// IsInt returns if the handle type is int64.
	IsInt() bool
	// IntValue returns the int64 value if IsInt is true, it panics if IsInt returns false.
	IntValue() int64
	// Next returns the minimum handle that is greater than this handle.
	Next() Handle
	// Equal returns if the handle equals to another handle, it panics if the types are different.
	Equal(h Handle) bool
	// Compare returns the comparison result of the two handles, it panics if the types are different.
	Compare(h Handle) int
	// Encoded returns the encoded bytes.
	Encoded() []byte
	// Len returns the length of the encoded bytes.
	Len() int
	// NumCols returns the number of columns of the handle,
	NumCols() int
	// EncodedCol returns the encoded column value at the given column index.
	EncodedCol(idx int) []byte
	// Data returns the data of all columns of a handle.
	Data() ([]types.Datum, error)
	// String implements the fmt.Stringer interface.
	String() string
}
</code></pre>
<h4 id="commanhandle"><a class="header" href="#commanhandle">CommanHandle</a></h4>
<pre><code class="language-go">// CommonHandle implements the Handle interface for non-int64 type handle.
type CommonHandle struct {
	encoded       []byte
	colEndOffsets []uint16
}
</code></pre>
<h4 id="partitionhandle"><a class="header" href="#partitionhandle">PartitionHandle</a></h4>
<p>PartitionHandle 用于GlobalIndex</p>
<pre><code class="language-go">type PartitionHandle struct {
	Handle
	PartitionID int64
}
</code></pre>
<h4 id="inthandle"><a class="header" href="#inthandle">IntHandle</a></h4>
<pre><code class="language-go">type IntHandle int64
</code></pre>
<h2 id="index"><a class="header" href="#index">Index</a></h2>
<h3 id="indexkey"><a class="header" href="#indexkey">IndexKey</a></h3>
<p>indexKey形式如下:</p>
<pre><code>{tablePrefix}{phyTblID}{indexPrefixSep}{idxInfo.ID}{indexedValues}
</code></pre>
<p>一般的local index phyTblID用的是分区table对应的Physical table ID</p>
<p><img src="tidb/./dot/indexKey.svg" alt="" /></p>
<h3 id="global-index"><a class="header" href="#global-index">Global Index</a></h3>
<p>如果是GlobalIndex，phyTblID 则用的是tablInfo.ID, 真实的ParitionID会被编码到IndexKey的value中.</p>
<pre><code class="language-go">// GenIndexKey generates storage key for index values. Returned distinct indicates whether the
// indexed values should be distinct in storage (i.e. whether handle is encoded in the key).
func (c *index) GenIndexKey(sc *stmtctx.StatementContext, indexedValues []types.Datum, h kv.Handle, buf []byte) (key []byte, distinct bool, err error) {
	idxTblID := c.phyTblID
	if c.idxInfo.Global {
		idxTblID = c.tblInfo.ID
	}
	return tablecodec.GenIndexKey(sc, c.tblInfo, c.idxInfo, idxTblID, indexedValues, h, buf)
}
</code></pre>
<p>在<code>PartitionHandlesToKVRanges</code>中则会使用partitionID来计算handle的RowKey</p>
<h3 id="index-value-layout"><a class="header" href="#index-value-layout">Index Value Layout</a></h3>
<p><img src="tidb/./dot/index_value_layout.svg" alt="" /></p>
<p><img src="tidb/./dot/GenIndexValue.svg" alt="" /></p>
<h2 id="decodeindexkv"><a class="header" href="#decodeindexkv">DecodeIndexKV</a></h2>
<p><img src="tidb/./dot/decodeIndexKV.svg" alt="" /></p>
<h2 id="indextype"><a class="header" href="#indextype">IndexType</a></h2>
<p>TiDB 中IndexType是为了兼容MySQL语法而设置的，在实际起到什么作用。</p>
<pre><code>	IndexTypeBtree
	IndexTypeHash
	IndexTypeRtree
</code></pre>
<h2 id="参考-6"><a class="header" href="#参考-6">参考:</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-internal-2/">三篇文章了解 TiDB 技术内幕 - 说计算</a></li>
<li><a href="https://github.com/pingcap/tidb/blob/master/docs/design/2020-08-04-global-index.md">Proposal: Support global index for partition table</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/48745358">2.0解析系列 | 一文详解 OceanBase 2.0 的“全局索引”功能</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="partitionprocessor-1"><a class="header" href="#partitionprocessor-1">partitionProcessor</a></h1>
<!-- toc -->
<h2 id="partitionprocessor-2"><a class="header" href="#partitionprocessor-2">partitionProcessor</a></h2>
<p><img src="tidb/./dot/prunePartition.svg" alt="" /></p>
<p>逻辑优化partitionProcessor会计算出Datasource的possibleAccessPath
prune时候，会参照datasource 的allconds</p>
<pre><code class="language-go">var optRuleList = []logicalOptRule{
	&amp;partitionProcessor{},
}
// partitionProcessor rewrites the ast for table partition.
//
// create table t (id int) partition by range (id)
//   (partition p1 values less than (10),
//    partition p2 values less than (20),
//    partition p3 values less than (30))
//
// select * from t is equal to
// select * from (union all
//      select * from p1 where id &lt; 10
//      select * from p2 where id &lt; 20
//      select * from p3 where id &lt; 30)
//
// partitionProcessor is here because it's easier to prune partition after predicate push down.
</code></pre>
<p>首先处理分区，然后会根据hints: IndexHints, indexMergeHints, preferStoreType 以及
Table自己的Index(在getPossibleAccessPath中会遍历TableInfo.Indices), 组合列举出所有的accessPath.
最后会去掉违反IsolationRead的path.</p>
<h3 id="prunerangepartition"><a class="header" href="#prunerangepartition">pruneRangePartition</a></h3>
<pre><code class="language-sql">REATE TABLE employees (
    id INT NOT NULL,
    fname VARCHAR(30),
    lname VARCHAR(30),
    hired DATE NOT NULL DEFAULT '1970-01-01',
    separated DATE NOT NULL DEFAULT '9999-12-31',
    job_code INT NOT NULL,
    store_id INT NOT NULL
)

PARTITION BY RANGE (store_id) (
    PARTITION p0 VALUES LESS THAN (6),
    PARTITION p1 VALUES LESS THAN (11),
    PARTITION p2 VALUES LESS THAN (16),
    PARTITION p3 VALUES LESS THAN (21)
);
</code></pre>
<blockquote>
<p>The optimizer can prune partitions through WHERE conditions in the following two scenarios:</p>
<p>partition_column = constant
partition_column IN (constant1, constant2, ..., constantN)</p>
</blockquote>
<p><img src="tidb/./dot/pruneRangePartition.svg" alt="" /></p>
<h3 id="prunehashpartition"><a class="header" href="#prunehashpartition">pruneHashPartition</a></h3>
<pre><code class="language-sql">CREATE TABLE employees (
    id INT NOT NULL,
    fname VARCHAR(30),
    lname VARCHAR(30),
    hired DATE NOT NULL DEFAULT '1970-01-01',
    separated DATE NOT NULL DEFAULT '9999-12-31',
    job_code INT,
    store_id INT
)

PARTITION BY HASH(store_id)
PARTITIONS 4;
</code></pre>
<p><img src="tidb/./dot/pruneHashPartition.svg" alt="" /></p>
<h3 id="prunelistpartition"><a class="header" href="#prunelistpartition">pruneListPartition</a></h3>
<pre><code class="language-sql">CREATE TABLE employees (
    id INT NOT NULL,
    fname VARCHAR(30),
    lname VARCHAR(30),
    hired DATE NOT NULL DEFAULT '1970-01-01',
    separated DATE NOT NULL DEFAULT '9999-12-31',
    job_code INT,
    store_id INT
)
PARTITION BY LIST(store_id) (
    PARTITION pNorth VALUES IN (3,5,6,9,17),
    PARTITION pEast VALUES IN (1,2,10,11,19,20),
    PARTITION pWest VALUES IN (4,12,13,14,18),
    PARTITION pCentral VALUES IN (7,8,15,16)
);
</code></pre>
<p><img src="tidb/./dot/pruneListPartition.svg" alt="" /></p>
<h3 id="makeunionallchildren"><a class="header" href="#makeunionallchildren">makeUnionAllChildren</a></h3>
<p>对于每个Partition对应的Datasource, 生成所有可能的AccessPath</p>
<p><img src="tidb/./dot/possibleAccessPath.svg" alt="possibleAccessPaths" /></p>
<h2 id="参考-7"><a class="header" href="#参考-7">参考</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-cascades-planner/">揭秘 TiDB 新优化器：Cascades Planner 原理解析</a></li>
</ol>
<h1 id="trash"><a class="header" href="#trash">Trash</a></h1>
<h2 id="分区剪枝"><a class="header" href="#分区剪枝">分区剪枝</a></h2>
<pre><code class="language-go">func (s *partitionProcessor) prune(ds *DataSource) (LogicalPlan, error) {
	pi := ds.tableInfo.GetPartitionInfo()
	if pi == nil {
		return ds, nil
	}
	// Try to locate partition directly for hash partition.
	if pi.Type == model.PartitionTypeHash {
		return s.processHashPartition(ds, pi)
	}
	if pi.Type == model.PartitionTypeRange {
		return s.processRangePartition(ds, pi)
	}

	// We haven't implement partition by list and so on.
	return s.makeUnionAllChildren(ds, pi, fullRange(len(pi.Definitions)))
}
</code></pre>
<p>TiDB中分区主要有Range和Hash两种,一下文字摘自<a href="https://book.tidb.io/session4/chapter6/partition-table-info.html">book.tidb.io</a></p>
<h3 id="range分区"><a class="header" href="#range分区">Range分区</a></h3>
<blockquote>
<p>Range 分区是指将数据行按分区表达式计算的值都落在给定的范围内。
在 Range 分区中，你必须为每个分区指定值的范围，并且不能有重叠，
通过使用 VALUES LESS THAN 操作进行定义。目前只支持单列的 Range 分区表。</p>
</blockquote>
<h3 id="hash分区"><a class="header" href="#hash分区">Hash分区</a></h3>
<blockquote>
<p>Hash 分区主要用于保证数据均匀地分散到一定数量的分区里面。
在 Hash 分区中，你只需要指定分区的数量。
使用 Hash 分区时，需要在 CREATE TABLE 后面添加 
PARTITION BY HASH (expr) PARTITIONS num ，
其中：expr 是一个返回整数的表达式，它可以是一个列名，
但这一列的类型必须整数类型；num 是一个正整数，表示将表划分为多少个分区。</p>
</blockquote>
<p>prune parition 调用图</p>
<p>每个partition 会生成一个新的DataSource,
然后用LogicalPartitionUnionAll最为父节点，把这些Datasource Union起来。 </p>
<p><img src="tidb/./dot/partition-prune.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="predicatepushdown"><a class="header" href="#predicatepushdown">PredicatePushDown</a></h1>
<!-- toc -->
<p>PredicatePushDown 整个数据流程如下：</p>
<p><img src="tidb/./dot/predicatedown-flow.svg" alt="" /></p>
<h2 id="predicatepushdown-1"><a class="header" href="#predicatepushdown-1">PredicatePushDown</a></h2>
<pre><code class="language-go">func (ds *DataSource) PredicatePushDown(predicates []expression.Expression) ([]expression.Expression, LogicalPlan) {
	ds.allConds = predicates
	ds.pushedDownConds, predicates = expression.PushDownExprs(ds.ctx.GetSessionVars().StmtCtx, predicates, ds.ctx.GetClient(), kv.UnSpecified)
	return predicates, ds
}
</code></pre>
<p><img src="tidb/./dot/PredicatePushDown.svg" alt="" /></p>
<h2 id="derivestats"><a class="header" href="#derivestats">DeriveStats</a></h2>
<p>在<code>DeriveStats</code>中把pushedDownConds derive到每个AccessPath上</p>
<pre><code class="language-go">func (ds *DataSource) DeriveStats(childStats []*property.StatsInfo, selfSchema *expression.Schema, childSchema []*expression.Schema, colGroups [][]*expression.Column) (*property.StatsInfo, error) {
//...
	for _, path := range ds.possibleAccessPaths {
		if path.IsTablePath() {
			continue
		}
		err := ds.fillIndexPath(path, ds.pushedDownConds)
    //...
	}
	ds.stats = ds.deriveStatsByFilter(ds.pushedDownConds, ds.possibleAccessPaths)
	for _, path := range ds.possibleAccessPaths {
		if path.IsTablePath() {
			noIntervalRanges, err := ds.deriveTablePathStats(path, ds.pushedDownConds, false)
      //...
    }
    //...
		noIntervalRanges := ds.deriveIndexPathStats(path, ds.pushedDownConds, false)
    // ...
}
</code></pre>
<p><img src="tidb/./dot/DataSource-DeriveStat.svg" alt="" /></p>
<h3 id="derivetablepathstats"><a class="header" href="#derivetablepathstats">deriveTablePathStats</a></h3>
<p>pushedDownConds 中的primary key column 相关的过滤条件会被分离出来 
作为AccessConds, 其他Column的cond留作TableFilter.</p>
<pre><code class="language-go">// deriveTablePathStats will fulfill the information that the AccessPath need.
// And it will check whether the primary key is covered only by point query.
// isIm indicates whether this function is called to generate the partial path for IndexMerge.
func (ds *DataSource) deriveTablePathStats(path *util.AccessPath, conds []expression.Expression, isIm bool) (bool, error) {

  //...
  //pkcol为primary key的column
	path.AccessConds, path.TableFilters = ranger.DetachCondsForColumn(ds.ctx, conds, pkCol)

  //...
	path.Ranges, err = ranger.BuildTableRange(path.AccessConds, sc, pkCol.RetType)
}
</code></pre>
<h2 id="accesspath-后续处理"><a class="header" href="#accesspath-后续处理">AccessPath 后续处理</a></h2>
<p>上面得到的AccessPath的Ranges 最终会被转换为KVRanges，用来表示去TiKV层去扫哪些数据，
TableFilters/IndexFilters 将会最终下推到TiKV层, 作为TiPB.Selection 在TiKV层提提前过滤</p>
<h3 id="tablerangestokvranges"><a class="header" href="#tablerangestokvranges">TableRangesToKVRanges</a></h3>
<p>在<code>PhysicalTableScan.ToPB</code>转为KvRange</p>
<pre><code class="language-go">// ToPB implements PhysicalPlan ToPB interface.
func (p *PhysicalTableScan) ToPB(ctx sessionctx.Context, storeType kv.StoreType) (*tipb.Executor, error) {
    //...
		ranges := distsql.TableRangesToKVRanges(tsExec.TableId, p.Ranges, nil)
		for _, keyRange := range ranges {
      //...
			tsExec.Ranges = append(tsExec.Ranges, tipb.KeyRange{Low: keyRange.StartKey, High: keyRange.EndKey})
		}
}
</code></pre>
<p>TableRange转为KVRange主要是把tableId encode进去.</p>
<pre><code class="language-go">func TableRangesToKVRanges(tids []int64, ranges []*ranger.Range, fb *statistics.QueryFeedback) []kv.KeyRange {
//...
		for _, tid := range tids {
			startKey := tablecodec.EncodeRowKey(tid, low)
			endKey := tablecodec.EncodeRowKey(tid, high)
			krs = append(krs, kv.KeyRange{StartKey: startKey, EndKey: endKey})
//..
}
</code></pre>
<h3 id="tablefilters转换为tipbselection"><a class="header" href="#tablefilters转换为tipbselection">TableFilters转换为Tipb.Selection</a></h3>
<p>TableFilters 则会被转成PhysicalSelection，并且在ToPB调用时候，被下推到TiKV层。
下图中的tipb则为发送到TiKV的GRPC请求.</p>
<p><img src="tidb/./dot/datasource-pushedDownConds.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="physical-optimize-1"><a class="header" href="#physical-optimize-1">Physical Optimize</a></h1>
<!-- toc -->
<h2 id="findbesttask-2"><a class="header" href="#findbesttask-2">findBestTask</a></h2>
<p>DataSource 对应的Physical plan分为三种：</p>
<ul>
<li>PhysicalTableReader: 读表</li>
<li>PhysicalIndexReader: 读index</li>
<li>PhysicalIndexLookUpReader: 读完index之后，根据rowID再去读index</li>
</ul>
<p>其对应的copTask为PhysicalTableScan, PhysicalIndexScan</p>
<p><img src="tidb/./dot/datasource-findbesttask.svg" alt="data source findBestTask" /></p>
<h2 id="cost"><a class="header" href="#cost">cost</a></h2>
<p>估算Datasource的rowCount, rowSize，然后使用session vars中定义的一些factor来计算cost.</p>
<h4 id="session-vars-factor"><a class="header" href="#session-vars-factor">session vars factor</a></h4>
<p>TiDB中定义了一些Session Vars, 这些值由<code>SetSystemVars</code>来设置</p>
<pre><code>func (s *SessionVars) SetSystemVar(name string, val string) error {
</code></pre>
<pre><code class="language-go">type SessionVars struct {
  //..
	// CPUFactor is the CPU cost of processing one expression for one row.
	CPUFactor float64
	// CopCPUFactor is the CPU cost of processing one expression for one row in coprocessor.
	CopCPUFactor float64
	// CopTiFlashConcurrencyFactor is the concurrency number of computation in tiflash coprocessor.
	CopTiFlashConcurrencyFactor float64
	// NetworkFactor is the network cost of transferring 1 byte data.
	NetworkFactor float64
	// ScanFactor is the IO cost of scanning 1 byte data on TiKV and TiFlash.
	ScanFactor float64
	// DescScanFactor is the IO cost of scanning 1 byte data on TiKV and TiFlash in desc order.
	DescScanFactor float64
	// SeekFactor is the IO cost of seeking the start value of a range in TiKV or TiFlash.
	SeekFactor float64
	// MemoryFactor is the memory cost of storing one tuple.
	MemoryFactor float64
	// DiskFactor is the IO cost of reading/writing one byte to temporary disk.
	DiskFactor float64
	// ConcurrencyFactor is the CPU cost of additional one goroutine.
	ConcurrencyFactor float64
  //..
}
</code></pre>
<p>可以在tidb client中看下当前session对应的factor</p>
<pre><code class="language-sql">show session variables like '%factor'
+-------------------------------------+-------+
| Variable_name                       | Value |
+-------------------------------------+-------+
| innodb_fill_factor                  |       |
| tidb_opt_concurrency_factor         | 3     |
| tidb_opt_copcpu_factor              | 3     |
| tidb_opt_correlation_exp_factor     | 1     |
| tidb_opt_cpu_factor                 | 3     |
| tidb_opt_desc_factor                | 3     |
| tidb_opt_disk_factor                | 1.5   |
| tidb_opt_memory_factor              | 0.001 |
| tidb_opt_network_factor             | 1     |
| tidb_opt_scan_factor                | 1.5   |
| tidb_opt_seek_factor                | 20    |
| tidb_opt_tiflash_concurrency_factor | 24    |
+-------------------------------------+-------+
</code></pre>
<h4 id="crossestimaterowcount"><a class="header" href="#crossestimaterowcount">crossEstimateRowCount</a></h4>
<p>估算rowcount
这个地方用到了信息统计的Histogram和CMSketch，用来估算RowCount(filter后的rowCount)
crossEstimateTableRowCount</p>
<p><img src="tidb/./dot/crossEstimateTableRowCount.svg" alt="" /></p>
<h3 id="converttotablescan"><a class="header" href="#converttotablescan">convertToTableScan</a></h3>
<p><img src="tidb/./dot/DataSource-converToTableScane-cost.svg" alt="datasource_table scan cost" /></p>
<h3 id="converttoindexscan"><a class="header" href="#converttoindexscan">convertToIndexScan</a></h3>
<p><img src="tidb/./dot/convertToIndexScan.svg" alt="datasource table index scan" /></p>
<h3 id="converttoindexmergescan"><a class="header" href="#converttoindexmergescan">convertToIndexMergeScan</a></h3>
<p><img src="tidb/./dot/convertToIndexMergeScan.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executors"><a class="header" href="#executors">Executors</a></h1>
<!-- toc -->
<h2 id="tablereaderexecutor"><a class="header" href="#tablereaderexecutor">TableReaderExecutor</a></h2>
<p>PhyscialTableReader对应的Executor为TableReaderExecutor, 其build过程如下:</p>
<p><img src="tidb/./dot/build_physical_table_reader_executor.svg" alt="build physical table reader" /></p>
<p>TableReaderExecutor 对应的Open/Next/Close调用，其中对TiKV层的调用封装在了distsql模块中。</p>
<p><img src="tidb/./dot/table_reader_executor.svg" alt="table reader executor" /></p>
<h2 id="tableindexexecutor"><a class="header" href="#tableindexexecutor">TableIndexExecutor</a></h2>
<p>PhysicalIndexReader 对应的Execturo为TableIndexExecutor, 其build过程如下:</p>
<p><img src="tidb/./dot/build_index_reader.svg" alt="build_index_reader" /></p>
<p>IndexExecutor Open/Next/Close方法, 也调用了distsql的方法</p>
<p><img src="tidb/./dot/table_index_reader_executor.svg" alt="table_index_reader_exexutor" /></p>
<h2 id="indexlookupexecutor"><a class="header" href="#indexlookupexecutor">IndexLookUpExecutor</a></h2>
<p>PhysicalIndexLookUpReader 对应的Execturo为IndexLookupReader, 其build过程如下:</p>
<p><img src="tidb/./dot/build_index_lookup_executor.svg" alt="build_index_lookup_executor" /></p>
<p>index Worker/Table Worker</p>
<p><img src="tidb/./dot/IndexLookUpExecutor.svg" alt="IndexLookUpExecutor" /></p>
<h3 id="extracttaskhandles"><a class="header" href="#extracttaskhandles">extractTaskHandles</a></h3>
<p>从index中获取row handlers</p>
<p><img src="tidb/./dot/extractTaskHandlers.svg" alt="" /></p>
<h3 id="buildtablereader"><a class="header" href="#buildtablereader">buildTableReader</a></h3>
<p>根据row handlers 去获取相应的Row</p>
<p><img src="tidb/./dot/buildTableReader.svg" alt="" /></p>
<h2 id="distsql"><a class="header" href="#distsql">DistSQL</a></h2>
<p>上面的TableReaderExecutor/TableIndexExecutor/IndexLookUpExecutor 最后 
都会去调用DistSQL模块的代码, 去TiKV请求数据。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="distsql-1"><a class="header" href="#distsql-1">DistSQL</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regincache"><a class="header" href="#regincache">ReginCache</a></h1>
<!-- toc -->
<h2 id="简介"><a class="header" href="#简介">简介</a></h2>
<blockquote>
<p>TiDB 的数据分布是以 Region 为单位的，一个 Region 包含了一个范围内的数据，通常是 96MB 的大小，Region 的 meta 信息包含了 StartKey 和 EndKey 这两个属性。当某个 key &gt;= StartKey &amp;&amp; key &lt; EndKey 的时候，我们就知道了这个 key 所在的 Region，然后我们就可以通过查找该 Region 所在的 TiKV 地址，去这个地址读取这个 key 的数据</p>
</blockquote>
<p>TiKV中数据是按照Region为单位存储key,value的，
TiDB拿到key, 或者key range之后，需要定位去哪个TiKV服务去取数据。</p>
<p>PDServer(placement driver)就是用来做这个事情的，TiDB需要先去PDserver
获取region leader的addr,然后再向TiKV发起请求。</p>
<p>为了提高效率，TiDB 本地对region做了一层cache，避免每次都要向Pd server发请求。
TiKV层region split之后，TiDB的cache就过期了，这时候，TiDB去TikV发请求，TiKV
会返回错误，然后TiDB根据错误信息，更新region Cache.</p>
<p><img src="tidb/./tikv-overview.png" alt="tikv-overview" /></p>
<h2 id="copclientsend"><a class="header" href="#copclientsend">CopClient.Send</a></h2>
<p><img src="tidb/./dot/CopClientSend.svg" alt="" /></p>
<h2 id="locatekey"><a class="header" href="#locatekey">LocateKey</a></h2>
<blockquote>
<p>RegionCache 的内部，有两种数据结构保存 Region 信息，一个是 map，另一个是 b-tree，用 map 可以快速根据 region ID 查找到 Region，用 b-tree 可以根据一个 key 找到包含该 key 的 Region</p>
</blockquote>
<p><img src="tidb/./dot/LocateKey.svg" alt="" /></p>
<h2 id="regionstore"><a class="header" href="#regionstore">RegionStore</a></h2>
<p>RegionStore represents region stores info</p>
<p><img src="tidb/./dot/RegionStore.svg" alt="" /></p>
<h2 id="sendreqctx"><a class="header" href="#sendreqctx">SendReqCtx</a></h2>
<p>根据RegionVerID，去cache中获取region, 然后获取peer(TiKV/TiFlash)的addr
发送GRPC请求.</p>
<p><img src="tidb/./dot/SendReqCtx.svg" alt="" /></p>
<h2 id="onregionerror"><a class="header" href="#onregionerror">onRegionError</a></h2>
<p>TiKV返回RegionError, TiDB根据error 信息更新本地RegionCache</p>
<p><img src="tidb/./dot/onRegionError.svg" alt="" /></p>
<p><img src="tidb/./dot/build-cop-tasks.svg" alt="build cop tasks" /></p>
<h2 id="参考文献-4"><a class="header" href="#参考文献-4">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-18/">TiDB 源码阅读系列文章（十八）tikv-client（上）</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tikv-grpc-client"><a class="header" href="#tikv-grpc-client">TiKV GRPC Client</a></h1>
<h2 id="client"><a class="header" href="#client">Client</a></h2>
<pre><code class="language-go">// Client is a client that sends RPC.
// It should not be used after calling Close().
type Client interface {
	// Close should release all data.
	Close() error
	// SendRequest sends Request.
	SendRequest(ctx context.Context, addr string, req *tikvrpc.Request, timeout time.Duration) (*tikvrpc.Response, error)
}
</code></pre>
<h2 id="sendrequest"><a class="header" href="#sendrequest">SendRequest</a></h2>
<p><img src="tidb/./dot/tikv-grpc-client-SendRequest.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coptask-1"><a class="header" href="#coptask-1">CopTask</a></h1>
<!-- toc -->
<h2 id="coptask-2"><a class="header" href="#coptask-2">copTask</a></h2>
<pre><code class="language-go">// copTask contains a related Region and KeyRange for a kv.Request.
type copTask struct {
	id     uint32
	region RegionVerID
	ranges *copRanges

	respChan  chan *copResponse
	storeAddr string
	cmdType   tikvrpc.CmdType
	storeType kv.StoreType
}

// copRanges is like []kv.KeyRange, but may has extra elements at head/tail.
// It's for avoiding alloc big slice during build copTask.
type copRanges struct {
	first *kv.KeyRange
	mid   []kv.KeyRange
	last  *kv.KeyRange
}
</code></pre>
<h2 id="buildcoptask"><a class="header" href="#buildcoptask">buildCopTask</a></h2>
<h2 id="从keyranges到coptask"><a class="header" href="#从keyranges到coptask">从KeyRanges到copTask</a></h2>
<p>pingcap的<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-19/#tidb-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E5%8D%81%E4%B9%9Dtikv-client%E4%B8%8B">TiDB 源码阅读系列文章（十九）tikv-client（下）</a> 
详细介绍了distsql.</p>
<blockquote>
<p>distsql 是位于 SQL 层和 coprocessor 之间的一层抽象，它把下层的 coprocessor 请求封装起来对上层提供一个简单的 Select 方法。执行一个单表的计算任务。最上层的 SQL 语句可能会包含 JOIN，SUBQUERY 等复杂算子，涉及很多的表，而 distsql 只涉及到单个表的数据。一个 distsql 请求会涉及到多个 region，我们要对涉及到的每一个 region 执行一次 coprocessor 请求。
所以它们的关系是这样的，一个 SQL 语句包含多个 distsql 请求，一个 distsql 请求包含多个 coprocessor 请求。</p>
</blockquote>
<p><img src="tidb/./dot/sql-distsql-coptask.svg" alt="sql-distsql-coptask" /></p>
<h3 id="kvrequest"><a class="header" href="#kvrequest">kv.Request</a></h3>
<pre><code class="language-go">// Request represents a kv request.
type Request struct {
	// Tp is the request type.
	Tp        int64
	StartTs   uint64
	Data      []byte
	KeyRanges []KeyRange
  // ..
}

// KeyRange represents a range where StartKey &lt;= key &lt; EndKey.
type KeyRange struct {
	StartKey Key
	EndKey   Key
}

// Key represents high-level Key type.
type Key []byte
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="copiteratorworker"><a class="header" href="#copiteratorworker">CopIteratorWorker</a></h1>
<p>Coprocessor 中通过copIteratorWorker来并发的向tikv(可能是多个tikv sever) 发送请求.</p>
<p>Worker负责发送RPC请求到Tikv server，处理错误，然后将正确的结果放入respCh channel中
在copIterator Next方法中会respCh中获取结果。</p>
<p><img src="tidb/./dot/dist_sql.svg" alt="dist sql" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coprocessor"><a class="header" href="#coprocessor">Coprocessor</a></h1>
<p><a href="https://pingcap.com/blog-cn/tikv-source-code-reading-14/">TiKV 源码解析系列文章（十四）Coprocessor 概览</a>
中介绍了TiKV端的Coprocessor, 相关信息摘抄如下：</p>
<p>TiKV Coprocessor 处理的读请求目前主要分类三种：</p>
<ul>
<li>DAG：执行物理算子，为 SQL 计算出中间结果，从而减少 TiDB 的计算和网络开销。这个是绝大多数场景下 Coprocessor 执行的任务。</li>
<li>Analyze：分析表数据，统计、采样表数据信息，持久化后被 TiDB 的优化器采用。</li>
<li>CheckSum：对表数据进行校验，用于导入数据后一致性校验。</li>
</ul>
<p><img src="tidb/./dot/2-read-process.png" alt="tikv 2 read process" /></p>
<h3 id="dagrequest"><a class="header" href="#dagrequest">DAGRequest</a></h3>
<p>以下结构由tipb 中Proto自动生成, 这些Executor将在TiKV端执行。</p>
<p><img src="tidb/./dot/dag_request.svg" alt="dag_request" /></p>
<h3 id="physicalplantopb"><a class="header" href="#physicalplantopb">PhysicalPlan.ToPB</a></h3>
<p>PhysicalPlan有ToPB方法，用来生成tipb Executor</p>
<pre><code class="language-go">// PhysicalPlan is a tree of the physical operators.
type PhysicalPlan interface {
	Plan

	// attach2Task makes the current physical plan as the father of task's physicalPlan and updates the cost of
	// current task. If the child's task is cop task, some operator may close this task and return a new rootTask.
	attach2Task(...task) task

	// ToPB converts physical plan to tipb executor.
	ToPB(ctx sessionctx.Context, storeType kv.StoreType) (*tipb.Executor, error)
}
</code></pre>
<p>调用ToPB流程</p>
<p><img src="tidb/./dot/to-pd.svg" alt="to-pb" /></p>
<p>physical plan 的toPB方法，可以看到基本TableScan和IndexScan是作为叶子节点的.
其他的比如PhysicalLimit, PhyscialTopN, PhyscialSelection 都用child executor.</p>
<p><img src="tidb/./dot/to-pb2.svg" alt="to-pb" /></p>
<h1 id="参考-8"><a class="header" href="#参考-8">参考</a></h1>
<ol>
<li><a href="https://github.com/pingcap/blog-cn/blob/master/mpp-smp-tidb.md">MPP and SMP in TiDB</a></li>
<li><a href="https://pingcap.com/blog-cn/tikv-source-code-reading-14/">TiKV 源码解析系列文章（十四）Coprocessor 概览</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="join"><a class="header" href="#join">Join</a></h1>
<!-- toc -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="join算法"><a class="header" href="#join算法">Join算法</a></h1>
<!-- toc -->
<h2 id="nest-loop-join"><a class="header" href="#nest-loop-join">Nest loop join</a></h2>
<p><code>nest loop join</code>, 遍历取外表R中一条记录r, 然后遍历inner表S每条记录和r做join。
对于外表中的每一条记录，都需要对Inner表做一次全表扫描。IO比较高</p>
<pre><code>algorithm nested_loop_join is
    for each tuple r in R do
        for each tuple s in S do
            if r and s satisfy the join condition then
                yield tuple &lt;r,s&gt;
</code></pre>
<h2 id="block-nest-loop-join"><a class="header" href="#block-nest-loop-join">Block nest loop join</a></h2>
<p>Block Nest Loop Join是对NestLoop Join的一个优化</p>
<pre><code>for each block Br of r do begin
  for each block Bs of s do begin
    for each tuple tr in Br do begin
      for each tuple ts in Bs do begin
        test pair (tr, ts) to see if they satisfy the join condition
          if they do, add tr ⋅ ts to the result;
      end
    end
  end
end
</code></pre>
<h2 id="indexed-nested-loop-join"><a class="header" href="#indexed-nested-loop-join">Indexed Nested loop join</a></h2>
<p><code>index join</code> inner表中对于要join的attribute由了索引, 可以使用索引
来避免对inner表的全表扫描, 复杂度为<code>O(M * log N)</code></p>
<pre><code>for each tuple r in R do
    for each tuple s in S in the index lookup do
        yield tuple &lt;r,s&gt;
</code></pre>
<h2 id="hash-join"><a class="header" href="#hash-join">Hash join</a></h2>
<pre><code class="language-pascal">/* Partition s */
for each tuple ts in s do begin
  i := h(ts[JoinAttrs]);
  Hsi := Hsi ∪ {ts};
end

/* Partition r */
for each tuple tr in r do begin
  i := h(tr[JoinAttrs]);
  Hri := Hri ∪ {tr};
end

/* Perform join on each partition */
for i := 0 to nh do begin
  read Hsi and build an in-memory hash index on it;
  for each tuple tr in Hri do begin
    probe the hash index on Hsi to locate all tuples ts
    such that ts[JoinAttrs] = tr[JoinAttrs];
    for each matching tuple ts in Hsi do begin
      add tr ⋈ ts to the result;
    end
  end
end
</code></pre>
<h2 id="sort-mergejoin"><a class="header" href="#sort-mergejoin">Sort MergeJoin</a></h2>
<pre><code>function sortMerge(relation left, relation right, attribute a)
    var relation output
    var list left_sorted := sort(left, a) // Relation left sorted on attribute a
    var list right_sorted := sort(right, a)
    var attribute left_key, right_key
    var set left_subset, right_subset // These sets discarded except where join predicate is satisfied
    advance(left_subset, left_sorted, left_key, a)
    advance(right_subset, right_sorted, right_key, a)
    while not empty(left_subset) and not empty(right_subset)
        if left_key = right_key // Join predicate satisfied
            add cartesian product of left_subset and right_subset to output
            advance(left_subset, left_sorted, left_key, a)
            advance(right_subset, right_sorted,right_key, a)
        else if left_key &lt; right_key
            advance(left_subset, left_sorted, left_key, a)
        else // left_key &gt; right_key
            advance(right_subset, right_sorted, right_key, a)
    return output

// Remove tuples from sorted to subset until the sorted[1].a value changes
function advance(subset out, sorted inout, key out, a in)
    key := sorted[1].a
    subset := emptySet
    while not empty(sorted) and sorted[1].a = key
        insert sorted[1] into subset
        remove sorted[1] 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logical-optimize"><a class="header" href="#logical-optimize">Logical Optimize</a></h1>
<h2 id="join-reorder"><a class="header" href="#join-reorder">join reorder</a></h2>
<h2 id="参考文献-5"><a class="header" href="#参考文献-5">参考文献</a></h2>
<ol>
<li><a href="https://docs.pingcap.com/tidb/stable/join-reorder">Introduction to Join Reorder</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="physical-optimize-2"><a class="header" href="#physical-optimize-2">Physical Optimize</a></h1>
<!-- toc -->
<h2 id="physical-join-继承关系"><a class="header" href="#physical-join-继承关系">Physical Join 继承关系</a></h2>
<p><img src="tidb/./dot/physical-join-inherit.svg" alt="" /></p>
<h2 id="physical-physicalproperty-1"><a class="header" href="#physical-physicalproperty-1">Physical PhysicalProperty</a></h2>
<p><img src="tidb/./dot/physical-property.svg" alt="" /></p>
<pre><code class="language-go">// It contains the orders and the task types.
type PhysicalProperty struct {
	Items []Item

	// TaskTp means the type of task that an operator requires.
	//
	// It needs to be specified because two different tasks can't be compared
	// with cost directly. e.g. If a copTask takes less cost than a rootTask,
	// we can't sure that we must choose the former one. Because the copTask
	// must be finished and increase its cost in sometime, but we can't make
	// sure the finishing time. So the best way to let the comparison fair is
	// to add TaskType to required property.
	TaskTp TaskType

	// ExpectedCnt means this operator may be closed after fetching ExpectedCnt
	// records.
	ExpectedCnt float64

	// hashcode stores the hash code of a PhysicalProperty, will be lazily
	// calculated when function &quot;HashCode()&quot; being called.
	hashcode []byte

	// whether need to enforce property.
	Enforced bool
}
</code></pre>
<h3 id="tasktype-1"><a class="header" href="#tasktype-1">taskType</a></h3>
<pre><code class="language-go">// TaskType is the type of execution task.
type TaskType int

const (
	// RootTaskType stands for the tasks that executed in the TiDB layer.
	RootTaskType TaskType = iota

	// CopSingleReadTaskType stands for the a TableScan or IndexScan tasks
	// executed in the coprocessor layer.
	CopSingleReadTaskType

	// CopDoubleReadTaskType stands for the a IndexLookup tasks executed in the
	// coprocessor layer.
	CopDoubleReadTaskType

	// CopTiFlashLocalReadTaskType stands for flash coprocessor that read data locally,
	// and only a part of the data is read in one cop task, if the current task type is
	// CopTiFlashLocalReadTaskType, all its children prop's task type is CopTiFlashLocalReadTaskType
	CopTiFlashLocalReadTaskType

	// CopTiFlashGlobalReadTaskType stands for flash coprocessor that read data globally
	// and all the data of given table will be read in one cop task, if the current task
	// type is CopTiFlashGlobalReadTaskType, all its children prop's task type is
	// CopTiFlashGlobalReadTaskType
	CopTiFlashGlobalReadTaskType
)
</code></pre>
<h2 id="findbesttask-3"><a class="header" href="#findbesttask-3">findBestTask</a></h2>
<p>枚举所有满足parent plan physicalProperty 的join物理计划, 其中GetMergeJoin, 给child加了PhyscialProp 要求child plan是按照joinKey 降序排序.</p>
<pre><code class="language-go">// LogicalJoin can generates hash join, index join and sort merge join.
// Firstly we check the hint, if hint is figured by user, we force to choose the corresponding physical plan.
// If the hint is not matched, it will get other candidates.
// If the hint is not figured, we will pick all candidates.
func (p *LogicalJoin) exhaustPhysicalPlans(prop *property.PhysicalProperty) ([]PhysicalPlan, bool) {
//...
}
</code></pre>
<p><img src="tidb/./dot/logicaljoin_exhaustPhysicalPlans.svg" alt="logicaljoin exhaustPhysicalPlans" /></p>
<h2 id="cost-估算"><a class="header" href="#cost-估算">Cost 估算</a></h2>
<p>估算每个join计划的cost</p>
<p><img src="tidb/./dot/physical-join-cost.svg" alt="" /></p>
<h3 id="physicalmergejoin"><a class="header" href="#physicalmergejoin">PhysicalMergeJoin</a></h3>
<pre><code class="language-go">func (p *PhysicalMergeJoin) attach2Task(tasks ...task) task {
	lTask := finishCopTask(p.ctx, tasks[0].copy())
	rTask := finishCopTask(p.ctx, tasks[1].copy())
	p.SetChildren(lTask.plan(), rTask.plan())
	return &amp;rootTask{
		p:   p,
		cst: lTask.cost() + rTask.cost() + p.GetCost(lTask.count(), rTask.count()),
	}
}
</code></pre>
<h3 id="physicalhashjoin"><a class="header" href="#physicalhashjoin">PhysicalHashJoin</a></h3>
<pre><code class="language-go">func (p *PhysicalHashJoin) attach2Task(tasks ...task) task {
	lTask := finishCopTask(p.ctx, tasks[0].copy())
	rTask := finishCopTask(p.ctx, tasks[1].copy())
	p.SetChildren(lTask.plan(), rTask.plan())
	task := &amp;rootTask{
		p:   p,
		cst: lTask.cost() + rTask.cost() + p.GetCost(lTask.count(), rTask.count()),
	}
	return task
}
</code></pre>
<h3 id="physicalindexjoin"><a class="header" href="#physicalindexjoin">PhysicalIndexJoin</a></h3>
<pre><code class="language-go">func (p *PhysicalIndexJoin) attach2Task(tasks ...task) task {
	innerTask := p.innerTask
	outerTask := finishCopTask(p.ctx, tasks[1-p.InnerChildIdx].copy())
	if p.InnerChildIdx == 1 {
		p.SetChildren(outerTask.plan(), innerTask.plan())
	} else {
		p.SetChildren(innerTask.plan(), outerTask.plan())
	}
	return &amp;rootTask{
		p:   p,
		cst: p.GetCost(outerTask, innerTask),
	}
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hash-join-1"><a class="header" href="#hash-join-1">Hash Join</a></h1>
<!-- toc -->
<h2 id="hashjoinexec-struct"><a class="header" href="#hashjoinexec-struct">HashJoinExec struct</a></h2>
<p><img src="tidb/./dot/hashjoinexecutor_struct.svg" alt="" /></p>
<p><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-9/">TiDB 源码阅读系列文章（九）Hash Join</a></p>
<p>HashJoin整体流程如下，主要有三个4个goroutine, 几个goroutine之间通过channel 来协作.</p>
<ul>
<li><code>fetchBuildSideRows</code>读取buildSides表中数据, 放入<code>buildSideResultCh</code>中</li>
<li><code>fetchAndBuildHashTable</code> 根据buildSide表中数据, 创建hashRowContainer</li>
<li><code>fetchProbeSideChunks</code>: 读取probeSide表数据, 放入<code>probeResultChs</code>中</li>
<li><code>runJoinWorker</code> 多个joinWorker并发执行，从<code>probeResultChs</code>读取probe数据，然后和rowContainer做匹配, 并将结果放入joinResultCh中</li>
</ul>
<p><img src="tidb/./dot/hash-join.svg" alt="hash join" /> </p>
<h2 id="fetchandbuildhashtable"><a class="header" href="#fetchandbuildhashtable">fetchAndBuildHashTable</a></h2>
<p>读取buildSideExec中所有数据，然后写入hasRowContainer中
如果内存不够，会写到磁盘上</p>
<p><img src="tidb/./dot/hashjoin_fetchAndBuildHashTable.svg" alt="" /></p>
<h2 id="fetchprobesidechunks"><a class="header" href="#fetchprobesidechunks">fetchProbeSideChunks</a></h2>
<p>fetchProbeSideChunks get chunks from fetches chunks from the big table in a background goroutine
and sends the chunks to multiple channels which will be read by multiple join workers.</p>
<p><img src="tidb/./dot/hashjoin_fetchProbeSideChunks.svg" alt="" /></p>
<h2 id="runjoinworker"><a class="header" href="#runjoinworker">runJoinWorker</a></h2>
<p><code>HashJoinExec.Next</code>会启动多个runJoinWorker来做hashJoin,</p>
<p>每个runJoinWorker会从<code>probeResultChs</code> chan中去取要Probe的数据
然后做Join,最后写到joinResultCh中，由<code>HashJoinExec.Next</code>接受，
返回给上层调用者。</p>
<p><img src="tidb/./dot/hashjoin_runJoinWorker.svg" alt="" /></p>
<h2 id="join2chunk"><a class="header" href="#join2chunk">join2Chunk</a></h2>
<p>join2Chunk负责将probeResult和inner table做Join</p>
<p><img src="tidb/./dot/hashjoin_join2Chunk.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mergejoin"><a class="header" href="#mergejoin">MergeJoin</a></h1>
<!-- toc -->
<h2 id="mergejoinexec-struct"><a class="header" href="#mergejoinexec-struct">MergeJoinExec Struct</a></h2>
<pre><code class="language-go">// MergeJoinExec implements the merge join algorithm.
// This operator assumes that two iterators of both sides
// will provide required order on join condition:
// 1. For equal-join, one of the join key from each side
// matches the order given.
// 2. For other cases its preferred not to use SMJ and operator
// will throw error.
type MergeJoinExec struct {
	baseExecutor

	stmtCtx      *stmtctx.StatementContext
	compareFuncs []expression.CompareFunc
	joiner       joiner
	isOuterJoin  bool
	desc         bool

	innerTable *mergeJoinTable
	outerTable *mergeJoinTable

	hasMatch bool
	hasNull  bool

	memTracker  *memory.Tracker
	diskTracker *disk.Tracker
}
</code></pre>
<p><img src="tidb/./dot/merge-join-struct.svg" alt="" /></p>
<p>首先使用vecGroupCheck分别将innner chunk和outerchunk 分为相同groupkey的组</p>
<p><img src="tidb/./dot/merge-join.svg" alt="merge join exec" /></p>
<h4 id="fetchnextinnergroup"><a class="header" href="#fetchnextinnergroup">fetchNextInnerGroup</a></h4>
<p>这个地方没怎么看明白，不太明白它是怎么处理一个groupkey超过多个chunk的情况</p>
<p><img src="tidb/./dot/mergeJoinTable_fetchNextInnerGroup.svg" alt="" /></p>
<h4 id="fetchnextoutergroup"><a class="header" href="#fetchnextoutergroup">fetchNextOuterGroup</a></h4>
<p><img src="tidb/./dot/mergeJoinTable_fetchNextOuterGroup.svg" alt="" /></p>
<h2 id="ref-7"><a class="header" href="#ref-7">Ref</a></h2>
<p>参考资料<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-15/">TiDB 源码阅读系列文章（十五）Sort Merge Join</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="index-lookup-join"><a class="header" href="#index-lookup-join">Index Lookup Join</a></h1>
<!-- toc -->
<h2 id="indexlookupjoin-struct"><a class="header" href="#indexlookupjoin-struct">IndexLookUpJoin Struct</a></h2>
<p><img src="tidb/./dot/index_lookup_join_struct.svg" alt="" /></p>
<h2 id="主要流程"><a class="header" href="#主要流程">主要流程</a></h2>
<p>执行index lookup join时候，会启动一个outerWorker go routine和多个innerWorker go routine, go routine之间通过innerCh和resultCh来协作，
他们关系如下：</p>
<ul>
<li><code>outerWorker</code> 负责读取probe side 数据，然后建立map，创建完毕后，将task 同时放入<code>innerCh</code>中和<code>resultCh</code>中</li>
<li><code>innerWorker</code> 从innerCh取task，去inner表取数据，执行完毕后，将task的doneCh close用以通知Main线程（执行IndexLookupJoin.Next的groutine)</li>
<li>调用IndexLookupJoin.Next的groutine从 resultCh中取一个task, 然后等待task执行完毕，执行完毕后做join, 将数据返回给上层调用者。</li>
</ul>
<p><img src="tidb/./dot/index_lookup_join_flow.svg" alt="" /></p>
<h3 id="buildtask"><a class="header" href="#buildtask">buildTask</a></h3>
<p>buildTask builds a lookUpJoinTask and read outer rows.</p>
<p>When err is not nil, task must not be nil to send the error to the main thread via task.</p>
<p><img src="tidb/./dot/index_lookup_join_buildtask.svg" alt="" /></p>
<h3 id="handletask"><a class="header" href="#handletask">handleTask</a></h3>
<p><img src="tidb/./dot/index_lookup_join_handletask.svg" alt="" /></p>
<h4 id="buildexecutorforindexjoin"><a class="header" href="#buildexecutorforindexjoin">buildExecutorForIndexJoin</a></h4>
<p><img src="tidb/./dot/build_executor_for_index_join.svg" alt="buildExecutorForIndexJoin" /></p>
<h2 id="参考资料-2"><a class="header" href="#参考资料-2">参考资料</a></h2>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-11/">TiDB 源码阅读系列文章（十一）Index Lookup Join</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nested_loop_join">wikipedia: Nested_loop_join</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="agg"><a class="header" href="#agg">Agg</a></h1>
<!-- toc -->
<h2 id="logicalaggregation"><a class="header" href="#logicalaggregation">LogicalAggregation</a></h2>
<pre><code class="language-go">type LogicalAggregation struct {
	logicalSchemaProducer

	AggFuncs     []*aggregation.AggFuncDesc
	GroupByItems []expression.Expression

	// aggHints stores aggregation hint information.
	aggHints aggHintInfo

	possibleProperties [][]*expression.Column
	inputCount         float64 // inputCount is the input count of this plan.

	// noCopPushDown indicates if planner must not push this agg down to coprocessor.
	// It is true when the agg is in the outer child tree of apply.
	noCopPushDown bool
}

type aggHintInfo struct {
	preferAggType  uint
	preferAggToCop bool
}
</code></pre>
<h2 id="aggregationpushdownsolver"><a class="header" href="#aggregationpushdownsolver">aggregationPushDownSolver</a></h2>
<h2 id="findbesttask-4"><a class="header" href="#findbesttask-4">findBestTask</a></h2>
<p><img src="tidb/./dot/agg_exhaustPhysicalPlans.svg" alt="" /></p>
<h3 id="tasktype-2"><a class="header" href="#tasktype-2">TaskType</a></h3>
<pre><code class="language-go">	// CopSingleReadTaskType stands for the a TableScan or IndexScan tasks
	// executed in the coprocessor layer.
	CopSingleReadTaskType

	// CopDoubleReadTaskType stands for the a IndexLookup tasks executed in the
	// coprocessor layer.
	CopDoubleReadTaskType

	// CopTiFlashLocalReadTaskType stands for flash coprocessor that read data locally,
	// and only a part of the data is read in one cop task, if the current task type is
	// CopTiFlashLocalReadTaskType, all its children prop's task type is CopTiFlashLocalReadTaskType
	CopTiFlashLocalReadTaskType

	// CopTiFlashGlobalReadTaskType stands for flash coprocessor that read data globally
	// and all the data of given table will be read in one cop task, if the current task
	// type is CopTiFlashGlobalReadTaskType, all its children prop's task type is
	// CopTiFlashGlobalReadTaskType
	CopTiFlashGlobalReadTaskType

	// MppTaskType stands for task that would run on Mpp nodes, currently meaning the tiflash node.
	MppTaskType
</code></pre>
<h3 id="newpartialaggregate"><a class="header" href="#newpartialaggregate">newPartialAggregate</a></h3>
<p><img src="tidb/./dot/newPartialAggregate.svg" alt="" /></p>
<h4 id="checkaggpushflash"><a class="header" href="#checkaggpushflash">CheckAggPushFlash</a></h4>
<pre><code class="language-go">func CheckAggPushFlash(aggFunc *AggFuncDesc) bool {
	switch aggFunc.Name {
	case ast.AggFuncSum, ast.AggFuncCount, ast.AggFuncMin, ast.AggFuncMax, ast.AggFuncAvg, ast.AggFuncFirstRow, ast.AggFuncApproxCountDistinct:
		return true
	}
	return false
}
</code></pre>
<h4 id="expr_pushdown_blacklist"><a class="header" href="#expr_pushdown_blacklist">expr_pushdown_blacklist</a></h4>
<p>该blacklist 存在mysql.expr_pushdown_blacklist表中</p>
<pre><code class="language-go">func LoadExprPushdownBlacklist(ctx sessionctx.Context) (err error) {
	sql := &quot;select HIGH_PRIORITY name, store_type from mysql.expr_pushdown_blacklist&quot;
	rows, _, err := ctx.(sqlexec.RestrictedSQLExecutor).ExecRestrictedSQL(sql)
  }
</code></pre>
<h4 id="对distinct特殊处理"><a class="header" href="#对distinct特殊处理">对Distinct特殊处理</a></h4>
<pre><code class="language-go">		if aggFunc.HasDistinct {
			/*
				eg: SELECT COUNT(DISTINCT a), SUM(b) FROM t GROUP BY c

				change from
					[root] group by: c, funcs:count(distinct a), funcs:sum(b)
				to
					[root] group by: c, funcs:count(distinct a), funcs:sum(b)
						[cop]: group by: c, a
			*/
</code></pre>
<h4 id="对first-row-function-特殊处理"><a class="header" href="#对first-row-function-特殊处理">对first row function 特殊处理</a></h4>
<pre><code class="language-go">					if !partialIsCop {
						// if partial is a cop task, firstrow function is redundant since group by items are outputted
						// by group by schema, and final functions use group by schema as their arguments.
						// if partial agg is not cop, we must append firstrow function &amp; schema, to output the group by
						// items.
						// maybe we can unify them sometime.
</code></pre>
<h2 id="topb"><a class="header" href="#topb">ToPB</a></h2>
<p><img src="tidb/./dot/agg_to_pb.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aggfunc"><a class="header" href="#aggfunc">AggFunc</a></h1>
<!-- toc -->
<h2 id="aggfunc-interface"><a class="header" href="#aggfunc-interface">AggFunc interface</a></h2>
<pre><code class="language-go">// AggFunc is the interface to evaluate the aggregate functions.
type AggFunc interface {
	// AllocPartialResult allocates a specific data structure to store the
	// partial result, initializes it, and converts it to PartialResult to
	// return back. The second returned value is the memDelta used to trace
	// memory usage. Aggregate operator implementation, no matter it's a hash
	// or stream, should hold this allocated PartialResult for the further
	// operations like: &quot;ResetPartialResult&quot;, &quot;UpdatePartialResult&quot;.
	AllocPartialResult() (pr PartialResult, memDelta int64)

	// ResetPartialResult resets the partial result to the original state for a
	// specific aggregate function. It converts the input PartialResult to the
	// specific data structure which stores the partial result and then reset
	// every field to the proper original state.
	ResetPartialResult(pr PartialResult)

	// UpdatePartialResult updates the specific partial result for an aggregate
	// function using the input rows which all belonging to the same data group.
	// It converts the PartialResult to the specific data structure which stores
	// the partial result and then iterates on the input rows and update that
	// partial result according to the functionality and the state of the
	// aggregate function. The returned value is the memDelta used to trace memory
	// usage.
	UpdatePartialResult(sctx sessionctx.Context, rowsInGroup []chunk.Row, pr PartialResult) (memDelta int64, err error)

	// MergePartialResult will be called in the final phase when parallelly
	// executing. It converts the PartialResult `src`, `dst` to the same specific
	// data structure which stores the partial results, and then evaluate the
	// final result using the partial results as input values. The returned value
	// is the memDelta used to trace memory usage.
	MergePartialResult(sctx sessionctx.Context, src, dst PartialResult) (memDelta int64, err error)

	// AppendFinalResult2Chunk finalizes the partial result and append the
	// final result to the input chunk. Like other operations, it converts the
	// input PartialResult to the specific data structure which stores the
	// partial result and then calculates the final result and append that
	// final result to the chunk provided.
	AppendFinalResult2Chunk(sctx sessionctx.Context, pr PartialResult, chk *chunk.Chunk) error
}
</code></pre>
<h2 id="aggfunc-数据继承关系"><a class="header" href="#aggfunc-数据继承关系">AggFunc 数据继承关系</a></h2>
<pre><code class="language-go">type baseAggFunc struct {
	// args stores the input arguments for an aggregate function, we should
	// call arg.EvalXXX to get the actual input data for this function.
	args []expression.Expression

	// ordinal stores the ordinal of the columns in the output chunk, which is
	// used to append the final result of this function.
	ordinal int

	// frac stores digits of the fractional part of decimals,
	// which makes the decimal be the result of type inferring.
	frac int
}
</code></pre>
<p><img src="tidb/./dot/agg_func.svg" alt="" /></p>
<h3 id="sum4float64"><a class="header" href="#sum4float64">sum4Float64</a></h3>
<p><img src="tidb/./dot/sum4Float64.svg" alt="" /></p>
<h3 id="sum4distinctfloat64"><a class="header" href="#sum4distinctfloat64">sum4DistinctFloat64</a></h3>
<blockquote>
<p>存在某个聚合函数参数为 DISTINCT 时。TiDB 暂未实现对 DedupMode 的支持，因此对于含有 DISTINCT 的情况目前仅能单线程执行。</p>
</blockquote>
<p>所以这个没有MergePartialResult过程</p>
<p><img src="tidb/./dot/agg_sum_distinct.svg" alt="" /></p>
<h2 id="aggfuncdesc"><a class="header" href="#aggfuncdesc">AggFuncDesc</a></h2>
<pre><code class="language-go">type AggFuncDesc struct {
	baseFuncDesc
	// Mode represents the execution mode of the aggregation function.
	Mode AggFunctionMode
	// HasDistinct represents whether the aggregation function contains distinct attribute.
	HasDistinct bool
	// OrderByItems represents the order by clause used in GROUP_CONCAT
	OrderByItems []*util.ByItems
}
</code></pre>
<p><img src="tidb/./dot/AggFuncDesc.svg" alt="" /></p>
<h3 id="aggfunctionmode"><a class="header" href="#aggfunctionmode">AggFunctionMode</a></h3>
<pre><code class="language-go">// AggFunctionMode stands for the aggregation function's mode.
type AggFunctionMode int

// |-----------------|--------------|--------------|
// | AggFunctionMode | input        | output       |
// |-----------------|--------------|--------------|
// | CompleteMode    | origin data  | final result |
// | FinalMode       | partial data | final result |
// | Partial1Mode    | origin data  | partial data |
// | Partial2Mode    | partial data | partial data |
// | DedupMode       | origin data  | origin data  |
// |-----------------|--------------|--------------|
const (
	CompleteMode AggFunctionMode = iota
	FinalMode
	Partial1Mode
	Partial2Mode
	DedupMode
)
</code></pre>
<p><img src="tidb/./dot/AggFunctionMode.svg" alt="" /></p>
<p>不同mode，最后会生成不同的aggfunc, 在不同的phase执行。</p>
<h2 id="aggfunctopbexpr"><a class="header" href="#aggfunctopbexpr">AggFuncToPBExpr</a></h2>
<p>可以下推的agg func</p>
<pre><code class="language-go">func AggFuncToPBExpr(sc *stmtctx.StatementContext, client kv.Client, aggFunc *AggFuncDesc) *tipb.Expr {
//..
	switch aggFunc.Name {
	case ast.AggFuncCount:
		tp = tipb.ExprType_Count
	case ast.AggFuncApproxCountDistinct:
		tp = tipb.ExprType_ApproxCountDistinct
	case ast.AggFuncFirstRow:
		tp = tipb.ExprType_First
	case ast.AggFuncGroupConcat:
		tp = tipb.ExprType_GroupConcat
	case ast.AggFuncMax:
		tp = tipb.ExprType_Max
	case ast.AggFuncMin:
		tp = tipb.ExprType_Min
	case ast.AggFuncSum:
		tp = tipb.ExprType_Sum
	case ast.AggFuncAvg:
		tp = tipb.ExprType_Avg
	case ast.AggFuncBitOr:
		tp = tipb.ExprType_Agg_BitOr
	case ast.AggFuncBitXor:
		tp = tipb.ExprType_Agg_BitXor
	case ast.AggFuncBitAnd:
		tp = tipb.ExprType_Agg_BitAnd
	case ast.AggFuncVarPop:
		tp = tipb.ExprType_VarPop
	case ast.AggFuncJsonObjectAgg:
		tp = tipb.ExprType_JsonObjectAgg
	case ast.AggFuncStddevPop:
		tp = tipb.ExprType_StddevPop
	case ast.AggFuncVarSamp:
		tp = tipb.ExprType_VarSamp
	case ast.AggFuncStddevSamp:
		tp = tipb.ExprType_StddevSamp
	}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hashagg"><a class="header" href="#hashagg">HashAgg</a></h1>
<!-- toc -->
<h2 id="buildhashagg"><a class="header" href="#buildhashagg">buildHashAgg</a></h2>
<p><img src="tidb/./dot/buildHashAgg.svg" alt="" /></p>
<h2 id="hashaggexec"><a class="header" href="#hashaggexec">HashAggExec</a></h2>
<p><img src="tidb/./dot/HashAggExecStruct.svg" alt="" /></p>
<p>HashAggExec 主要有如下几种gorotine, 他们之间通过ch来协作，每个go routine 主要功能如下</p>
<ul>
<li><code>fetchChildData</code> 负责从child Exec中读取chunk数据</li>
<li><code>HashAggPartialWorker</code> 处理<code>fetchChildData</code>的输出数据，调用AggFunc的<code>UpdatePartialResult</code>, 做一个预处理.</li>
<li><code>HashAggFinalWorker</code> 处理<code>HashAggPartialWorker</code>的输出数据，调用AggFunc的<code>MergePartialResult</code>
和<code>AppendFinalResult2Chunk</code>, 输出最终结果到finalOutputCh中</li>
<li><code>HashAggExec.Next</code> 从finalOutputCh中获取最后结果，输出给上层调用者。</li>
</ul>
<p><img src="tidb/./dot/HashAggExec_flow.svg" alt="" /></p>
<h3 id="getgroupkey"><a class="header" href="#getgroupkey">getGroupKey</a></h3>
<p><img src="tidb/./dot/getGroupKey.svg" alt="" /></p>
<h3 id="updatepartialresult"><a class="header" href="#updatepartialresult">updatePartialResult</a></h3>
<p><img src="tidb/./dot/updatePartialResult.svg" alt="" /></p>
<h3 id="consumeintermdata"><a class="header" href="#consumeintermdata">consumeIntermData</a></h3>
<p><img src="tidb/./dot/consumeIntermData.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stream-agg"><a class="header" href="#stream-agg">Stream Agg</a></h1>
<!-- toc -->
<h2 id="streamaggexec"><a class="header" href="#streamaggexec">StreamAggExec</a></h2>
<pre><code class="language-go">// StreamAggExec deals with all the aggregate functions.
// It assumes all the input data is sorted by group by key.
// When Next() is called, it will return a result for the same group.
</code></pre>
<!-- ![](./dot/StreamAggExec.svg) -->
<p>TODO: 这个地方加一些描述</p>
<p><img src="tidb/./dot/StreamAggExec_Next.svg" alt="" /></p>
<h2 id="vecgroupchecker"><a class="header" href="#vecgroupchecker">vecGroupChecker</a></h2>
<p>TODO: 这个地方加一些描述
<img src="tidb/./dot/vecGroupChecker.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pd-1"><a class="header" href="#pd-1">PD</a></h1>
<!-- toc -->
<blockquote>
<p>PD 是 TiKV 的全局中央控制器，存储整个 TiKV 集群的元数据信息，负责整个 TiKV 集群的调度，全局 ID 的生成，以及全局 TSO 授时等。</p>
</blockquote>
<blockquote>
<p>PD 是一个非常重要的中心节点，它通过集成 etcd，自动的支持了分布式扩展以及 failover，解决了单点故障问题。关于 PD 的详细介绍，后续我们会新开一篇文章说明。</p>
</blockquote>
<p>资料收集： </p>
<ol>
<li>官方PD相关文档 https://pingcap.com/blog-cn/#PD</li>
</ol>
<h2 id="主要数据结构-1"><a class="header" href="#主要数据结构-1">主要数据结构</a></h2>
<p><img src="pd/./dot/pd-server-struct.svg" alt="" /></p>
<h3 id="raftcluster"><a class="header" href="#raftcluster">RaftCluster</a></h3>
<pre><code class="language-go">// RaftCluster is used for cluster config management.
// Raft cluster key format:
// cluster 1 -&gt; /1/raft, value is metapb.Cluster
// cluster 2 -&gt; /2/raft
// For cluster 1
// store 1 -&gt; /1/raft/s/1, value is metapb.Store
// region 1 -&gt; /1/raft/r/1, value is metapb.Region
</code></pre>
<h3 id="basiccluster"><a class="header" href="#basiccluster">BasicCluster</a></h3>
<p>provides basic data member and interface of a tikv cluster</p>
<p>用来在内存中保存(查找) tikv cluster的store和region信息</p>
<h2 id="main-流程"><a class="header" href="#main-流程">Main 流程</a></h2>
<p><img src="pd/./dot/main.svg" alt="" /></p>
<h2 id="grpc-service-api"><a class="header" href="#grpc-service-api">GRPC Service api</a></h2>
<h3 id="bootstrap"><a class="header" href="#bootstrap">Bootstrap</a></h3>
<p><img src="pd/./dot/Bootstrap.svg" alt="" /></p>
<h3 id="storeheartbeat"><a class="header" href="#storeheartbeat">StoreHeartbeat</a></h3>
<p>定期 store 向 PD 汇报自己的相关信息，供 PD 做后续调度。譬如，Store 会告诉 PD 当前的磁盘大小，以及剩余空间，如果 PD 发现空间不够了，就不会考虑将其他的 Peer 迁移到这个 Store 上面。</p>
<p><img src="pd/./dot/StoreHeartbeat.svg" alt="" /></p>
<h3 id="regionheartbeat"><a class="header" href="#regionheartbeat">RegionHeartbeat</a></h3>
<p>Region Leader定期向pd 汇报region情况，PD返回Operator
<img src="pd/./dot/RegionHeartbeat.svg" alt="" /></p>
<h3 id="asksplit"><a class="header" href="#asksplit">AskSplit</a></h3>
<p><img src="pd/./dot/AskSplit.svg" alt="" /></p>
<h3 id="splitregions"><a class="header" href="#splitregions">SplitRegions</a></h3>
<h3 id="getregion"><a class="header" href="#getregion">GetRegion</a></h3>
<p><img src="pd/./dot/GetRegion.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tikv-1"><a class="header" href="#tikv-1">TiKV</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raft-rs"><a class="header" href="#raft-rs">raft-rs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rawnode"><a class="header" href="#rawnode">RawNode</a></h1>
<!-- toc -->
<h2 id="raft-module"><a class="header" href="#raft-module">raft module</a></h2>
<p><img src="tikv/./dot/raft-process-in-tikv.png" alt="" /></p>
<p>上图摘自<a href="https://en.pingcap.com/blog/how-tikv-reads-and-writes">How TiKV Reads and Writes</a></p>
<p>基本过程为，client发读写请求给raft leader。</p>
<p>raft leader在处理写请求(比如put k, v)时，将写请求包装为一个log entry, 写到自己
本地的raft log中，然后发给各个follower, follower写成功后，发送ack给leader, 当集群中
大部分节点写成功时(达到commit状态，可以安全的apply 到state machine上，leader返回写成功给client.</p>
<p>leader在处理读请求时，会通过read index检查确认自己还是不是leader,</p>
<h2 id="rawnode-api"><a class="header" href="#rawnode-api">RawNode API</a></h2>
<p>raft对外暴露的接口为RawNode，它和App关系如下图所示:</p>
<p><img src="tikv/./dot/app_raw_node_storage.svg" alt="" /></p>
<p>App在处理client write操作时候，调用Raft Propose 将writes数据作为log entry写入
到raft log。 等该log entry在raft group中达到commit状态时，Client write操作就可以返回了。</p>
<p>App通过RawNode的tick 来驱动raft的logical时钟(驱动leader节点发送hearbeat, follower节点累计触发election timeout)</p>
<p>调用RawNode::step将其他节点发来的raft message发给raft处理。</p>
<p>然后调用RawNode Ready获取需要发送给raft peer的Message, 需要持久化保存的log 
entries, 以及需要apply 到state machine的log entries.</p>
<p>最后App调用RawNode的advance，Raft更新完一些状态后，准备处理App下一次调用。</p>
<p><img src="tikv/./dot/raft_rawnode_tick.svg" alt="" /></p>
<h2 id="tick"><a class="header" href="#tick">tick</a></h2>
<p><code>RawNode::tick</code> 定时时钟，用来驱动leader的定期的向follower/candidate 发送
heartbeat, 而follower则在累积election timoout, 如果follower如果发现超过election timeout没收到
leader的心跳包，则会成为candidate发起campaign.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Tick advances the internal logical clock by a single tick.
///
/// Returns true to indicate that there will probably be some readiness which
/// needs to be handled.
pub fn tick(&amp;mut self) -&gt; bool
<span class="boring">}
</span></code></pre></pre>
<h2 id="step"><a class="header" href="#step">step</a></h2>
<p>App会调用RawNode::step来处理集群其他peer发来的Message。</p>
<p>比如Leader的heartbeat, leader的AppendMsg, candidate的vote request,
follower节点的heartbeat resp, append resp, vote resp等消息。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Step advances the state machine using the given message.
pub fn step(&amp;mut self, m: Message) -&gt; Result&lt;()&gt;
<span class="boring">}
</span></code></pre></pre>
<h2 id="propose"><a class="header" href="#propose">propose</a></h2>
<p>App 通过<code>RawNode::propose</code>来write data到raft log，当这个log entry被
复制到集群大部分节点，并且可以被安全提交时候，App调用 <code>RawNode::ready</code>
获取可以被安全的applied到state machine上的log entries，
把它们apply到state machine上。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Propose proposes data be appended to the raft log.
pub fn propose(&amp;mut self, context: Vec&lt;u8&gt;, data: Vec&lt;u8&gt;) -&gt; Result&lt;()&gt; 
<span class="boring">}
</span></code></pre></pre>
<h2 id="propose_conf_change"><a class="header" href="#propose_conf_change"><code>propose_conf_change</code></a></h2>
<p>App调用<code>RawNode::propose_conf_change</code>来提交对raft 集群成员的配置修改，
等该提交committed, 并且App把集群配置信息保存好后，
App调用<code>RawNode::apply_conf_change</code> 真正的去修改
Raft的集群配置(对应于Raft的<code>ProgressTracker</code>)</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// ProposeConfChange proposes a config change.
///
/// If the node enters joint state with `auto_leave` set to true, it's
/// caller's responsibility to propose an empty conf change again to force
/// leaving joint state.
#[cfg_attr(feature = &quot;cargo-clippy&quot;, allow(clippy::needless_pass_by_value))]
pub fn propose_conf_change(&amp;mut self, context: Vec&lt;u8&gt;, cc: impl ConfChangeI) -&gt; Result&lt;()&gt; {
<span class="boring">}
</span></code></pre></pre>
<h2 id="read_index"><a class="header" href="#read_index"><code>read_index</code></a></h2>
<p>App调用<code>RawNode::read_index</code> 来获取<code>read_index</code></p>
<p>除此之外raft中还有一个lease read.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// ReadIndex requests a read state. The read state will be set in ready.
/// Read State has a read index. Once the application advances further than the read
/// index, any linearizable read requests issued before the read request can be
/// processed safely. The read state will have the same rctx attached.
pub fn read_index(&amp;mut self, rctx: Vec&lt;u8&gt;) {
<span class="boring">}
</span></code></pre></pre>
<h2 id="从step-到ready"><a class="header" href="#从step-到ready">从step 到Ready</a></h2>
<p>App先将hardstate, log entries等保存下来，再将raft message 发出去。</p>
<h3 id="raftmsgs"><a class="header" href="#raftmsgs">Raft::msgs</a></h3>
<p>发送的消息都会暂存在Raft::msgs数组中,在RawNode::ready被调用时，
会先放到RawNode::records中，等entries都保存完后，
app再取到这些消息，将消息发送给对应的peers.</p>
<h4 id="leader"><a class="header" href="#leader">leader</a></h4>
<p>leader会主动定期的发送Heartbeat给follower。
在处理follower的heartbeat resp和Append resp中
会发送AppendEntry(MsgSnapshot, MsgAppend)消息给follower</p>
<p><img src="tikv/./dot/raft_leader_step_msgs.svg" alt="" /></p>
<h4 id="follower"><a class="header" href="#follower">follower</a></h4>
<p>follower 处理leader发送的heartbeat消息和Append消息，然后
发送HeartBeatResp和AppendResp给leader</p>
<p><img src="tikv/./dot/raft_follower_step_msgs.svg" alt="" /></p>
<h4 id="candidate"><a class="header" href="#candidate">candidate</a></h4>
<p>如果在投票期间收到了其他leader的消息，并且验证（term不小于自己的term)Ok的话，
就成为follower，处理heartbeat, appendEntry等消息,流程和上面的follwer一样。</p>
<p>如果没有其他leader的消息，就处理peer发来的投票resp，
如果只是赢得了<code>PRE_ELECTION</code> 就接着发起<code>ELECTION</code>,如果赢了<code>ELECTION</code>,就成为新的leader,
然后立刻<code>bcast_append</code> 发送消息所有peers.</p>
<p><img src="tikv/./dot/raft_candidate_step_msgs.svg" alt="" /></p>
<h3 id="entries"><a class="header" href="#entries">entries</a></h3>
<p><img src="tikv/./dot/raft_log_entries.svg" alt="" /></p>
<h3 id="snapshot"><a class="header" href="#snapshot">snapshot</a></h3>
<p><img src="tikv/./dot/raft_log_snapshot.svg" alt="" /></p>
<h3 id="hardstate-and-softstate"><a class="header" href="#hardstate-and-softstate">hardState and softState</a></h3>
<p>hardstate:</p>
<ol>
<li>term:当前任期, </li>
<li>vote: 给谁投票了。</li>
<li>commit: 当前的commit index</li>
</ol>
<p>softstate 则包含leaderId是谁，当前node的角色是什么</p>
<p><img src="tikv/./dot/raft_state.png" alt="" /></p>
<p>term 扮演逻辑时钟的角色.</p>
<p><img src="tikv/./dot/raft_term.jpeg" alt="" /></p>
<p><img src="tikv/./dot/raft_hard_state.svg" alt="" /></p>
<h3 id="read-states"><a class="header" href="#read-states">read states</a></h3>
<p>在处理读请求时，从Leader节点读数据，leader节点需要确认自己是否还是leader
，如果从follower节点读数据，follower节点要知道当前leader节点的committed index, 
等自己的state machine apply到这个committed index后，再回复数据给client.</p>
<p>Raft提供了两种方法一个是ReadIndex，ReadIndex就是leader节点广播一次心跳，确认自己是leader.</p>
<p>另外一种是LeaseRead,  他假设leader 的 lease 有效期可以到 start + election timeout / clock drift bound 
这个时间点。需要各个服务器之间的clock频率是准的,在lease有效期内，不用发送心跳。</p>
<p>ReadState 负责记录每个客户端读请求状态，</p>
<ol>
<li><code>request_ctx</code>: 客户端唯一标识</li>
<li><code>index</code>: committed index</li>
</ol>
<p><img src="tikv/./dot/raft_read_states.svg" alt="" /></p>
<h2 id="ready"><a class="header" href="#ready">ready</a></h2>
<p>RaftCore::step之后，raft会产生一系列的状态更新，比如要发送raft message, 
有些committed log entry 需要apply到state machine上, 有些log entry 需要保存等.</p>
<p>App通过调用RawNode::ready 返回的struct Ready来获取这些更新</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Returns the outstanding work that the application needs to handle.
///
/// This includes appending and applying entries or a snapshot, updating the HardState,
/// and sending messages. The returned `Ready` *MUST* be handled and subsequently
/// passed back via advance() or its families.
///
/// `has_ready` should be called first to check if it's necessary to handle the ready.
pub fn ready(&amp;mut self) -&gt; Ready
<span class="boring">}
</span></code></pre></pre>
<p><code>Ready</code> struct如下:</p>
<p><img src="tikv/./dot/raft_rawnode_ready.svg" alt="" /></p>
<p>其中主要字段如下：</p>
<ul>
<li><code>hs</code>: Raft 相关的元信息更新，如当前的term，投票结果，committed index 等等。</li>
<li><code>LightReady::committed_entries</code>: 最新被 commit 的日志，需要apply到state machine上。</li>
<li><code>LightReady::messages</code>: 需要发送给其他 peer的Message。</li>
<li><code>Ready::snapshot</code>: 需要apply到state machine 的snapshot。</li>
<li><code>Ready::entries</code>: 需要保存的 log entries。</li>
<li><code>Ready::ReadState</code>: <code>read_index</code>?</li>
</ul>
<p>解释下ReadyRecord, <code>max_number</code>, records的作用。</p>
<p>需要注意的是，Raft需要把entries持久化，才能把message发出去。
所以这个地方用了ReadyRecord先把message 保存起来，
等RawNode::advance之后，才会把message放到RawNode::messages数组。</p>
<p><img src="tikv/./dot/raft_rawnode_ready2.svg" alt="" /></p>
<h2 id="advance"><a class="header" href="#advance">advance</a></h2>
<p>应用在保存完ready中的entries, apply完snapshot， 发送完messages之后，
调用RawNode::advance更新raft一些状态。</p>
<p>主要会更新</p>
<ol>
<li><code>RaftLog::persisted</code>: 表示已经持久化保存日志的index</li>
<li><code>RaftLog::committed_index</code>： 由ProgressTracker的votes来计算committed index</li>
<li><code>RaftLog::applied</code>: 已经apply 到state machine的index</li>
</ol>
<p><img src="tikv/./dot/raft_advance.svg" alt="" /></p>
<h2 id="参考文献-6"><a class="header" href="#参考文献-6">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-2">raft-rs proposal 示例情景分析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31050303">etcd-raft的线性一致读方法一：ReadIndex</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage-1"><a class="header" href="#storage-1">Storage</a></h1>
<!-- toc -->
<h2 id="trait-storage"><a class="header" href="#trait-storage">trait Storage</a></h2>
<p>另外raft底层又抽象出一个trait Storage负责保存Raft log entries和hard state. App需要实现Storage的trait</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Storage saves all the information about the current Raft implementation, including Raft Log,
/// commit index, the leader to vote for, etc.
///
/// If any Storage method returns an error, the raft instance will
/// become inoperable and refuse to participate in elections; the
/// application is responsible for cleanup and recovery in this case.
pub trait Storage {
    /// `initial_state` is called when Raft is initialized. This interface will return a `RaftState`
    /// which contains `HardState` and `ConfState`.
    ///
    /// `RaftState` could be initialized or not. If it's initialized it means the `Storage` is
    /// created with a configuration, and its last index and term should be greater than 0.
    fn initial_state(&amp;self) -&gt; Result&lt;RaftState&gt;;

    /// Returns a slice of log entries in the range `[low, high)`.
    /// max_size limits the total size of the log entries returned if not `None`, however
    /// the slice of entries returned will always have length at least 1 if entries are
    /// found in the range.
    ///
    /// # Panics
    ///
    /// Panics if `high` is higher than `Storage::last_index(&amp;self) + 1`.
    fn entries(&amp;self, low: u64, high: u64, max_size: impl Into&lt;Option&lt;u64&gt;&gt;) -&gt; Result&lt;Vec&lt;Entry&gt;&gt;;

    /// Returns the term of entry idx, which must be in the range
    /// [first_index()-1, last_index()]. The term of the entry before
    /// first_index is retained for matching purpose even though the
    /// rest of that entry may not be available.
    fn term(&amp;self, idx: u64) -&gt; Result&lt;u64&gt;;

    /// Returns the index of the first log entry that is possible available via entries, which will
    /// always equal to `truncated index` plus 1.
    ///
    /// New created (but not initialized) `Storage` can be considered as truncated at 0 so that 1
    /// will be returned in this case.
    fn first_index(&amp;self) -&gt; Result&lt;u64&gt;;

    /// The index of the last entry replicated in the `Storage`.
    fn last_index(&amp;self) -&gt; Result&lt;u64&gt;;

    /// Returns the most recent snapshot.
    ///
    /// If snapshot is temporarily unavailable, it should return SnapshotTemporarilyUnavailable,
    /// so raft state machine could know that Storage needs some time to prepare
    /// snapshot and call snapshot later.
    /// A snapshot's index must not less than the `request_index`.
    fn snapshot(&amp;self, request_index: u64) -&gt; Result&lt;Snapshot&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p>从RawNode到Storage之间的调用路径如下：</p>
<p><img src="tikv/./dot/raft_storage.svg" alt="" /></p>
<h3 id="initial_state"><a class="header" href="#initial_state"><code>initial_state</code></a></h3>
<p>获取初始的RaftState, 设置HardState和ConfState
初始state 为follower, <code>leader_id</code>是<code>INVALID_ID</code></p>
<p><img src="tikv/./dot/raft_storage_initial_state.svg" alt="" /></p>
<h3 id="entries-和snapshot"><a class="header" href="#entries-和snapshot"><code>entries</code> 和<code>snapshot</code></a></h3>
<p>leader在向follower发送log entry或者snapshot时，会调用entries或者snapshot接口。</p>
<p><img src="tikv/./dot/raft_storage_entries.svg" alt="" /></p>
<h3 id="term"><a class="header" href="#term">term</a></h3>
<p><img src="tikv/./dot/raft_storage_term.svg" alt="" /></p>
<h2 id="参考文献-7"><a class="header" href="#参考文献-7">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-2">raft-rs proposal 示例情景分析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31050303">etcd-raft的线性一致读方法一：ReadIndex</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="progresstracker"><a class="header" href="#progresstracker">ProgressTracker</a></h1>
<p>Leader 上对每一个peer，都维护了一个 Progress</p>
<p><img src="tikv/./dot/raft_progresstracker.svg" alt="" /></p>
<h2 id="progress-tracker初始化"><a class="header" href="#progress-tracker初始化">progress tracker初始化</a></h2>
<h2 id="maximal-committed-index"><a class="header" href="#maximal-committed-index">maximal committed index</a></h2>
<p>计算committed index</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Returns the maximal committed index for the cluster. The bool flag indicates whether
/// the index is computed by group commit algorithm successfully.
///
/// Eg. If the matched indexes are [2,2,2,4,5], it will return 2.
/// If the matched indexes and groups are `[(1, 1), (2, 2), (3, 2)]`, it will return 1.
pub fn maximal_committed_index(&amp;mut self) -&gt; (u64, bool)
<span class="boring">}
</span></code></pre></pre>
<p>在日志entries更新保存后，会重新计算一次commit index</p>
<p>leader节点在收到follower的AppendResponse后， 会更新follower pr的mached index.
也会重新计算一次commited index.</p>
<p><img src="tikv/./dot/pr_maximal_committed_index.svg" alt="" /></p>
<h2 id="tally-votes"><a class="header" href="#tally-votes">tally votes</a></h2>
<p>统计election 投票</p>
<p><img src="tikv/./dot/progress_tracker_tally_votes.svg" alt="" /></p>
<h2 id="progress"><a class="header" href="#progress">Progress</a></h2>
<p>这部分分析可以放到Log entry 那儿。</p>
<h3 id="progressnext_idx"><a class="header" href="#progressnext_idx"><code>Progress::next_idx</code></a></h3>
<p>Raft::reset函数中。这个函数会在 Raft 完成选举之后选出的 Leader 上调用，
会将 Leader 的所有其他副本的 <code>next_idx</code>设置为跟 Leader 相同的值。之后，
Leader 就可以会按照 Raft 论文里的规定，广播一条包含了自己的 term 的空 Entry 了</p>
<h2 id="参考文献-8"><a class="header" href="#参考文献-8">参考文献：</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-6">raft-rs 日志复制过程分析</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hearbeat"><a class="header" href="#hearbeat">Hearbeat</a></h1>
<!-- toc -->
<p>hearbeat是leader向follower发送自己还活着，重置follower的election timer, 
同步commit和term.</p>
<p>hearbeat 同时用来实现ReadIndex功能, leader通过hearbeat的ack来确认自己是leader.
readIndex相关实现细节，将在后面readindex 章节专门讲述。</p>
<h2 id="tick-1"><a class="header" href="#tick-1">tick</a></h2>
<p>定时触发leader发送heartbeat消息.</p>
<h2 id="leader-send-heartbeat"><a class="header" href="#leader-send-heartbeat">leader send heartbeat</a></h2>
<p>send hearbeat消息中会把read context带上发给follower, follower的hearbeat resp会把这个context 回传回来。</p>
<p><img src="tikv/./dot/raft_hearbeat.svg" alt="" /></p>
<h2 id="follower-handle_hearbeat"><a class="header" href="#follower-handle_hearbeat">follower <code>handle_hearbeat</code></a></h2>
<p>重置<code>election_elapse</code>= 0, follower 根据Msg中的commit 更新自己的raftlog comitted</p>
<p>如果candidate收到了比自己更高的term的心跳消息，会成为它的follower，并且会
重置自己的 <code>randomized_election_timeout</code>为<code>min_election_timeout</code>到<code>max_election_timeout</code>
之间的一个随机值，这样降低了多个candidate同时发起election,导致split vote概率.</p>
<p>follower发送给leader的MsgHeartbeatResponse中会将heatBeat消息中的context 带上，然后
加上自己的commit index.</p>
<p><img src="tikv/./dot/raft_follower_handle_heartbeat.svg" alt="" /></p>
<h2 id="leader-handle_heartbeat_response"><a class="header" href="#leader-handle_heartbeat_response">leader <code>handle_heartbeat_response</code></a></h2>
<p>在处理follower HeartbeatResp时，首先将resp中的commit信息保存到follower对应的progress.
然后如果需要的话，发送log entries, 从<code>pr.next_idx</code>开始发送，发送<code>max_msg_size</code>
个消息，或者发送给snapshot给follower.</p>
<p>leader根据resp中context信息来更新readindex中的<code>recv_ack</code>
如果收到了quorum的回应，就将该context对应的read state 发送给ReadIndex的发起者（如果是leader
自己就放到<code>RaftCore::read_states</code>中，如果是follower发来的ReadIndexReq, 就发送ReadIndexResp
给follower.</p>
<p>在代码中没找到<code>Progress::comitted_index</code>被使用的地方，不知道这个功能是干什么用的。</p>
<p><img src="tikv/./dot/raft_handle_heartbeat_response.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="election"><a class="header" href="#election">Election</a></h1>
<blockquote>
<ul>
<li>只有日志足够新(按照term和index来比较)的candidate 能够当选为leader</li>
<li>节点决定投给某个candidate后，不能再投给别人(节点在发出vote消息前，需要持久化自己的<code>vote_for</code>)。</li>
<li>candidate 先不增大自己term，先进行pre election，确保自己能当选, 然后再增加自己的term，发起真正的election.</li>
<li>随机化选举超时时间，降低多个candidate同时发起election，导致split vote的概率。</li>
</ul>
</blockquote>
<!-- toc -->
<p><img src="tikv/./dot/raft_server_states.png" alt="" /></p>
<h2 id="pre-vote"><a class="header" href="#pre-vote">Pre vote</a></h2>
<p>raft-rs中新增了pre candidate角色, 可以跟配置<code>Config::pre_vote</code>来决定是否启用这个功能。
在正式开始election之前，先不增加自己的term, 是尝试pre candidate, 如果赢得pre election
才正式的将自己的term +1, 真正的发起选举。</p>
<p>prevote 可以避免在网络分区的情况避免反复的 election 打断当前 leader，触发新的选举造成可用性降低的问题</p>
<p>在prevote过程中，不会更改集群任何节点的hardstate，只是一次查询，查看pre candidate是否有当选的潜质。</p>
<h2 id="发起election"><a class="header" href="#发起election">发起election</a></h2>
<p>follower节点在一段时间内没收到leader的heartbeat后，就会election timeout,  然后先发起pre election,
角色成为pre candidate, 先投自己一票，然后发pre vote消息给集群中的其他节点，进行pre election。</p>
<p>注意pre candidate不会更改自己的hard state vote,也不会更改自己的hardstate term，只是发的消息中term+1</p>
<p>赢得pre election后，将自己的term + 1, 角色成为candidate, 然后投自己一票，<b>更改自己的hard state vote</b>，然后发送Vote 请求给集群中
其他节点，进行election.</p>
<p>发送的pre vote消息和vote消息，会附带上candidate 的last index和term，其他节点据此来决定要不要给它投票。</p>
<p>同时也会附带上自己当前的commit index和commit term, 这样其他节点<code>maybe_commit_by_vote</code>，
尝试更新自己的commit index.</p>
<p>candidate使用<code>ProgressTracker::votes</code> HashMap，来存放voter 给自己的投票结果。(grant or reject)
<code>ProgressTracker::tally_votes</code> 会使用JointConfig来统计是否都得到了大部节点的投票。</p>
<p><img src="tikv/./dot/raft_tick_election.svg" alt="" /></p>
<h2 id="处理投票请求"><a class="header" href="#处理投票请求">处理投票请求</a></h2>
<p>如果节点收到了<b>MsgVote</b>，且term比自己的大，节点就<code>become_follower</code>成为follower，并将自己的term设置和MsgVote.term, 并且将自己的<code>leader_id</code>设置为<code>INVALID_ID</code></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn step(&amp;mut self, m: Message) -&gt; Result&lt;()&gt; {
      // 收到term比自己大的消息
      if m.term &gt; self.term {
                if m.get_msg_type() == MessageType::MsgAppend
                    || m.get_msg_type() == MessageType::MsgHeartbeat
                    || m.get_msg_type() == MessageType::MsgSnapshot
                {
                    self.become_follower(m.term, m.from);
                } else {
                    self.become_follower(m.term, INVALID_ID);
                }
      }
}
<span class="boring">}
</span></code></pre></pre>
<p>如果节点已经给其他candidate投票了(hardstate中的vote)，会reject掉这个candidate的投票（但是不会reject PreVote)</p>
<p>为什么这个地方要自己的<code>leader_id == INVALID_ID</code>才会去投票？看了etcd项目的issues <a href="https://github.com/etcd-io/etcd/pull/9204">8517</a> 解释
看完了也不知道为什么，没有对应的paper链接.</p>
<blockquote>
<p>This includes one theoretical logic change: A node that knows the
leader of the current term will no longer grant votes, even if it has
not yet voted in this term. It also adds a <code>m.Type == MsgPreVote</code>
guard on the <code>m.Term &gt; r.Term</code> check, which was previously thought to
be incorrect (see #8517) but was actually just unclear.</p>
</blockquote>
<p>需要注意的是对于MsgPreVote节点不会更改自己任何hard state, 节点发送的MsgPreVoteResp中的term
是MsgPreVote的term.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn step(&amp;mut self, m: Message) -&gt; Result&lt;()&gt; {
//...
match m.get_msg_type() {
    MessageType::MsgRequestVote | MessageType::MsgRequestPreVote =&gt; {
        // We can vote if this is a repeat of a vote we've already cast...
        let can_vote = (self.vote == m.from) ||
            // ...we haven't voted and we don't think there's a leader yet in this term...
            (self.vote == INVALID_ID &amp;&amp; self.leader_id == INVALID_ID) ||
            // ...or this is a PreVote for a future term...
            (m.get_msg_type() == MessageType::MsgRequestPreVote &amp;&amp; m.term &gt; self.term);
        // ...and we believe the candidate is up to date.
        if can_vote
            &amp;&amp; self.raft_log.is_up_to_date(m.index, m.log_term)
            &amp;&amp; (m.index &gt; self.raft_log.last_index() || self.priority &lt;= m.priority)
        {
               self.log_vote_approve(&amp;m);
               let mut to_send =
                   new_message(m.from, vote_resp_msg_type(m.get_msg_type()), None);
               to_send.reject = false;
               to_send.term = m.term;
               self.r.send(to_send, &amp;mut self.msgs);
               if m.get_msg_type() == MessageType::MsgRequestVote {
                   // Only record real votes.
                   self.election_elapsed = 0;
                   self.vote = m.from;
               }
        }
<span class="boring">}
</span></code></pre></pre>
<p>在<code>RaftLog::is_up_to_date</code>中把自己的<code>last_term</code>和<code>last_index</code>和candidate的做比较，如果candidate日志没自己的新，会reject candidate 的vote.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    pub fn is_up_to_date(&amp;self, last_index: u64, term: u64) -&gt; bool {
        term &gt; self.last_term() || (term == self.last_term() &amp;&amp; last_index &gt;= self.last_index())
    }
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/raft_handle_vote.svg" alt="" /></p>
<p>只有对于MsgRequestVote, 投票时候，才会节点才会修改自己的vote, 重置自己的election_elapsed,
对于MsgPreVote 只是投票，并不会修改vote和<code>election_elapsed</code>。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if m.get_msg_type() == MessageType::MsgRequestVote {
    // Only record real votes.
    self.election_elapsed = 0;
    self.vote = m.from;
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="处理投票响应"><a class="header" href="#处理投票响应">处理投票响应</a></h2>
<p>如果在选举期间，candidate如果收到了term和自己term 相同的MsgAppend, MsgHeartbeat, MsgSnapshot,
说明已经有节点已经赢得了选举，成为了leader, candidate会转变为它的follower.</p>
<p>收到MsgPreVoteResp或MsgPreVoteResp后，candidate会将peer的投票结果保存在<code>ProgressTracker::votes</code> HashMap
中，然后<code>ProgressTracker::tally_votes</code>,根据自己在JointConfig中是否收到了大部分节点的投票，来判断是否赢得了选举。</p>
<p>如果选举失败，则转变为follower。</p>
<p>如果赢得了选举. PreCandidate 状态的会成为Candidate, 增大自己的term，发起真正的Election。 </p>
<p>Candidate状态的会变为真正的leader, 因为leader只能commit包含当前term的log entry，因此当选后，leader立刻广播发送AppendMsg，AppendMsg中的entries可能是空的。加快
commit log的速度。</p>
<p><img src="tikv/./dot/raft_handle_vote_resp.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logentry"><a class="header" href="#logentry">LogEntry</a></h1>
<blockquote>
<ul>
<li>只要term ,index相同，则log entry内容一定相同</li>
<li>当log entry被复制到大多节点时，log entry才能被commit.</li>
<li>leader只能commit <b>包含当前term的</b>log entry.</li>
<li>只有raft log (拥有最大term, 最长Log index)的最新的candidate 才能当选leader.</li>
<li>当follower log entry和leader冲突时，以leader为准,清理掉和leader log不一致的log。</li>
</ul>
</blockquote>
<!-- toc -->
<h2 id="raft-log-处理过程"><a class="header" href="#raft-log-处理过程">Raft log 处理过程</a></h2>
<p>在raft中，一条日志从propose到最后apply到sate machine流程如下</p>
<p><img src="tikv/./dot/raft_log_entry_flow.svg" alt="" /></p>
<h2 id="propose-1"><a class="header" href="#propose-1">propose</a></h2>
<p>收到Propose, append到自己的Log 上，然后<code>bcast_send</code>, 发送Appendmsg给所有的follower.</p>
<p>从<code>pr.next_idx</code>发送<code>max_msg_size</code>个log entry给follower, 发送的log entry可能和
follower的不匹配，follower在AppendResp中会reject，并给出<code>reject_hint</code>,</p>
<p>leader<code>Progress::maybe_decr_to</code>重新调整发送的<code>next_idx</code>，然后重新发送AppendMsg给follower</p>
<p><img src="tikv/./dot/raft_propose.svg" alt="" /></p>
<h2 id="follower-handle_append_entries"><a class="header" href="#follower-handle_append_entries">follower: <code>handle_append_entries</code></a></h2>
<p><img src="tikv/./dot/raft_handle_append_entries.svg" alt="" /></p>
<p>关键函数为<code>RaftLog::maybe_append</code>, 检查term是否一致</p>
<p><img src="tikv/./dot/raft_log_match_term.webp" alt="" /></p>
<p><code>RaftLog::find_conflict</code>, 找到和leader log entry冲突的地方，清理掉和leader不一致的log entry</p>
<h2 id="leader-handle_append_response"><a class="header" href="#leader-handle_append_response">leader: <code>handle_append_response</code></a></h2>
<p><img src="tikv/./dot/raft_handle_append_response.svg" alt="" /></p>
<p>如果<code>Progress::next_idx</code>不对，follower在AppendRespMsg中会reject，然后leader调用<code>Progress::maybe_decr_to</code>来尝试减小<code>Progress::next_idx</code>，然后重新
发送log entries给follower</p>
<p><img src="tikv/./dot/raft_log_next_idx.webp" alt="" /></p>
<p>另外收到follower的append resp之后，leader会计算committed index。由函数<code>ProgressTracker::maximal_comitted_index</code>来根据incoming votes和outging votes中，
已经复制到大部分节点的log entry 最大index，作为<code>maximal_commit_index</code>。</p>
<p>为了安全提交old leader的Log entry.  leader只能commit当前任期Log entry，<code>RaftLog::maybe_commit</code> 会检查计算出来的<code>max_index</code>的term
是否是当前leader的。</p>
<p>如果不是，则不能提交commit index.  所以leader一当选，就会发送一个空的NoOp AppendMsg给所有的follower，
尽快使自己term内log entry达到commit 状态。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Attempts to commit the index and term and returns whether it did.
pub fn maybe_commit(&amp;mut self, max_index: u64, term: u64) -&gt; bool {
    if max_index &gt; self.committed &amp;&amp; self.term(max_index).map_or(false, |t| t == term) {
        debug!(
            self.unstable.logger,
            &quot;committing index {index}&quot;,
            index = max_index
        );
        self.commit_to(max_index);
        true
    } else {
        false
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>比如在下图<b>c</b>中，重新当选的s1 commit了日志<code>(term=2,idx=2)</code>, 然后此时它挂了的话，
出现情况<b>d</b>中 S5重新被选为leader
会出现该被commit的日志被覆盖掉的情况, 此时就出现了不一致情况。</p>
<p>因此要达到图<b>e</b>, 重新当选的s1,在term 4中已经由log entry达到了commit状态，它才能将之前的
日志<code>(term=2,idx=2)</code> commit.</p>
<p><img src="tikv/./dot/leader_commit_current_term.awebp" alt="" /></p>
<h2 id="参考资料-3"><a class="header" href="#参考资料-3">参考资料</a></h2>
<ol>
<li><a href="https://www.jianshu.com/p/1f5cb602dc71">Raft算法分析与实现</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="snapshot-1"><a class="header" href="#snapshot-1">Snapshot</a></h1>
<h2 id="snapshot-msg-struct"><a class="header" href="#snapshot-msg-struct">snapshot msg struct</a></h2>
<p>这个需要主动发起才行？</p>
<p><img src="tikv/./dot/snapshot-struct.svg" alt="" /></p>
<h2 id="follower-send-request-snapshot"><a class="header" href="#follower-send-request-snapshot">follower send request snapshot</a></h2>
<p><img src="tikv/./dot/request-snapshot.svg" alt="" /></p>
<h2 id="leader-send-snapshot"><a class="header" href="#leader-send-snapshot">leader send snapshot</a></h2>
<p><img src="tikv/./dot/send-snapshot.svg" alt="" /></p>
<h2 id="follower-handle-snapshot"><a class="header" href="#follower-handle-snapshot">follower handle snapshot</a></h2>
<p>要处理snapshot，以及confstate</p>
<p><img src="tikv/./dot/handle_snapshot.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="confchange"><a class="header" href="#confchange">ConfChange</a></h1>
<!-- toc -->
<h2 id="joint-consensus"><a class="header" href="#joint-consensus">joint consensus</a></h2>
<p>raft中的决策(投票和计算commit index)，基础是集群中的majority， 由于无法同时原子性的将集群中所有成员配置都修改了，如果一次加入集群节点比较多，
就可能造成集群中使用新配置和使用旧配置的节点形成两个分裂的majority.</p>
<p><img src="tikv/./dot/two_disjoint_majority.png" alt="" /></p>
<p>因此需要加入一个过渡期的概念，在过渡期的节点同时使用新老配置，保证新老conf change可以正常交接。</p>
<p>在conf change期间, 由于各个节点apply conf change的时间点不同，不同节点的配置也会不同。
有的会用conf old, 有的节点开始使用conf new.
有的节点还处于过渡期，投票和计算commit index需要同时使用新老配置来做决策。</p>
<p><img src="tikv/./dot/joint-conf.png" alt="" /></p>
<h3 id="progresstracker-1"><a class="header" href="#progresstracker-1">ProgressTracker</a></h3>
<p><code>ProgressTracker::Configuration</code> 存放着raft集群配置。更改raft 集群配置，主要就是更改
ProgressTracker的<code>conf</code>和<code>ProgressMap</code>。</p>
<p><img src="tikv/./dot/raft_progresstracker.svg" alt="" /></p>
<h3 id="使用新老配置做决策"><a class="header" href="#使用新老配置做决策">使用新老配置做决策</a></h3>
<p>在Joint consensus期间，ProgressTracker同时使用新老配置来计算commit index和vote result</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//JointConfig
pub struct Configuration {
    //incoming 为新的配置
    pub(crate) incoming: MajorityConfig,
    //outgoing 为老的配置
    pub(crate) outgoing: MajorityConfig,
}

// MajorityConfig
pub struct Configuration {
    voters: HashSet&lt;u64&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="计算committed-index"><a class="header" href="#计算committed-index">计算committed index</a></h4>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    //同时统计新老配置中的committed index
    // JointConfig
    pub fn committed_index(&amp;self, use_group_commit: bool, l: &amp;impl AckedIndexer) -&gt; (u64, bool) {
        let (i_idx, i_use_gc) = self.incoming.committed_index(use_group_commit, l);
        let (o_idx, o_use_gc) = self.outgoing.committed_index(use_group_commit, l);
        (cmp::min(i_idx, o_idx), i_use_gc &amp;&amp; o_use_gc)
    }
<span class="boring">}
</span></code></pre></pre>
<h4 id="统计vote-result"><a class="header" href="#统计vote-result">统计vote result</a></h4>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    //
    pub fn vote_result(&amp;self, check: impl Fn(u64) -&gt; Option&lt;bool&gt;) -&gt; VoteResult {
        let i = self.incoming.vote_result(&amp;check);
        let o = self.outgoing.vote_result(check);
        match (i, o) {
            // It won if won in both.
            (VoteResult::Won, VoteResult::Won) =&gt; VoteResult::Won,
            // It lost if lost in either.
            (VoteResult::Lost, _) | (_, VoteResult::Lost) =&gt; VoteResult::Lost,
            // It remains pending if pending in both or just won in one side.
            _ =&gt; VoteResult::Pending,
        }
    }
<span class="boring">}
</span></code></pre></pre>
<h3 id="conf-change流程"><a class="header" href="#conf-change流程">conf change流程</a></h3>
<p>raft-rs中的conf change流程下图所示，比较关键的是，leader节点在conf change被applied后，
会自动append一个空的conf change，开始leave joint流程。空的conf change被app applied之后
该节点就使用新的配置。</p>
<p><img src="tikv/./dot/conf_change_flow.svg" alt="" /></p>
<h2 id="propose-conf-change"><a class="header" href="#propose-conf-change">propose conf change</a></h2>
<p>raft-rs中，conf change先像正常的log entry 那样append 到leader的log中，然后由leader，分发给其他
follower.</p>
<p><code>RaftCore::pending_conf_index</code>指向了该log entry的index，该index可用于防止在这个conf change 被apply完之前，
app 又propose conf change。</p>
<p><img src="tikv/./dot/raft_conf_change.svg" alt="" /></p>
<p>AppPropose的ConfChange如下, ConfChange 会被转换为ConfChangeV2</p>
<p>这里面的context的作用是什么？</p>
<p><img src="tikv/./dot/message_confchange.svg" alt="" /></p>
<h2 id="enter-joint-consensus"><a class="header" href="#enter-joint-consensus">enter joint consensus</a></h2>
<p>conf change被app保存后，应用调用<code>RawNode::apply_conf_change</code>，
来修改<code>ProgressTracker</code>的conf和progress。
在更改配置时，先clone一份<code>ProgressTracker</code>, 然后修改他的conf,
最后在<code>ProgressTracker::apply_conf</code>中实用新的conf。</p>
<p>另外这个地方还会设置conf的<code>auto_leave</code>字段，如果该字段为true, 在后面的<code>RaftCore::commit_apply</code>
会自动的apply 一个空的EntryConfChangeV2消息，开始leave joint.</p>
<p>修改完毕后，就开始了joint consensus，同时会使用新老配置(incoming/outging)。
来统计投票 <code>ProgressTracker::tall_votes</code>和 计算commit index.
<code>ProgressTracker::maximal_committed_index</code>。</p>
<p><img src="tikv/./dot/raft_apply_conf_change.svg" alt="" /></p>
<h2 id="leave-joint-consensus"><a class="header" href="#leave-joint-consensus">leave joint consensus</a></h2>
<p>在conf change 被commit时，说明集群<b>老配置</b>中的大部分节点，都收到了该conf change, 并且会
apply 这它， 这时候集群开始准备leave joint.</p>
<p>leader会append一个空的confchange 给集群中新老配置。当这个空的conf change达到commit 状态时，集群开始
leave joint, 开逐步切换到新的配置。</p>
<h3 id="auto-leave"><a class="header" href="#auto-leave">auto leave</a></h3>
<p>在log entry被applied到state machine时候，raft-rs可以根据<code>applied_index</code>和<code>pending_conf_index</code> 
来判断pending conf change是否已被applied到state machine上。</p>
<p>leader 节点在conf change log entry 被applied之后, 会自动(根据<code>conf.auto_leave</code>)
append一个空的EntryConfChangeV2消息，开始leave joint.</p>
<p><img src="tikv/./dot/raft_leader_empty_conf_change.svg" alt="" /></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn commit_apply(&amp;mut self, applied: u64) {
    let old_applied = self.raft_log.applied;
    #[allow(deprecated)]
    self.raft_log.applied_to(applied);

    // TODO: it may never auto_leave if leader steps down before enter joint is applied.
    if self.prs.conf().auto_leave
        &amp;&amp; old_applied &lt;= self.pending_conf_index
        &amp;&amp; applied &gt;= self.pending_conf_index
        &amp;&amp; self.state == StateRole::Leader
    {
        // If the current (and most recent, at least for this leader's term)
        // configuration should be auto-left, initiate that now. We use a
        // nil Data which unmarshals into an empty ConfChangeV2 and has the
        // benefit that appendEntry can never refuse it based on its size
        // (which registers as zero).
        let mut entry = Entry::default();
        entry.set_entry_type(EntryType::EntryConfChangeV2);

        // append_entry will never refuse an empty
        if !self.append_entry(&amp;mut [entry]) {
            panic!(&quot;appending an empty EntryConfChangeV2 should never be dropped&quot;)
        }
        self.pending_conf_index = self.raft_log.last_index();
        info!(self.logger, &quot;initiating automatic transition out of joint configuration&quot;; &quot;config&quot; =&gt; ?self.prs.conf());
    }
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="leave-joint"><a class="header" href="#leave-joint">leave joint</a></h3>
<p>第二次自动append空的ConfChange 达到commit 状态，App在处理该log entry时，调用</p>
<p><code>RawNode::apply_conf_change</code>开始leave joint, 使用新配置(ProgressTracker.conf.incoming).</p>
<p>在<code>RaftCore::apply_conf_change</code> 的change 为空时候，开始leave joint</p>
<p><img src="tikv/./dot/raft_conf_change_leave_joint.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="readindex"><a class="header" href="#readindex">ReadIndex</a></h1>
<h2 id="readindex-要解决的问题"><a class="header" href="#readindex-要解决的问题">Readindex 要解决的问题</a></h2>
<blockquote>
<p>当出现网络隔离，原来的 Leader 被隔离在了少数派这边，多数派那边选举出了新的 Leader，但是老的 Leader 并没有感知，在任期内他可能会给客户端返回老的数据。</p>
</blockquote>
<h2 id="read-index流程"><a class="header" href="#read-index流程">Read index流程</a></h2>
<p>leader节点在处理读请求时，首先需要与集群多数节点确认自己依然是Leader，然后读取已经被应用到应用状态机的最新数据。</p>
<ol>
<li>记录当前的commit index，称为 ReadIndex</li>
<li>向 Follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是 Leader</li>
<li>等待状态机至少应用到 ReadIndex 记录的 Log</li>
<li>执行读请求，将结果返回给 Client</li>
</ol>
<h2 id="发起readindex"><a class="header" href="#发起readindex">发起ReadIndex</a></h2>
<p>应用通过调用<code>RawNode::read_index</code>方法来获取当前的leader的committed index，
调该接口时，会附带上一个ctx, 它的类型为<code>vec&lt;u8&gt;</code>，起到唯一标识的作用。
在read index ready后，该ctx会回传给App.</p>
<p>如果是在follower 节点，follower节点会将<code>MsgReadIndex</code>转发给leader，等待
leader回复<code>MsgReadIndexResp</code>。</p>
<p>如果ReadOnlyOption为Safe, leader节点则会广播发送一次心跳信息，来确认自己
还是leader,发送的心跳信息，会附带上ctx, follower的hearbeat resp中
会带回该ctx.</p>
<p>如果ReadOnlyOption为LeaseBased并且leader的lease还没过期，就省掉了一次广播心跳信息过程。</p>
<p><img src="tikv/./dot/raft_read_index.svg" alt="" /></p>
<p>等leader确认好自己还是集群的leader后，如果在MsgReadIndex是由leader节点自己发起的，
leader节点就直接将ReadState放入<code>RaftCore::read_states</code>。</p>
<p>如果是由follower 发起的，leader会发送MsgReadIndexResp给follower, follower放入自己的<code>RaftCore::read_states</code>中</p>
<p>等app下次调用ready时，就能跟ctx获取对应的<code>comitted_index</code>了
<code>RaftCore::read_states</code>中。</p>
<h2 id="处理follower-hearbeat-resp"><a class="header" href="#处理follower-hearbeat-resp">处理follower hearbeat resp</a></h2>
<p>leader在收到follower的hearbeat resp时，会使用resp中的ctx,
找到之前的<code>ReadIndexStatus</code>, 更新里面的<code>acks</code>，当<code>acks</code> 达到大多数时候，
read index就Ready了，可以返回给上层应用了。</p>
<p>leader节点上的<code>read_index</code>, leader节点会将ReadIndexStatus中的index,和ctx 放入
<code>RaftCore::read_states</code>, 在App调用ready时候，返回给App.</p>
<p>follower节点上的<code>read_index</code>, leader节点会发送MsgReadIndexResp给follower, follower
将index和ctx放入它自己的<code>RaftCore::states</code>, 然后在App调用ready时，返回给App。</p>
<p><img src="tikv/./dot/raft_read_index_heartbeat_resp.svg" alt="" /></p>
<h2 id="retry"><a class="header" href="#retry">retry</a></h2>
<p>如果client读到了老的leader节点，leader一直没达到quorum，这个该怎么办？</p>
<p>在TiKV代码中，会由上层周期性的检查一次，如果再一个election timout 时间周期内
有的read index没有ready，就重试。</p>
<p><img src="tikv/./dot/peer_read_index.svg" alt="" /></p>
<h2 id="参考资料-4"><a class="header" href="#参考资料-4">参考资料</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/follower-read-the-new-features-of-tidb">TiDB 新特性漫谈：从 Follower Read 说起</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raftkv"><a class="header" href="#raftkv">RaftKV</a></h1>
<blockquote>
<ul>
<li>RaftKV对上层提供了<code>async_snapshot</code>和<code>async_write</code>异步读写接口</li>
<li>RaftKV使用RaftStoreRouter将propose（读写请求）发送给region的peer来处理请求。</li>
<li>RaftKV中和raft相关部分代码封装在PeerStorage中。</li>
<li>RaftKV存储Engine分两种，一个负责存储key,value，一个负责raft log存储.</li>
</ul>
</blockquote>
<!-- toc -->
<h2 id="engines"><a class="header" href="#engines">Engines</a></h2>
<p>RaftKV 的存储分两种，一个为负责存储state machine的key, value, 对应于模板参数<code>EK</code>,
其实现为<code>RocksEngine</code>, </p>
<p>另一个负责存储raft log, 对应于模板参数<code>ER</code>，其实现为<code>RocksEngine</code>或者<code>RaftLogEngine</code>.</p>
<p><a href="https://github.com/tikv/raft-engine">RaftLogEngine</a>是一个单独的repo，对raft log存储做了优化。</p>
<blockquote>
<p>A WAL-is-data engine that used to store multi-raft log</p>
</blockquote>
<p>在初始化调用<code>run_tikv</code>函数时，会根据配置<code>config.raft_engine.enable</code>来决定
是否采用<code>RaftLogEngine</code>来存储raft log日志</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn run_tikv(config: TiKvConfig) {
    //other code...
    if !config.raft_engine.enable {
        run_impl!(RocksEngine)
    } else {
        run_impl!(RaftLogEngine)
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>关键数据结构关系如下：</p>
<p><img src="tikv/./dot/raft_kv.svg" alt="" /></p>
<h2 id="raftrouter"><a class="header" href="#raftrouter">RaftRouter</a></h2>
<p>根据<code>region_id</code>将RaftCmdRequest消息发送到对应的PeerFSM, 由RaftPoller线程池来
批量的处理消息，处理消息时候，先将写操作写入write batch，在这一批处理完毕后
再将整个write batch写入底层的RaftLogEngine或者RocksEngine, 这样降低了IO频率
, 提高了性能。</p>
<p>Normals Hashmap的初始化和batchSystem的机制，详见后面的BatchSystem相关代码分析。</p>
<p><img src="tikv/./dot/RaftKv_RaftRouter.svg" alt="" /></p>
<h2 id="peerstorage"><a class="header" href="#peerstorage">PeerStorage</a></h2>
<p>PeerStorage 使用raftlog和kv engine, 实现了Raft-rs中的Storage trait接口。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Storage {
    fn initial_state(&amp;self) -&gt; Result&lt;RaftState&gt;;
    fn entries(&amp;self, low: u64, high: u64, max_size: impl Into&lt;Option&lt;u64&gt;&gt;) -&gt; Result&lt;Vec&lt;Entry&gt;&gt;;
    fn term(&amp;self, idx: u64) -&gt; Result&lt;u64&gt;;
    fn first_index(&amp;self) -&gt; Result&lt;u64&gt;;
    fn last_index(&amp;self) -&gt; Result&lt;u64&gt;;
    fn snapshot(&amp;self, request_index: u64) -&gt; Result&lt;Snapshot&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p>Raft的log entries，raft state， apply state写入流程如下：</p>
<ol>
<li>先调用PeerFsmDelegate<code>handle_msgs</code>，将RaftCmdRequest 发给<code>raft_group</code></li>
<li>collect ready调用<code>raft_group.ready</code>,获取需要保存的log entries</li>
<li><code>PeerStorage::handle_raft_ready</code> 将log entries, raft state, apply state等信息写到write batch中</li>
<li><code>RaftPoller::end</code> 将write batch写入磁盘中，然后<code>PeerStorage::post_ready</code>更改<code>raft_state</code>,<code>apply_state</code>等状态</li>
</ol>
<p><img src="tikv/./dot/RaftKV_PeerStorage.svg" alt="" /></p>
<h2 id="读写队列"><a class="header" href="#读写队列">读写队列</a></h2>
<p>每个raft region的异步读写队列，存放在<code>Peer</code>中。
调用<code>Peer::propose</code> 处理RaftCmdRequest时，会同时传入一个callback.
Peer会将根据request类型，将request,callback打包在一起放入等待队列中。</p>
<p>对于读请求，会放在ReadIndexQueue，写请求则放入ProposalQueue</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Peer&lt;EK, ER&gt;
where
    EK: KvEngine,
    ER: RaftEngine,
{

    /// The Raft state machine of this Peer.
    pub raft_group: RawNode&lt;PeerStorage&lt;EK, ER&gt;&gt;,
    pending_reads: ReadIndexQueue&lt;EK::Snapshot&gt;,
    proposals: ProposalQueue&lt;EK::Snapshot&gt;,
    //...
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="readindexqueue"><a class="header" href="#readindexqueue">ReadIndexQueue</a></h3>
<p>ReadIndex 大致流程如下:</p>
<ol>
<li>将ReadIndex request和callback放入ReadIndexQueue中，request会生成一个uuid::u64作为Id, 来标识这个request.</li>
<li>带上生成的uuid, 调用<code>raft_group</code>的<code>read_index</code>方法</li>
<li><code>apply_reads</code>处理<code>raft_group.ready()</code>返回的<code>ready.read_states</code></li>
<li>根据uuid从队列中找到对应的callback, 调用callback.(TODO: 这块逻辑好像不是这样的）</li>
</ol>
<p><img src="tikv/./dot/RaftKV_async_snapshot.svg" alt="" /></p>
<h3 id="proposalqueue"><a class="header" href="#proposalqueue">ProposalQueue</a></h3>
<p>在向Raft group propose之后，会调用Callback的<code>invoke_proposed</code>,</p>
<p>Raft ready 之后log entries commited 之后，会回调Callback的<code>invoke_committed</code>
然后将cb 包在Apply中，发送apply task给ApplyFsm.</p>
<p><img src="tikv/./dot/RaftKV_peer_proposals.svg" alt="" /></p>
<p>ApplyFsm在修改写入底层kv engine后，会回调callback的<code>invoke_all</code></p>
<p><img src="tikv/./dot/RaftKV_apply_fsm.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="batchsystem"><a class="header" href="#batchsystem">BatchSystem</a></h1>
<!-- toc -->
<h2 id="batchsystem-init"><a class="header" href="#batchsystem-init">BatchSystem init</a></h2>
<p><img src="tikv/./dot/batch_system_create.svg" alt="" /></p>
<h2 id="router-normals-初始化"><a class="header" href="#router-normals-初始化">Router normals 初始化</a></h2>
<p>RaftPollerBuilder::init 扫描kv engine的<code>CF_RAFT</code> faimly, 加载所有的Region.
对于每个Region,调用PeerFsm::create 创建一个PeerFsm以及用来和它通信的
<code>loose_bounded</code>，tx部分则会放入BasicMailbox，然后放到RaftRouter的normals map中。</p>
<p>代码调用流程如下图：</p>
<p><img src="tikv/./dot/BatchSystem_Router_normals.svg" alt="" /></p>
<h2 id="消息发送处理流程"><a class="header" href="#消息发送处理流程">消息发送处理流程</a></h2>
<p>TODO: 怎么根据key找到对应的regionID ?这个流程需要明确下.</p>
<p>给某个<code>region_id</code>的PeerFsm发送PeerMsg流程如下：</p>
<ol>
<li>通过RaftRouter找到<code>region_id</code>对应的mailbox，并通过mailbox发送到PeerFsm 的msg channel</li>
<li>如果Mailbox中的FsmState是Idle, 则需要用<code>RaftRouter::normalScheduler</code> 将PeerFsm发送到NormalChannel</li>
</ol>
<p>消息处理流程如下：</p>
<ol>
<li>poller线程池poll时，从调用<code>fetch_fsm</code> 从Normal Channel读取一批PeerFsm</li>
<li>poller调用RaftPoller.begin 开始处理这批PeerFsm的消息。</li>
<li>poller从这批PeerFsm 每个rx中unblock方式读取PeerFsm要处理的PeerMsg，由<code>RaftPoller::handle_normals</code>处理消息。
将修改写入write batch.</li>
<li>poller在一批消息处理完毕后，调用RaftPoller.end, 将write batch等写入磁盘中</li>
</ol>
<p><img src="tikv/./dot/batch_system2_flow.svg" alt="" /></p>
<h2 id="routertry_send-发送消息给fsm"><a class="header" href="#routertry_send-发送消息给fsm"><code>Router::try_send</code> 发送消息给Fsm</a></h2>
<p>poller线程工作主要流程是从channel中去fetch 一批fsm，然后再从每个fsm的rx中取消息，处理消息。
为了保证发消息给fsm后，fsm能被poller fetch到，需要将fsm
发送到poller的channel中(使用FsmScheduler来发送)。</p>
<p>为了避免重复的将fsm发送到channel中，TiKV中封装了一个BasicMailbox，
在发给fsm消息的tx上，加了一个FsmState, 用来标记Fsm.</p>
<p>Notified表示已经发送到poller的channel，Idle则表示还没有，在BasicMailbox在发消息时，
如果FsmState为Idle, 则还需要使用FsmScheduler将fsm发送到poller的channel。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct BasicMailbox&lt;Owner: Fsm&gt; {
    sender: mpsc::LooseBoundedSender&lt;Owner::Message&gt;,
    state: Arc&lt;FsmState&lt;Owner&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/router-try-send.svg" alt="" /></p>
<h2 id="poller"><a class="header" href="#poller">Poller</a></h2>
<p><img src="tikv/./dot/poller_fetch_fsm.svg" alt="" /></p>
<p>PollHandler的实现有RaftPoller和ApplyPoller, RaftPoller负责处理RaftCmd和RaftMessage, raft log的保存，
以及驱动raft的状态机。 raft 日志被committed后，交给ApplyPoller来处理。</p>
<p>ApplyPoller将key,value的修改写入KvEngine, 会发送ApplyRes给RaftPoller，告知Apply 结果.</p>
<p><img src="tikv/./dot/raft_apply_poller.svg" alt="" /></p>
<h3 id="raftpoller"><a class="header" href="#raftpoller">RaftPoller</a></h3>
<p><img src="tikv/./dot/raft_poller.svg" alt="" /></p>
<h3 id="applypoller"><a class="header" href="#applypoller">ApplyPoller</a></h3>
<p><img src="tikv/./dot/apply_poller.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raftmessage"><a class="header" href="#raftmessage">RaftMessage</a></h1>
<!-- toc -->
<h2 id="proto"><a class="header" href="#proto">proto</a></h2>
<p><img src="tikv/./dot/raft_message.svg" alt="" /></p>
<h2 id="send_raft_message"><a class="header" href="#send_raft_message"><code>send_raft_message</code></a></h2>
<p>在PeerFsmDelegate <code>handle_msg</code>之后，会调用<code>collect_ready</code>
获取raft中可以发送的raft messages. 在<code>RaftPoller::end</code>将
<code>handle_msg</code>中产生的write batch写入磁盘，然后更新完PeerStorage状态后，
会再Raft的<code>advance_append</code>获取要发送的raft message.</p>
<p>这些raft message 都会通过调用<code>RaftClient::send</code> 先将消息缓存到队列里面，
在最后<code>RaftPoller::end</code>或者<code>RaftPoller::pause</code>时，会调用<code>RaftClient::flush</code>,
将raft message真正的发送出去。</p>
<p><img src="tikv/./dot/send_raft_message.svg" alt="" /></p>
<h2 id="grpc-接口-raftbatch_raft"><a class="header" href="#grpc-接口-raftbatch_raft">grpc 接口: <code>raft</code>/<code>batch_raft</code></a></h2>
<p><img src="tikv/./dot/kv_raft.svg" alt="" /></p>
<h2 id="on_raft_message"><a class="header" href="#on_raft_message"><code>on_raft_message</code></a></h2>
<p>peer收到RaftMessage后处理流程</p>
<p><img src="tikv/./dot/on_raft_message.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raftclient"><a class="header" href="#raftclient">RaftClient</a></h1>
<!-- toc -->
<h2 id="trait-transport"><a class="header" href="#trait-transport">trait Transport</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Transports messages between different Raft peers.
pub trait Transport: Send + Clone {
    fn send(&amp;mut self, msg: RaftMessage) -&gt; Result&lt;()&gt;;

    fn need_flush(&amp;self) -&gt; bool;

    fn flush(&amp;mut self);
}
<span class="boring">}
</span></code></pre></pre>
<p>raft client使用方式如下，先send 将消息放入队列中，最后flush，才真正的发送消息。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A raft client that can manages connections correctly.
///
/// A correct usage of raft client is:
///
/// ```text
/// for m in msgs {
///     if !raft_client.send(m) {
///         // handle error.   
///     }
/// }
/// raft_client.flush();
/// ```
<span class="boring">}
</span></code></pre></pre>
<h2 id="servertransport"><a class="header" href="#servertransport">ServerTransport</a></h2>
<p><img src="tikv/./dot/server_transport.svg" alt="" /></p>
<h3 id="connection-pool"><a class="header" href="#connection-pool">connection pool</a></h3>
<p><img src="tikv/./dot/raft_client_connection_pool.svg" alt="" /></p>
<h3 id="connection-builder"><a class="header" href="#connection-builder">connection builder</a></h3>
<p><img src="tikv/./dot/raft_client_connection_builder.svg" alt="" /></p>
<h3 id="raftclient的创建"><a class="header" href="#raftclient的创建">RaftClient的创建</a></h3>
<p><img src="tikv/./dot/raft_client_new.svg" alt="" /></p>
<h2 id="主要函数调用流程"><a class="header" href="#主要函数调用流程">主要函数调用流程</a></h2>
<h3 id="send"><a class="header" href="#send">send</a></h3>
<p>先从LRUCache 中获取<code>(store_id, conn_id)</code>对应的Queue，如果成功, 
则向 Queue中push raftMessage, 如果push消息时返回Full错误，就调用<code>notify</code>，
通知RaftCall 去<code>pop</code> Queue消息, 将消息发送出去。</p>
<p>如果<code>LRUCache</code>中没有，则向Connection Pool中获取，如果获取还失败的话，则创建一个。</p>
<p><img src="tikv/./dot/raft_client_send1.svg" alt="" /></p>
<p>最后在future pool中执行<code>start</code>, </p>
<h3 id="load_stream"><a class="header" href="#load_stream"><code>load_stream</code></a></h3>
<p><img src="tikv/./dot/raft_client_load_stream.svg" alt="" /></p>
<h3 id="start"><a class="header" href="#start">start</a></h3>
<p>start会异步的调用<code>PdStoreAddrResolver</code>去resolve <code>store_id</code>的addr, 
然后创建连接。</p>
<p>调用<code>batch_call</code> 新建一个<code>RaftCall</code>. 
<code>RaftCall</code>被poll时会不断的去Queue中pop 消息, 并通过grpc stream将消息发出去。</p>
<p>由于包含snap的Message太大，会有<code>send_snapshot_sock</code>专门处理</p>
<p><img src="tikv/./dot/raft_client_start.svg" alt="" /></p>
<h3 id="resolve-store-addr解析"><a class="header" href="#resolve-store-addr解析">resolve: store addr解析</a></h3>
<p>在<code>TiKVServer::init</code>时，store addr resolve worker，会在background yatp 线程池中执行。
调用者使用<code>PdStoreAddrResolver</code>来向add resolver线程
发消息。它创建流程如下:</p>
<p><img src="tikv/./dot/create_resolver.svg" alt="" /></p>
<p>Resolve流程如下：addr-resolver worker收到消息后，先本地cache中看查看有没有store 的addr，如果没有或者
已经过期了，就调用PdClient的<code>get_store</code>方法，获取store的addr地址。</p>
<p>成功后回调<code>task_cb</code>函数，在该回调函数中会触发<code>oneshot_channel</code>, StreamBackEnd::resolve 接着执行
await resolve后边代码。</p>
<p><img src="tikv/./dot/raft_client_resolve.svg" alt="" /></p>
<h3 id="snapshot-发送和接收"><a class="header" href="#snapshot-发送和接收">snapshot 发送和接收</a></h3>
<h4 id="send_snap"><a class="header" href="#send_snap"><code>send_snap</code></a></h4>
<p>包含snap的RaftMessage消息体比较大，将由<code>snap-handler</code> worker来发送.</p>
<p>snap-handler的worker创建和启动流程如下：</p>
<p><img src="tikv/./dot/create_snap_handler.svg" alt="" /></p>
<p><code>send_snapshot_sock</code> 使用<code>scheduler</code>的tx，向<code>snap-handler</code>
worker 发送<code>SnapTask::Send</code> Task, snap-handler worker 调用<code>send_snap</code>创建
发送snap的异步任务，然后在上面创建的Tokio 线程池Runtime中执行。</p>
<p><code>send_snap</code>会去snap manager获取snapshot 构造一个SnapChunk
然后创建和peer所在store addr的grpc connection channel，使用<code>snapshot</code>grpc调用
将SnapChunk数据发送给peer.</p>
<p>SnapChunk实现了Stream trait, 在<code>poll_next</code>中调用<code>read_exact</code>一块块的将snap数据发出去。</p>
<p><img src="tikv/./dot/send_snap.svg" alt="" /></p>
<h4 id="recv_snap"><a class="header" href="#recv_snap"><code>recv_snap</code></a></h4>
<p><img src="tikv/./dot/recv_snap.svg" alt="" /></p>
<h3 id="broadcast_unreachable"><a class="header" href="#broadcast_unreachable"><code>broadcast_unreachable</code></a></h3>
<p>往<code>store_id</code>消息失败, 向自己所有region广播store unreachable消息</p>
<p><img src="tikv/./dot/raft_client_broadcast_unreachable.svg" alt="" /></p>
<h3 id="参考-9"><a class="header" href="#参考-9">参考</a></h3>
<ol>
<li><a href="https://pingcap.com/blog-cn/tikv-source-code-reading-10">Snapshot 的发送和接收</a></li>
</ol>
<h2 id="draft-2"><a class="header" href="#draft-2">draft</a></h2>
<p><img src="tikv/./dot/raft_client_draft.svg" alt="" /></p>
<p><img src="tikv/./dot/raft_client_send.svg" alt="" /></p>
<p><img src="tikv/./dot/raft_client_send_snap_sock.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="region"><a class="header" href="#region">Region</a></h1>
<!-- toc -->
<h2 id="proto-region"><a class="header" href="#proto-region">proto: Region</a></h2>
<p><img src="tikv/./dot/region2.svg" alt="" /></p>
<h2 id="bootstrap-region"><a class="header" href="#bootstrap-region">bootstrap region</a></h2>
<p><code>Node::check_or_prepare_bootstrap_cluster</code>, 创建第一个region.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const LOCAL_PREFIX: u8 = 0x01;
pub const PREPARE_BOOTSTRAP_KEY: &amp;[u8] = &amp;[LOCAL_PREFIX, 0x02];

const MAX_CHECK_CLUSTER_BOOTSTRAPPED_RETRY_COUNT: u64 = 60;
const CHECK_CLUSTER_BOOTSTRAPPED_RETRY_SECONDS: u64 = 3;

<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/check_or_prepare_bootstrap_cluster.svg" alt="" /></p>
<h3 id="初始化raft-state和apply-state"><a class="header" href="#初始化raft-state和apply-state">初始化raft state和apply state</a></h3>
<p>store::<code>prepare_bootstrap_cluster</code> 写入初始的raft state和apply state 到raft engine和kv engine.</p>
<p><code>region_id</code>的<code>raft_state_key</code>为 <code>0x010x02region_id0x02</code>， <code>apply_state_key</code>为 <code> 0x010x02region_id0x03</code></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const LOCAL_PREFIX: u8 = 0x01;
// When we create a region peer, we should initialize its log term/index &gt; 0,
// so that we can force the follower peer to sync the snapshot first.
pub const RAFT_INIT_LOG_TERM: u64 = 5;
pub const RAFT_INIT_LOG_INDEX: u64 = 5;

pub const REGION_STATE_SUFFIX: u8 = 0x01;
pub const RAFT_STATE_SUFFIX: u8 = 0x02;
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/prepare_bootstrap_cluster.svg" alt="" /></p>
<h2 id="region数据结构关系"><a class="header" href="#region数据结构关系">Region数据结构关系</a></h2>
<p>region信息在内存中会三处,<code>StoreMeta</code>, <code>ApplyFsmDelegate</code>, <code>PeerStorage</code>,</p>
<p>StoreMeta 包含了该store的所有region 信息，每个region的ApplyFsm和PeerFsm也都有一份。</p>
<p><img src="tikv/./dot/region_data_struct_ref.svg" alt="" /></p>
<h3 id="region信息保存"><a class="header" href="#region信息保存">region信息保存</a></h3>
<p>region信息(RegionLocalState) 存在Kv Engine，<code>regioin_id</code> 对应的key为<code>region_state_key</code>，
<code>0x01x03region_id0x01</code>, 由函数<code>write_peer_state</code>负责将REgionLocalState写入write batch中。</p>
<p>在ApplyPoller end时候，会将write batch数据写入kv engine对应的Rocksdb.</p>
<p><img src="tikv/./dot/write_peer_state2.svg" alt="" /></p>
<h3 id="region-信息加载"><a class="header" href="#region-信息加载">region 信息加载</a></h3>
<p>在初始化时，会扫描KV engine的<code>0x010x03</code>到<code>ox010x04</code>之间的所有key, 创建PeerFsm,
并将region信息 添加到StoreMeta.</p>
<p><img src="tikv/./dot/load_region.svg" alt="" /></p>
<h3 id="region-信息更改流程"><a class="header" href="#region-信息更改流程">region 信息更改流程</a></h3>
<p>在执行admin cmd时(比如change peer, split region, merge region) 会
更改region信息。ApplyFsmDelegate会先将数据写入write batch中。
然后在更新自己的region 信息。</p>
<p><img src="tikv/./dot/write_peer_state_caller2.svg" alt="" /></p>
<p>ApplyPolle线程在write batch 写入rocksdb后，会发消息个RaftPoller, 
RaftPoller负责更新StoreMeta中的region信息，PeerStorage 中的
region信息，并且如果peer是leader的话，还会将region信息通过
heartbeat 上报给Pd server.</p>
<p><img src="tikv/./dot/on_apply_res.svg" alt="" /></p>
<h2 id="raftapplystate"><a class="header" href="#raftapplystate">RaftApplyState</a></h2>
<p>RaftApplyState的作用是啥？主要记录了当前<code>applied_index</code>是多少。</p>
<p>在内存中有四处引用了RaftApplyState, 其中ApplyFsmDelegate和PeerStorage是长期持有，
而ApplyRes和InvokeContext则是短暂的，他们之间关系如下:</p>
<p>为什么ApplyFsmDelegate和PeerStorage都写了raftApplyState ?</p>
<p><img src="tikv/./dot/ref_RaftApplyState.svg" alt="" /></p>
<p>ApplyFsm 中RaftApplyState更新流程如下:</p>
<ol>
<li>收到raft log entries后，ApplyFsmDelegate使用最后一个log entry 更新自己的<code>commit_index</code>
和<code>commit_term</code>, </li>
<li>在处理每一条raft normal log entry时，根据entry中的cmd 将修改操作写入kv write batch中，并更新<code>applied_index</code>, </li>
<li>处理完毕后，会将<code>apply_state</code> 也写入kv write batch, 最后整个kv write batch会一起写入kv engine.</li>
</ol>
<p>注意此处将apply state和raft log entry 是放在同一write batch中写入kv engine的。这样写入是原子性的，
避免了可能出现不一致的情况。</p>
<p><img src="tikv/./dot/update_RaftApplyState.svg" alt="" /></p>
<p>PeerStorage中RaftApplyState更新流程如下:</p>
<p><img src="tikv/./dot/update_PeerStorage_RaftApplyState.svg" alt="" /></p>
<p>简单看了下,<code>read_index</code>, <code>conf_change</code>, 判断是否merging和spliting用到了这个.</p>
<p><code>applied_index</code>作用</p>
<p><code>applied_index</code> 起了哪些作用?</p>
<h2 id="raftlocalstate"><a class="header" href="#raftlocalstate">RaftLocalState</a></h2>
<p><img src="tikv/./dot/raft_local_state.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="peerstorage-1"><a class="header" href="#peerstorage-1">PeerStorage</a></h1>
<!-- toc -->
<h2 id="trait-storage-1"><a class="header" href="#trait-storage-1">Trait Storage</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Storage saves all the information about the current Raft implementation, including Raft Log,
/// commit index, the leader to vote for, etc.
///
/// If any Storage method returns an error, the raft instance will
/// become inoperable and refuse to participate in elections; the
/// application is responsible for cleanup and recovery in this case.
pub trait Storage {
    /// `initial_state` is called when Raft is initialized. This interface will return a `RaftState`
    /// which contains `HardState` and `ConfState`.
    ///
    /// `RaftState` could be initialized or not. If it's initialized it means the `Storage` is
    /// created with a configuration, and its last index and term should be greater than 0.
    fn initial_state(&amp;self) -&gt; Result&lt;RaftState&gt;;

    /// Returns a slice of log entries in the range `[low, high)`.
    /// max_size limits the total size of the log entries returned if not `None`, however
    /// the slice of entries returned will always have length at least 1 if entries are
    /// found in the range.
    ///
    /// # Panics
    ///
    /// Panics if `high` is higher than `Storage::last_index(&amp;self) + 1`.
    fn entries(&amp;self, low: u64, high: u64, max_size: impl Into&lt;Option&lt;u64&gt;&gt;) -&gt; Result&lt;Vec&lt;Entry&gt;&gt;;

    /// Returns the term of entry idx, which must be in the range
    /// [first_index()-1, last_index()]. The term of the entry before
    /// first_index is retained for matching purpose even though the
    /// rest of that entry may not be available.
    fn term(&amp;self, idx: u64) -&gt; Result&lt;u64&gt;;

    /// Returns the index of the first log entry that is possible available via entries, which will
    /// always equal to `truncated index` plus 1.
    ///
    /// New created (but not initialized) `Storage` can be considered as truncated at 0 so that 1
    /// will be returned in this case.
    fn first_index(&amp;self) -&gt; Result&lt;u64&gt;;

    /// The index of the last entry replicated in the `Storage`.
    fn last_index(&amp;self) -&gt; Result&lt;u64&gt;;

    /// Returns the most recent snapshot.
    ///
    /// If snapshot is temporarily unavailable, it should return SnapshotTemporarilyUnavailable,
    /// so raft state machine could know that Storage needs some time to prepare
    /// snapshot and call snapshot later.
    /// A snapshot's index must not less than the `request_index`.
    fn snapshot(&amp;self, request_index: u64) -&gt; Result&lt;Snapshot&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="log-entries"><a class="header" href="#log-entries">log entries</a></h2>
<p>接口<code>first_index</code>, <code>last_index</code>, <code>initial_state</code>和
Region, RaftLocalState, RaftApplyState之间的关系如下图:</p>
<p><img src="tikv/./dot/peerstorage.svg" alt="" /></p>
<h3 id="entries-和term"><a class="header" href="#entries-和term">entries 和term</a></h3>
<p>entries和term接口实现逻辑如下图所示，主要是调用
RaftEngine的<code>fetch_entries_to</code> 获取<code>[low,high)</code>
范围内的log entries.</p>
<p>如果RaftEngine没有<code>builtin_entry_cache</code>, 则中间加一层EntryCache</p>
<p>在<code>PeerStorage</code>append raft log entry时，会同时append 到EntryCach
和raft write batch中,而 write batch最终会写到raft engine。</p>
<p><img src="tikv/./dot/peerstorage_entries.svg" alt="" /></p>
<h2 id="raft-snapshot"><a class="header" href="#raft-snapshot">raft snapshot</a></h2>
<p>raft snapshot相关proto 如下，其中Snapshot是leader 发送给
follower的snapshot数据结构。 SnapshotMetadata则包含了confState
以及当前的index和term。</p>
<p><img src="tikv/./dot/raft_snapshot.svg" alt="" /></p>
<h3 id="生成-snapshot"><a class="header" href="#生成-snapshot">生成 snapshot</a></h3>
<p>snapshot 生成流程如下:</p>
<ol>
<li>PeerStorage::snapshot函数生成GenSnapTask, 然后<code>Peer::handle_raft_ready_append</code>将task发送给ApplyFsm</li>
<li>ApplyFsm将GenSnapTask转为<code>RegionTask::Gen</code>, 发送给snap-generator worker线程。</li>
<li>snap-generator worker 线程调用<code>peer_storage::do_snapshot</code>生成snapshot, 然后
使用notifier(对应GenSnapTask rx的tx),通知GenSnapTask已OK。</li>
<li>下次PeerStorage::snapshot被调用时，会从GenSnapTask::Receiver中<code>try_recv</code> snapshot, 如果未准备好会返回SnapshotTemporarilyUnavailable，后面会再重试。</li>
</ol>
<h4 id="gensnaptask"><a class="header" href="#gensnaptask">GenSnapTask</a></h4>
<p><img src="tikv/./dot/PeerStorage_snapshot.svg" alt="" /></p>
<h4 id="applyfsmhandle_snapshot"><a class="header" href="#applyfsmhandle_snapshot"><code>ApplyFsm::handle_snapshot</code></a></h4>
<p><code>ApplyFsm::handle_snapshot</code>, 此处主要处理<code>need_sync</code>的状况，
将write batch数据和apply sate flush写入rocksdb后，
再获取rocksdb 的snapshot. 最包装成<code>RegionTask::Gen</code>
由snap-generator worker线程池来执行。</p>
<p><img src="tikv/./dot/ApplyFsm_handle_snapshot.svg" alt="" /></p>
<h4 id="snap-generator线程池执行handle_gen"><a class="header" href="#snap-generator线程池执行handle_gen">snap-generator线程池执行<code>handle_gen</code></a></h4>
<p>在worker/region 的<code>snap-generator</code>线程池中执行生成snapshot的任务,线程池大小为<code>GENERATE_POOL_SIZE</code> 2
该线程池还负责apply snapshot.</p>
<p><img src="tikv/./dot/worker_region_handle_gen.svg" alt="" /></p>
<h4 id="生成snapshotmetadata-peer_storagedo_snapshot"><a class="header" href="#生成snapshotmetadata-peer_storagedo_snapshot">生成SnapshotMetadata: <code>peer_storage::do_snapshot</code></a></h4>
<p><code>do_snapshot</code>负责生成SnapshotMetadata, 而store/snap.rs中的build函数则负责生成snapshot的数据部分。</p>
<p><img src="tikv/./dot/peer_storage_do_snapshot.svg" alt="" /></p>
<h4 id="生成snapshot-数据-snapbuild"><a class="header" href="#生成snapshot-数据-snapbuild">生成Snapshot 数据: <code>Snap::build</code></a></h4>
<p>将region的default, lock, write 几个column family 数据分别写入对应的<code>cf_file</code>
先写入到<code>cf.tmp_file</code>,写入成功后再rename.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const SNAPSHOT_CFS_ENUM_PAIR: &amp;[(CfNames, CfName)] = &amp;[
    (CfNames::default, CF_DEFAULT),
    (CfNames::lock, CF_LOCK),
    (CfNames::write, CF_WRITE),
];

pub const CF_DEFAULT: CfName = &quot;default&quot;;
pub const CF_LOCK: CfName = &quot;lock&quot;;
pub const CF_WRITE: CfName = &quot;write&quot;;
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/snapshot_data_build.svg" alt="" /></p>
<h3 id="send-snapshot"><a class="header" href="#send-snapshot">send snapshot</a></h3>
<h3 id="recv-snapshot"><a class="header" href="#recv-snapshot">recv snapshot</a></h3>
<h3 id="apply-snapshot"><a class="header" href="#apply-snapshot">apply snapshot</a></h3>
<h4 id="schedule_applying_snapshot"><a class="header" href="#schedule_applying_snapshot"><code>schedule_applying_snapshot</code></a></h4>
<p>PeerStorage在处理raft的ready中的snapshot时，先将
snapshot metadata一些信息放入InvokeContext，写入write batch，</p>
<p>在write batch写完磁盘后，在<code>PeerStorage::post_ready</code>中，
将<code>snap_state</code> 设置为<code>SnapState::Applying</code>, 然后发送RegionTask::Apply给
snap generator worker线程池。</p>
<p><img src="tikv/./dot/schedule_applying_snapshot.svg" alt="" /></p>
<h4 id="snap-generator-线程池执行handle_apply"><a class="header" href="#snap-generator-线程池执行handle_apply">snap generator 线程池执行<code>handle_apply</code></a></h4>
<p><img src="tikv/./dot/apply_snap.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thread-local-engine"><a class="header" href="#thread-local-engine">Thread Local Engine</a></h1>
<!-- toc -->
<p>RaftStorage中有read pool和两个write pool, 分别负责storage的读写操作，每个pool中的
每个线程都有自己的RaftKV engine clone为tls engine。</p>
<p>每个线程启动完，在<code>after_start</code>方法中，会调用<code>set_tls_engine</code>设置好自己的<code>TLS_ENGINE_ANY</code>指针。
线程关闭时，会调用<code>destroy_tls_engine</code>清理掉 tls engine.
使用时，用<code>with_tls_engine</code>来使用该指针。</p>
<p><img src="tikv/./dot/raftkv_tls_engine.svg" alt="" /></p>
<p><code>TiKVServer::init_servers</code>初始化时，会创建一些yatp thread pool. TxnScheduler会创建两个
worker pool用来处理Txn command，而Storage和Coprocessor 则有个<code>read_pool</code>,</p>
<p>如果enable了config.pool中的<code>unified_read_pool</code>选项，Storage和coprocessor会共享一个read pool.
Unified thread pool 参见pingcap博客 <a href="https://pingcap.com/zh/blog/unified-thread-pool">Unified Thread Pool</a></p>
<h2 id="with_tls_engine"><a class="header" href="#with_tls_engine"><code>with_tls_engine</code></a></h2>
<p>使用<code>with_tls_engine</code>主要有三处，</p>
<ol>
<li>TxnScheduler在执行事务cmd时，会在worker thread pool，执行read/write cmd.</li>
<li>Storage的正常的<code>batch_get_command</code>, <code>scan</code>等读操作。</li>
<li>Coprocessor的读数据操作。</li>
</ol>
<p><img src="tikv/./dot/caller_with_tls_engine.svg" alt="" /></p>
<h2 id="tls-lrucache"><a class="header" href="#tls-lrucache">tls LRUcache</a></h2>
<p>tls engine的作用是，这样每个线程在使用RaftKV时，会优先使用自己线程 tls RaftKv的LruCache，如果cache miss或者cache的数据
stale了才会使用Lock去mutex共享的变量中获取数据，并插入LruCache中。
这样大大的降低了lock使用的概率.</p>
<p>RaftKv中LRUcache主要有两处：</p>
<h3 id="routercaches"><a class="header" href="#routercaches">Router::caches</a></h3>
<p>Router给region 的raft peer 发送消息时候(具体方法为<code>Router::check_do</code>)，先从Router::caches获取BasicMailbox, 如果cache miss
再加锁去normals中读取BasicMailbox.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Router&lt;N: Fsm, C: Fsm, Ns, Cs&gt; {
    normals: Arc&lt;Mutex&lt;HashMap&lt;u64, BasicMailbox&lt;N&gt;&gt;&gt;&gt;,
    caches: Cell&lt;LruCache&lt;u64, BasicMailbox&lt;N&gt;&gt;&gt;,
    //...
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="localreaderdelegate"><a class="header" href="#localreaderdelegate">LocalReader::delegate</a></h3>
<p>在读数据时，线程会先在自己的LocalReader::delegates LRUcache中获取delegate(具体方法为<code>LocalReader::get_delegate</code>)，
如果cache中没有或者delegate 版本发生了变化，才会去加lock 去<code>store_meta</code>中获取ReadDelegate.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct LocalReader&lt;C, E&gt;
where
    C: ProposalRouter&lt;E::Snapshot&gt;,
    E: KvEngine,
{
    store_meta: Arc&lt;Mutex&lt;StoreMeta&gt;&gt;,
    delegates: LruCache&lt;u64, Arc&lt;ReadDelegate&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="参考文献-9"><a class="header" href="#参考文献-9">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/unified-thread-pool/">Unified Thread Pool | Hackathon 2019 优秀项目介绍</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leader-lease"><a class="header" href="#leader-lease">Leader Lease</a></h1>
<p><a href="https://pingcap.com/zh/blog/lease-read">TiKV 功能介绍 - Lease Read</a> 中描述了
raft 中leader lease 机制如下:</p>
<blockquote>
<p>在 Raft 论文里面，提到了一种通过 clock + heartbeat 的 lease read 优化方法。
也就是 leader 发送 heartbeat 的时候，会首先记录一个时间点 start，
当系统大部分节点都回复了 heartbeat response，那么我们就可以认为 leader 的 
lease 有效期可以到 start + election timeout / clock drift bound这个时间点。</p>
</blockquote>
<blockquote>
<p>由于 Raft 的选举机制，因为 follower 会在至少 election timeout 的时间之后，
才会重新发生选举，所以下一个 leader 选出来的时间一定可以保证大于 
start + election timeout / clock drift bound。</p>
</blockquote>
<p>在TiKV实现中，并不是通过底层的raft-rs heatbeat 机制而是通过上层的读写操作
来renew lease的. </p>
<p>TiKV实现了transfer leadership, 在转移leadership时，target节点
不必等到election timout就能开始选举，上面的假设就不成立了，
因此TiKV引入了<code>LeaseState::Suspect</code>状态</p>
<h2 id="data-struct-2"><a class="header" href="#data-struct-2">data struct</a></h2>
<p>在TiKV中，每个region，有两个存放lease的地方，一个为<code>Peer::leader_lease</code>类型为<code>Lease</code>,一个是ReadDelegate
的<code>RemoteLease</code>. 在更改<code>Peer::lease_lease</code>时，同时也会更新<code>RemoteLease</code>. RemoteLease 主要由LocalReader来用. </p>
<p><img src="tikv/./dot/leader_lease_struct.svg" alt="" /></p>
<h2 id="lease-bound"><a class="header" href="#lease-bound">Lease bound</a></h2>
<p>记录leader lease变量为<code>Lease::bound</code>, Lease::bound值可能为以下两种:</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>///   1. Suspect Timestamp
///      A suspicious leader lease timestamp, which marks the leader may still hold or lose
///      its lease until the clock time goes over this timestamp.
///   2. Valid Timestamp
///      A valid leader lease timestamp, which marks the leader holds the lease for now.
///      The lease is valid until the clock time goes over this timestamp.
<span class="boring">}
</span></code></pre></pre>
<p>如果bound为None，说明lease还没被设置，
或者expired了，或者节点角色由leader变为了follower, lease失效了。</p>
<p>如果boud类型为<code>Either::Left</code>则当前LeaseState为Supsect(说明leadership可能在转移中).
如果bound类型为<code>Either::Right</code>, 并且ts &lt; bound, 则说明还在lease内。</p>
<h2 id="inspect"><a class="header" href="#inspect">Inspect</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Inspect the lease state for the ts or now.
pub fn inspect(&amp;self, ts: Option&lt;Timespec&gt;) -&gt; LeaseState {
    match self.bound {
        Some(Either::Left(_)) =&gt; LeaseState::Suspect,
        Some(Either::Right(bound)) =&gt; {
            if ts.unwrap_or_else(monotonic_raw_now) &lt; bound {
                LeaseState::Valid
            } else {
                LeaseState::Expired
            }
        }
        None =&gt; LeaseState::Expired,
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>inspect被调用流程如下：</p>
<p>主要有个trait RequestInspector，它有个默认的实现<code>inspect</code>方法，然后会调用<code>inspect_lease</code>和<code>has_applied_to_current_term</code>
两个方法。</p>
<p><code>LocalReader</code>的<code>Inspector</code>和<code>Peer</code>都implement了该trait, 各自实现了<code>has_applied_to_current_term</code>和<code>inspect_lease</code>方法。</p>
<p>LocalReader会使用ReadDelegate中的RemoteLease inspect一次看是否在lease内。如果不在，则会将请求通过raft route 发给对应的PeerFsm, Peer::propose时，会使用
Peer的Lease inspect检查一次。</p>
<p><img src="tikv/./dot/call_lease_inspect.svg" alt="" /></p>
<p>此外<code>Peer::inspect_lease</code>还有两个调用路径如下, 主要是对<code>read_index</code>的优化</p>
<p><img src="tikv/./dot/peer_inspect_lease.svg" alt="" /></p>
<p>在leader 节点处理其他peer发来的消息时，
如果消息为MsgReadIndex, 并且当前leader在lease内的话，就直接返回
当前store的commit index，不用再去调用<code>raft_group</code>的step方法了。</p>
<p>另外在leader节点, 在调用<code>raft_group</code>的<code>read_index</code>会如果当前LeaseState不为Supsect, 
则还会用<code>pending_reads</code>中最后一个ReadIndexRequest的<code>renew_lease_time</code>来看是否在
lease内。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // 3. There is already a read request proposed in the current lease;
    //Peer的read_index方法
    fn read_index&lt;T: Transport&gt;(
    //...

        if self.is_leader() {
            match self.inspect_lease() {
                // Here combine the new read request with the previous one even if the lease expired is
                // ok because in this case, the previous read index must be sent out with a valid
                // lease instead of a suspect lease. So there must no pending transfer-leader proposals
                // before or after the previous read index, and the lease can be renewed when get
                // heartbeat responses.
                LeaseState::Valid | LeaseState::Expired =&gt; {
                    // Must use the commit index of `PeerStorage` instead of the commit index
                    // in raft-rs which may be greater than the former one.
                    // For more details, see the annotations above `on_leader_commit_idx_changed`.
                    let commit_index = self.get_store().commit_index();
                    if let Some(read) = self.pending_reads.back_mut() {
                        let max_lease = poll_ctx.cfg.raft_store_max_leader_lease();
                        if read.renew_lease_time + max_lease &gt; renew_lease_time {
                            read.push_command(req, cb, commit_index);
                            return false;
                        }
                    }
                }
                // If the current lease is suspect, new read requests can't be appended into
                // `pending_reads` because if the leader is transferred, the latest read could
                // be dirty.
                _ =&gt; {}
            }
        }

<span class="boring">}
</span></code></pre></pre>
<h2 id="renew"><a class="header" href="#renew">renew</a></h2>
<p>为了保持和etcd逻辑一致，TiKV中并没有通过hearbeat来renew leader lease(参见PingCap博客<a href="https://pingcap.com/zh/blog/lease-read">TiKV 功能介绍 - Lease Read</a>)，
而是在上层通过读写操作来renew leader lease.</p>
<ol>
<li>在节点刚成为leader时，<code>on_role_changed</code>会更新新当选leader的lease.</li>
<li><code>Peer::apply_reads</code>时，使用<code>read_index</code>发起时候的ts更新leadr lease</li>
<li>在处理committed log entries时，会使用write 的ts leader lease.</li>
</ol>
<p>在2，3中，如果Peer处理Raft ready时，如果leader节点成功的处理了<code>read_index</code>和<code>propose</code>write请求，
则说明发起该请求时，该节点肯定是leader, 因此可以用发起请求时候的ts来renew leader lease.</p>
<p><img src="tikv/./dot/caller_maybe_renew_leader_lease.svg" alt="" /></p>
<p>函数<code>Peer::maybe_renew_leader_lease</code> 会更新Peer的leader lease, 同时会update ReadDelegate的RemoteLease</p>
<p><img src="tikv/./dot/maybe_renew_leader_lease.svg" alt="" /></p>
<h2 id="suspect"><a class="header" href="#suspect">suspect</a></h2>
<p>由于leader transfer时，target节点不必等待election timeout就可以发起election, 因此此时要将leader
的Lease设为Supsect状态。</p>
<p><img src="tikv/./dot/call_lease_suspect.svg" alt="" /></p>
<h2 id="expire"><a class="header" href="#expire">expire</a></h2>
<p>在peer节点由leader变为follower时，会将lease expire, 另一方面如果lease长时间没被renew(比如长时间没有读写操作），inspect时，lease会被expire</p>
<p>lease expire时会将bound设置为None，并将ReadDelegate的RemoteLease也设置为None.</p>
<p><img src="tikv/./dot/Leader_expire.svg" alt="" /></p>
<h2 id="参考文献-10"><a class="header" href="#参考文献-10">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/lease-read">TiKV 功能介绍 - Lease Read</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="read-index"><a class="header" href="#read-index">Read Index</a></h1>
<blockquote>
<ol>
<li>弄明白replica memory lock机制</li>
<li>弄明白为啥leader只等待<code>apply_current_term?</code></li>
<li>为什么要send ?</li>
</ol>
</blockquote>
<!-- toc -->
<h2 id="data-struct-3"><a class="header" href="#data-struct-3">data struct</a></h2>
<p>主要有个ReadIndexQueue 等待队列，每个ReadIndexRequest都有一个Uuid唯一标识，
ReadIndexQueue中的context 则记录了从Uuid到reads队列中index的映射。
这样方便根据uuid来找到reads对应的ReadIndexRequest。</p>
<p>在调用raft接口时，uuid会传过去，raft ready时，会将uuid 和commit index带回来。</p>
<p><img src="tikv/./dot/peer_read_index_struct.svg" alt="" /></p>
<h2 id="peer-read_index"><a class="header" href="#peer-read_index">Peer <code>read_index</code></a></h2>
<p>如果region正在splitting，或者merging 则会返回ReadIndexNotReady错误。</p>
<p><code>Peer::read_index</code> 主要流程是生成一个<code>uuid</code>, 然后调用raft RawNode的<code>read_index</code>方法，并传入该<code>uuid</code>
然后将请求放入<code>pending_reads</code>队列中, 等后面raft ready, 拿到commit index了，
再从<code>pending_reads</code>队列中找到该请求，然后调用它的callback。</p>
<p><img src="tikv/./dot/peer_read_index2.svg" alt="" /></p>
<p>此外Tikv对lease和<code>hibernate_state</code>做了一些特殊处理。</p>
<p>在TiKV中，每次<code>read_index</code>时候，还会加上当前<code>monotonic_raw_now</code>时间戳，在后面raft ready时，Peer
会使用该时间戳来renew leader lease.</p>
<p>如果当节点为leader, 并且pending reads中最后一个请求的<code>renew_lease_time + max_lease</code> &gt; <code>monotonic_raw_now</code>
则直接将req,cb,和当前store的<code>commit_index</code> push 到最后一个请求的cmds数组中，不用走后面raft的read index了。</p>
<p>如果当前节点为follower, 并且follower不知道自的leader是谁，leader可能出于hibernated状态，
follower需要广播WakeUP消息，唤醒region中所有peer. 并且向PD询问当前leader是谁。
然后返回NotLeader错误。</p>
<h2 id="leader-apply_reads"><a class="header" href="#leader-apply_reads">leader <code>apply_reads</code></a></h2>
<p>Peer <code>apply_reads</code>时， leader和follower节点的逻辑有些不同</p>
<p>leader 节点raft ready返回的read states 顺序和leader的<code>pending_reads</code>队列中的顺序是一致的。
因此可以直接遍历reads队列。</p>
<p><img src="tikv/./dot/peer_apply_reads.svg" alt="" /></p>
<p>leader节点的<code>ready_to_handle_read</code> 方法如下, 要等到leader当前
term的log entry已被applied, 并且当前不在splitting或者merging时，
才能准备<code>response_read</code></p>
<p>在<a href="https://pingcap.com/zh/blog/linearizability-and-raft">线性一致性和 Raft</a> 中说明了原因</p>
<blockquote>
<p>新选举产生的 Leader，它虽然有全部 committed Log，但它的状态机可能落后于之前的 Leader，状态机应用到当前 term 的 Log 就保证了新 Leader 的状态机一定新于旧 Leader，之后肯定不会出现 stale read。</p>
</blockquote>
<p>这个地方让我疑惑的是为什么不像follower节点那样，等到<code>applied_index</code> &gt;= <code>read_index</code> ?</p>
<p>在<a href="https://pingcap.com/zh/blog/follower-read-the-new-features-of-tidb">TiDB 新特性漫谈：从 Follower Read 说起</a> 中也谈到了这个问题</p>
<blockquote>
<p>因为 TiKV 的异步 Apply 机制，可能会出现一个比较诡异的情况：破坏线性一致性，本质原因是由于 Leader 虽然告诉了 Follower 最新的 Commit Index，但是 Leader 对这条 Log 的 Apply 是异步进行的，在 Follower 那边可能在 Leader Apply 前已经将这条记录 Apply 了，这样在 Follower 上就能读到这条记录，但是在 Leader 上可能过一会才能读取到。
这个并不会破坏TiKV的事务隔离级别.(TODO: 想下为什么不会)</p>
</blockquote>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn ready_to_handle_read(&amp;self) -&gt; bool {
    // TODO: It may cause read index to wait a long time.

    // There may be some values that are not applied by this leader yet but the old leader,
    // if applied_index_term isn't equal to current term.
    self.get_store().applied_index_term() == self.term()
        // There may be stale read if the old leader splits really slow,
        // the new region may already elected a new leader while
        // the old leader still think it owns the split range.
        &amp;&amp; !self.is_splitting()
        // There may be stale read if a target leader is in another store and
        // applied commit merge, written new values, but the sibling peer in
        // this store does not apply commit merge, so the leader is not ready
        // to read, until the merge is rollbacked.
        &amp;&amp; !self.is_merging()
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="follower-apply_reads"><a class="header" href="#follower-apply_reads">follower <code>apply_reads</code></a></h2>
<p>follower apply reads时，为什么要<code>send_read_command</code> ?</p>
<p><img src="tikv/./dot/follower_apply_reads.svg" alt="" /></p>
<p>follower节点要等到<code>applied_index</code> &gt;= <code>read_index</code>, 并且没在apply snapshot才能ready。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn ready_to_handle_unsafe_replica_read(&amp;self, read_index: u64) -&gt; bool {
    // Wait until the follower applies all values before the read. There is still a
    // problem if the leader applies fewer values than the follower, the follower read
    // could get a newer value, and after that, the leader may read a stale value,
    // which violates linearizability.
    self.get_store().applied_index() &gt;= read_index
        // If it is in pending merge state(i.e. applied PrepareMerge), the data may be stale.
        // TODO: Add a test to cover this case
        &amp;&amp; self.pending_merge_state.is_none()
        // a peer which is applying snapshot will clean up its data and ingest a snapshot file,
        // during between the two operations a replica read could read empty data.
        &amp;&amp; !self.is_applying_snapshot()
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="memory-locks-on-replica-read"><a class="header" href="#memory-locks-on-replica-read">memory locks on replica read</a></h2>
<p><a href="https://github.com/tikv/tikv/issues/8926">check memory locks in replica read #8926</a></p>
<blockquote>
<p>This PR changes the read index context from a UUID to something more complex. Now the context may contain the range to check and also may contain the lock returned by the leader. When a leader receives a read index context with key ranges, it will check the in-memory lock table and see if there is any lock blocking this read. If any, it will send the lock back to the follower or learner via the read index context.</p>
</blockquote>
<p>leader节点收到follower的ReadIndex请求后，会调用<code>ConcurrencyManager::read_range_check</code>
放到ReadIndexContext的lock字段中返回给follower</p>
<p><img src="tikv/./dot/ReadIndexContext.svg" alt="" /></p>
<p>在follower的<code>Peer::response_read</code>中，如果有lock，会将LockInfo 返回给<code>cb.invoke_read</code>
返回给上层调用。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn response_read&lt;T&gt;(
    &amp;self,
    read: &amp;mut ReadIndexRequest&lt;EK::Snapshot&gt;,
    ctx: &amp;mut PollContext&lt;EK, ER, T&gt;,
    replica_read: bool,
  ){
  //...
  for (req, cb, mut read_index) in read.cmds.drain(..) {
      // leader reports key is locked
      if let Some(locked) = read.locked.take() {
          let mut response = raft_cmdpb::Response::default();
          response.mut_read_index().set_locked(*locked);
          let mut cmd_resp = RaftCmdResponse::default();
          cmd_resp.mut_responses().push(response);
          cb.invoke_read(ReadResponse {
              response: cmd_resp,
              snapshot: None,
              txn_extra_op: TxnExtraOp::Noop,
          });
          continue;
      }
}
<span class="boring">}
</span></code></pre></pre>
<ol>
<li>为什么要加个memory lock ?</li>
<li>follower怎么处理这个lock ?</li>
<li>上层callback收到这个resp后，会怎么处理？</li>
</ol>
<h2 id="retry-1"><a class="header" href="#retry-1">retry</a></h2>
<p>retry时，需要注意哪些问题 ？</p>
<p><img src="tikv/./dot/peer_read_index.svg" alt="" /></p>
<h2 id="参考文献-11"><a class="header" href="#参考文献-11">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/linearizability-and-raft">线性一致性和 Raft</a></li>
<li><a href="https://pingcap.com/zh/blog/follower-read-the-new-features-of-tidb">TiDB 新特性漫谈：从 Follower Read 说起</a></li>
</ol>
<h1 id="questions-1"><a class="header" href="#questions-1">Questions</a></h1>
<ol>
<li>read index这个为什么不像raft论文中描述那样，等到commit index被applied才返回值?</li>
<li>follower read 和leader read在处理流程上有什么不同？</li>
<li>为什么role变了，要清理掉 在等待的read index ？</li>
<li>这块怎么会有个lock info ?</li>
<li>memory check locking的作用是什么？</li>
<li>TiDB,TiKV怎么开启follower read ? coprocessor 会走follower read吗？</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-snapshot"><a class="header" href="#async-snapshot">Async Snapshot</a></h1>
<!-- toc -->
<h2 id="async-snapshot-1"><a class="header" href="#async-snapshot-1">async snapshot</a></h2>
<p>async snapshot最终会调到LocalReader::read方法, 如果可以local read(根据当前leader
lease是否过期，request是否强制要求走readIndex等来判断）
就直接使用当前kv engine的snapshot. 否则就需要走一次read index,</p>
<p>使用router将readIndex请求发给regionId对应的PeerFsm.</p>
<p><img src="tikv/./dot/raftkv_async_snapshot2.svg" alt="" /></p>
<h2 id="localreader"><a class="header" href="#localreader">LocalReader</a></h2>
<p>LocalReader数据结构关系图如下, 它引用了SotreMeta，用来获取regionID对应的ReadDelegate,
然后根据ReadDelegate,来判断能否直接local read.</p>
<p><img src="tikv/./dot/LocalReader_struct.svg" alt="" /></p>
<p>LocalReader 首先会检查是否还在leader lease内，不在leader lease内,
会将raft 请求redirct给PeerFsm来处理。</p>
<p>在leader lease内的read request
会直接用自己的kv engine snapshot，并且对于同一个Grpc stream request
（使用ThreadReadId 来标识）会用同一个snapshot.</p>
<p>LocalReader 检查leader lease主要靠regionId对应的ReadDelegate中的RemoteLease, 
而ReadDelegate 是集中放在<code>StoreMeta::reads</code>中的，为了避免每次需要用lock来访问
<code>SotreMeta::reads</code>, LocalReader加了一层LruCache.</p>
<p>// TODO: 为啥？
注意这里面的<code>Inspector::inspect_lease</code> 
只检查了<code>ReadDelegate::leader_lease</code>是否为None.
在后面read时候，才会去真正检查是否是validate.</p>
<p>但是搜了代码，貌似没被钓到过。</p>
<p><img src="tikv/./dot/local_reader_read.svg" alt="" /></p>
<p><code>LocalReader::get_delegate</code> 先看LRU cache中是否有regionID对应的ReadDelegate, 
如果没有，或者ReadDelegate的<code>track_ver</code>发生了变化， 则需要加锁然后从StoreMeta.readers
获取最新ReadDelegate</p>
<h3 id="threadreadid"><a class="header" href="#threadreadid">ThreadReadId</a></h3>
<p>tikv 用ThreadReadId来判断reads是否from 同一个GRPC Stream，比如下面的KvService中的<code>batch_commands</code> stream接口中的read request会用同一个ThreadReadId</p>
<p>为什么coprocessor 那而没有用ThreadReadID?</p>
<p><img src="tikv/./dot/tikv_read_id.svg" alt="" /></p>
<p>在LocalReader <code>get_snapshot</code>时，如果<code>cache_read_id</code> == <code>read_id</code>
则直接返回<code>snap_cache</code>, 否则就调用<code>kv_engine::snapshot</code> ，并更新<code>snap_cache</code>
和<code>cache_read_id</code></p>
<p><img src="tikv/./dot/local_reader_read_id.svg" alt="" /></p>
<h3 id="readdelegate"><a class="header" href="#readdelegate">ReadDelegate</a></h3>
<p>StoreMeta的readers保存了regionID -&gt; ReadDelegate的映射。而LocalReader的则有一份<code>delegates</code> 保存了
一些ReadDelegate的cache.</p>
<p><img src="tikv/./dot/read_delegate_struct.svg" alt="" /></p>
<p>TrackVer则用判断ReadDelegate是否发生了变化, 里面的version是个Atomic Arc, 保存当前最新版本，在TrackVer被clone时，
<code>local_ver</code> 会保存当前version最新版本。ReadDelegate update时候，会将Atomic version inc 1.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Clone for TrackVer {
    fn clone(&amp;self) -&gt; Self {
        TrackVer {
            version: Arc::clone(&amp;self.version),
            local_ver: self.version.load(Ordering::Relaxed),
            source: false,
        }
    }
<span class="boring">}
</span></code></pre></pre>
<p>ReadDelegate update, 会更新TrackVer,这样LocalReader 就会知道自己LRU Cache中的
ReadDelegate过期了，会重新加载ReadDelegate.</p>
<h2 id="参考文献-12"><a class="header" href="#参考文献-12">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/lease-read">TiKV 功能介绍 - Lease Read</a></li>
</ol>
<h1 id="draft-3"><a class="header" href="#draft-3">draft</a></h1>
<pre><code>lease = max_lease - (commit_ts - send_ts)

raft_store_max_leader_lease: ReadableDuration::secs(9),
</code></pre>
<p>lease和split/merge之间也有相互影响</p>
<p>lease 的bound是啥？ <code>max_drift</code>起到的作用是？
<code>max_lease</code>是9s, election timeout是10</p>
<p>为什么要引入supsect这个状态呢？直接设置为Invalid 不可以吗？</p>
<p>LeaseState有<code>Suspect</code>, <code>Valid</code>, <code>Expired</code>三种状态</p>
<p><img src="tikv/./dot/leader_lease.svg" alt="" /></p>
<p>这里面的ts 需要使用<code>monotonic_raw_now</code>, 具体原因是？</p>
<h2 id="readindex-1"><a class="header" href="#readindex-1">ReadIndex</a></h2>
<p>这个corner case不是很明白，为什么新的leader的commit index不是最新的？</p>
<blockquote>
<p>但这里，需要注意，实现 ReadIndex 的时候有一个 corner case，在 etcd 和 TiKV 最初实现的时候，我们都没有注意到。也就是 leader 刚通过选举成为 leader 的时候，这时候的 commit index 并不能够保证是当前整个系统最新的 commit index，所以 Raft 要求当 leader 选举成功之后，首先提交一个 no-op 的 entry，保证 leader 的 commit index 成为最新的。</p>
</blockquote>
<blockquote>
<p>所以，如果在 no-op 的 entry 还没提交成功之前，leader 是不能够处理 ReadIndex 的。但之前 etcd 和 TiKV 的实现都没有注意到这个情况，也就是有 bug。解决的方法也很简单，因为 leader 在选举成功之后，term 一定会增加，在处理 ReadIndex 的时候，如果当前最新的 commit log 的 term 还没到新的 term，就会一直等待跟新的 term 一致，也就是 no-op entry 提交之后，才可以对外处理 ReadIndex。</p>
</blockquote>
<h2 id="follower-read"><a class="header" href="#follower-read">Follower read</a></h2>
<h2 id="参考文献-13"><a class="header" href="#参考文献-13">参考文献</a></h2>
<blockquote>
<p>TiKV 的 lease read 实现在原理上面跟 Raft 论文上面的一样，但实现细节上面有些差别，我们并没有通过 heartbeat 来更新 lease，</p>
</blockquote>
<blockquote>
<p>而是通过写操作。对于任何的写入操作，都会走一次 Raft log，所以我们在 propose 这次 write 请求的时候，记录下当前的时间戳 start，然后等到对应的请求 apply 之后，我们就可以续约 leader 的 lease。当然实际实现还有很多细节需要考虑的，譬如：</p>
</blockquote>
<ol>
<li><a href="https://pingcap.com/zh/blog/lease-read">TiKV 功能介绍 - Lease Read</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-write"><a class="header" href="#async-write">Async Write</a></h1>
<!-- toc -->
<h2 id="提交raft-proposal"><a class="header" href="#提交raft-proposal">提交raft proposal</a></h2>
<p>Peer调用raft RawNode::propose方法，将RaftCmdRequest 提交给raft log, 
然后将write callback和要写入的log entry的index放入了本地的proposals等待队列。</p>
<p>这个地方和ReadIndex一样，会带上当前时间戳，作为后续renew lease的时间戳</p>
<p><img src="tikv/./dot/peer_propose_normal.svg" alt="" /></p>
<h2 id="处理raft-committed"><a class="header" href="#处理raft-committed">处理raft committed</a></h2>
<p>leader节点在应用propse log entry后，会将该log entry先保存在本地，然后复制到各个follower上，待集群中大部分节点
都保存了该log entry时， log entry达到comitted 状态，TiKV可以安全的把它apply 到kv engine上了。</p>
<p>Peer在<code>handle_raft_committed_entries</code>时，会根据log entry的term和index找到它在proposals队列中
对应的proposal, 然后主要做如下工作</p>
<ol>
<li>renew leader lease, 使用proposal时候的时间戳来更新leader lease.</li>
<li>调用该proposal cb的<code>invoke_comitted</code>。</li>
<li>将comitted entries和它们的cbs 打包发给apply fsm处理。</li>
</ol>
<p><img src="tikv/./dot/peer_handle_raft_committed.svg" alt="" /></p>
<h2 id="apply-to-kvengine"><a class="header" href="#apply-to-kvengine">Apply to KvEngine</a></h2>
<p>comitted entries, 连同它对应的Proposals， 被路由到ApplyFsm后，由ApplyFsm 负责执行comitted entries中的<code>RaftCmdRequest</code>
保存完毕后，调用回调<code>cb.invoke_with_response</code>,通知write 已经apply 到kv engine了.</p>
<p>这些Proposals首先会被放到<code>ApplyDelegate::pending_cmds</code>队列中, 等raft cmd被执行完毕后，
从<code>ApplyDelegate::pending_cmds</code>队列中，找到对应的Proposal, 然后将它的callback 放入
<code>ApplyContext::cbs</code>中。</p>
<p>最后再<code>ApplyContext::flush</code>时, 回调<code>ApplyContext::cbs</code>中的callback, 然后将ApplyRes 发
给PeerFsm.</p>
<p><img src="tikv/./dot/applyfsm_handle_apply.svg" alt="" /></p>
<h3 id="raftapplystate-1"><a class="header" href="#raftapplystate-1">RaftApplyState</a></h3>
<p>处理完一个comitted entry后，会更新<code>applied_index_term</code>和<code>applied_index</code>
这两项会放到ApplyRes，后面通知PeerFsm, PeerFsm会更新PeerStorage中的
<code>apply_state</code>和<code>applied_index_term</code>.</p>
<p>在生成snaptask时，也会用到这两项。</p>
<p><img src="tikv/./dot/apply_RaftApplyState.svg" alt="" /></p>
<h3 id="exec-raft-cmd"><a class="header" href="#exec-raft-cmd">exec raft cmd</a></h3>
<p>write有四种命令，<code>Put</code>, <code>Delete</code>, <code>DeleteRange</code>, <code>IngestSst</code>, 其中put/delete是写到write batch上的。</p>
<p><img src="tikv/./dot/ApplyDelegate__exec_write_cmd.svg" alt="" /></p>
<h3 id="applyresultyield"><a class="header" href="#applyresultyield">ApplyResult::yield</a></h3>
<p>每次<code>ApplyPoller::handle_normal</code>时，会在<code>ApplyDelegate::handle_start</code>中记录开始的时间，
然后在<code>ApplyDelegate::handle_raft_entry_normal</code>, 每次处理一个raft log entries,
如果需要write batch需要写入kv engine的话，就调用ApplyContext::commit，将write batch
提交，然后计算已经消耗的时间。如果时间超过<code>ApplyContext::yield_duration</code>, 就返回ApplyResult::yield.</p>
<p>在<code>ApplyDelegate::handle_raft_committed_entries</code>中，会将剩余的committed entries 保存到
<code>ApplyDelegate::yield_state</code>中。</p>
<p>等下次重新开始<code>handle_normal</code>时，先调用<code>resume_pending</code>, 先处理<code>ApplyDelegate::yield_state</code>中的log 
entries.</p>
<p><img src="tikv/./dot/ApplyResult__yield.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="region-epoch"><a class="header" href="#region-epoch">Region Epoch</a></h1>
<p>Region Epoch变更规则如下：</p>
<blockquote>
<ol>
<li>配置变更的时候， <code>conf_ver</code>+ 1。</li>
<li>Split 的时候，原 region 与新 region 的 version均等于原 region 的 version+ 新 region 个数。</li>
<li>Merge 的时候，两个 region 的 version均等于这两个 region 的 version最大值 + 1。</li>
</ol>
</blockquote>
<p>在raft peer之间发送RaftMessage(比如hearbeat, append等消息）时，<code>Peer::send_raft_message</code>
会把region的epoch放到RaftMessage中, 然后再发出去.</p>
<p>在raft peer收到消息时，<code>PeerFsmDelegate::check_msg</code>中会检查Region Epoch，如果不匹配的话，会drop skip掉这个消息。</p>
<p>在处理上层应用通过raft router发来的RaftCmdRequest时，也会检查它的region epoch和term, 如果不Match会返回EpochNotMatch.</p>
<p>在ApplyFsm 执行raft cmd时，也会检查region epoch，如果不match的话，会返回EpochNotMatch，</p>
<p><img src="tikv/./dot/region_epoch.svg" alt="" /></p>
<p><code>check_region_epoch</code>检查逻辑如下</p>
<ol>
<li>对于normal request，只会检查version.</li>
<li>对于AdminCmd 有个map</li>
</ol>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AdminCmdEpochState {
    pub check_ver: bool,
    pub check_conf_ver: bool,
    pub change_ver: bool,
    pub change_conf_ver: bool,
}


lazy_static! {
    /// WARNING: the existing settings in `ADMIN_CMD_EPOCH_MAP` **MUST NOT** be changed!!!
    /// Changing any admin cmd's `AdminCmdEpochState` or the epoch-change behavior during applying
    /// will break upgrade compatibility and correctness dependency of `CmdEpochChecker`.
    /// Please remember it is very difficult to fix the issues arising from not following this rule.
    ///
    /// If you really want to change an admin cmd behavior, please add a new admin cmd and **do not**
    /// delete the old one.
    pub static ref ADMIN_CMD_EPOCH_MAP: HashMap&lt;AdminCmdType, AdminCmdEpochState&gt; = [
        (AdminCmdType::InvalidAdmin, AdminCmdEpochState::new(false, false, false, false)),
        (AdminCmdType::CompactLog, AdminCmdEpochState::new(false, false, false, false)),
        (AdminCmdType::ComputeHash, AdminCmdEpochState::new(false, false, false, false)),
        (AdminCmdType::VerifyHash, AdminCmdEpochState::new(false, false, false, false)),
        // Change peer
        (AdminCmdType::ChangePeer, AdminCmdEpochState::new(false, true, false, true)),
        (AdminCmdType::ChangePeerV2, AdminCmdEpochState::new(false, true, false, true)),
        // Split
        (AdminCmdType::Split, AdminCmdEpochState::new(true, true, true, false)),
        (AdminCmdType::BatchSplit, AdminCmdEpochState::new(true, true, true, false)),
        // Merge
        (AdminCmdType::PrepareMerge, AdminCmdEpochState::new(true, true, true, true)),
        (AdminCmdType::CommitMerge, AdminCmdEpochState::new(true, true, true, false)),
        (AdminCmdType::RollbackMerge, AdminCmdEpochState::new(true, true, true, false)),
        // Transfer leader
        (AdminCmdType::TransferLeader, AdminCmdEpochState::new(true, true, false, false)),
    ].iter().copied().collect();
}
<span class="boring">}
</span></code></pre></pre>
<p>会修改region epoch的RaftAdminCmd</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conf-change"><a class="header" href="#conf-change">Conf Change</a></h1>
<!--toc -->
<h2 id="propose-conf-change-1"><a class="header" href="#propose-conf-change-1">propose conf change</a></h2>
<p><img src="tikv/./dot/peer_propose_conf_change.svg" alt="" /></p>
<h2 id="applyfsm"><a class="header" href="#applyfsm">ApplyFsm</a></h2>
<h3 id="exec_change_peer"><a class="header" href="#exec_change_peer"><code>exec_change_peer</code></a></h3>
<p><img src="tikv/./dot/apply_exec_change_peer.svg" alt="" /></p>
<h3 id="exec_change_peer_v2"><a class="header" href="#exec_change_peer_v2"><code>exec_change_peer_v2</code></a></h3>
<p><img src="tikv/./dot/apply_exec_change_peer_v2.svg" alt="" /></p>
<h2 id="on_ready_change_peer"><a class="header" href="#on_ready_change_peer"><code>on_ready_change_peer</code></a></h2>
<p><img src="tikv/./dot/on_ready_change_peer2.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="split-region"><a class="header" href="#split-region">Split Region</a></h1>
<!-- toc -->
<h2 id="data-struct-4"><a class="header" href="#data-struct-4">data struct</a></h2>
<p><img src="tikv/./dot/split_proto.svg" alt="" /></p>
<p>region相关信息如下：</p>
<p><img src="tikv/./dot/region2.svg" alt="" /></p>
<h2 id="split-check"><a class="header" href="#split-check">Split Check</a></h2>
<h2 id="执行split操作"><a class="header" href="#执行split操作">执行Split操作</a></h2>
<p>Split操作是在当前region raft group的各个节点上原地进行的，
Split 操作被当做一条Proposal 通过 Raft 达成共识，
然后各自的Peer分别执行 Split。</p>
<p>Split操作会修改region epoch中的version字段。</p>
<h3 id="准备和propose-split"><a class="header" href="#准备和propose-split">准备和propose split</a></h3>
<p>向pd server发送<code>ask_batch_split</code>请求获取新的<code>region_ids</code>和以及每个region对应的<code>peer_ids</code>
然后发送AdminCmdType为BatchSplit的raft cmd给PeerFsm。</p>
<p><img src="tikv/./dot/peer__on_prepare_split_region.svg" alt="" /></p>
<p>然后PeerFsmDelegate在处理BatchSplit RaftCmdRequest时，会像正常的log entry那样,propose到raft, 然后有
leader 复制到各个peer, 等达到commit状态时，ApplyFsm开始执行<code>exec_batch_split</code>.</p>
<h3 id="applydelegateexec_batch_split-保存split-region-state"><a class="header" href="#applydelegateexec_batch_split-保存split-region-state"><code>ApplyDelegate::exec_batch_split</code>: 保存split region state</a></h3>
<p>SplitRequest会被转换为SplitBatchRequest, 然后执行<code>ApplyDelegate::exec_batch_split</code></p>
<p>分裂后的region epoch中的version会更新。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// fn exec_batch_split&lt;W: WriteBatch&lt;EK&gt;&gt;(
let mut derived = self.region.clone();

// 更新region epoch.version
let new_version = derived.get_region_epoch().get_version() + new_region_cnt as u64;
derived.mut_region_epoch().set_version(new_version);
<span class="boring">}
</span></code></pre></pre>
<p>新的region复用之前region的peers信息，会根据SplitRequest中更新new region的 <code>region_id</code>以及region的<code>peer_ids</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for req in split_reqs.get_requests() {
    let mut new_region = Region::default();
    new_region.set_id(req.get_new_region_id());
    new_region.set_region_epoch(derived.get_region_epoch().to_owned());
    new_region.set_start_key(keys.pop_front().unwrap());
    new_region.set_end_key(keys.front().unwrap().to_vec());
    new_region.set_peers(derived.get_peers().to_vec().into());
    for (peer, peer_id) in new_region
        .mut_peers()
        .iter_mut()
        .zip(req.get_new_peer_ids())
    {
        peer.set_id(*peer_id);
    }
    new_split_regions.insert(
        new_region.get_id(),
        NewSplitPeer {
            peer_id: util::find_peer(&amp;new_region, ctx.store_id).unwrap().get_id(),
            result: None,
        },
    )
    regons.push(new_region)
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/ApplyDelegate__exec_split.svg" alt="" /></p>
<p>最后调用<code>write_peer_state</code>将split 后的<code>new_region</code> RegionLocalState写入WriteBatch</p>
<p><code>pending_create_peers</code></p>
<h3 id="peerfsmdelegateon_ready_split_region"><a class="header" href="#peerfsmdelegateon_ready_split_region"><code>PeerFsmDelegate::on_ready_split_region</code></a></h3>
<p>创建新的region对应的PeerFsm并且注册到RaftRouter, 创建ApplyFsm
并且注册到ApplyRouter. 然后更新StoreMeta中的<code>readers</code>, 
<code>regions</code>, <code>region_ranges</code>等元信息。</p>
<p>如果是leader会向PD 上报自己和新的region的元信息。只有leader节点，在执行split时，
可以开始new region的campaign操作，其他非leader节点，
要等选举超时之后，才能开始选举操作。</p>
<blockquote>
<p>读过 Peer::handle_raft_ready_append中记录 last_committed_split_idx的小伙伴应该能注意这里并没有让租约立马失效，仅仅设置 index 阻止下次续约。换句话说，在 Split 期间的那次租约时间内是可以让原 Region 的 Leader 提供本地读取功能的。根据前面的分析，这样做貌似是不合理的。</p>
</blockquote>
<blockquote>
<p>原因十分有趣，对于原 Region 非 Leader 的 Peer 来说，它创建新 Region 的 Peer 是不能立马发起选举的，得等待一个 Raft 的选举超时时间，而对于原 Region 是 Leader 的 Peer 来说，新 Region 的 Peer 可以立马发起选举。Raft 的超时选举时间是要比租约时间长的，这是保证租约正确性的前提</p>
</blockquote>
<p><img src="tikv/./dot/PeerFsmDelegate__on_ready_split_region_new_region.svg" alt="" /></p>
<p><code>last_committed_split_idx</code></p>
<h2 id="spliting期间的一致性"><a class="header" href="#spliting期间的一致性">Spliting期间的一致性</a></h2>
<p><img src="tikv/./dot/peer__is_splitting.svg" alt="" /></p>
<p>在split时，会更改region epoch，split期间对于原有region的写操作会返回EpochNotMatch错误。</p>
<p>下面分几种情况讨论</p>
<ol>
<li>leader节点还没apply BatchSplit.</li>
</ol>
<p><code>is_splitting</code>
在split期间，不会renew leader lease, </p>
<h2 id="参考文献-14"><a class="header" href="#参考文献-14">参考文献</a></h2>
<ol>
<li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-20">Region Split 源码解析</a></li>
</ol>
<h2 id="questions-2"><a class="header" href="#questions-2">Questions</a></h2>
<ol>
<li>在splitting 期间是怎么处理读写的？</li>
<li>region A分裂为 region A, B， B的成员和A的一样吗？</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="merge-region"><a class="header" href="#merge-region">Merge Region</a></h1>
<!-- toc -->
<p>Merge Region时，PD先将source region和targer Region 的TiKV节点对齐。</p>
<h2 id="merge流程"><a class="header" href="#merge流程">Merge流程</a></h2>
<p>理解的关键点</p>
<ol>
<li>Source region 在向target region 提交CommitMerge前,怎么发现和处理target region发生了变动</li>
<li>source region的<code>on_catch_up_logs_for_merge</code>和<code>on_ready_prepare_merge</code>这两个被调用时序问题。</li>
<li>target和source region之间通过CatchUpLogs中的atomic <code>catch_up_logs</code>,来同步补齐的状态。</li>
</ol>
<h2 id="相关raftcmdrequest"><a class="header" href="#相关raftcmdrequest">相关RaftCmdRequest</a></h2>
<p>在merge region中，主要涉及到的raft cmd为PrePareMergeRequest和CommitMergeRequest</p>
<p>PrepareMergeRequest 将由source region来proposal并执行，在source region执行PrepareMerge时，
PeerState为Merging, 并在RaftLocalState中保存了一个<code>MergeState</code>。然后发CommitMergeRequest给本地的<code>target region</code>, </p>
<p>target region把CommitMergeRequest proposal到target region的raft group后，
由target region来执行CommitMerge.</p>
<p><img src="tikv/./dot/RegionMergeProto.svg" alt="" /></p>
<h2 id="preparemerge"><a class="header" href="#preparemerge">PrepareMerge</a></h2>
<h3 id="source-region-propose-preparemerge"><a class="header" href="#source-region-propose-preparemerge">Source Region: propose PrepareMerge</a></h3>
<p>Source Region leader在leader收到PrepareMerge请求后，会propose 一条PrepareMerge消息。</p>
<p>propose 之前会做一些检查, 最后会设置PrePareMerge中的<code>min_index</code>参数</p>
<p><img src="tikv/./dot/prepare_merge.svg" alt="" /></p>
<p>在ApplyFsm执行PrepareMerge时，region的epoch和<code>conf_version</code>都会+1,
这样PrepareMerge 之后Proposal的log entry 在Apply时都会被skip掉。
所以soure region在propose PreapreMerge 之后，就不可读写了。</p>
<h3 id="source-region-applydelegateexec_prepare_merge"><a class="header" href="#source-region-applydelegateexec_prepare_merge">Source Region <code>ApplyDelegate::exec_prepare_merge</code></a></h3>
<p>将PeerState设置为Merging, 将region epoch的<code>conf_ver</code>和version 都+1</p>
<p><img src="tikv/./dot/exec_prepare_merge.svg" alt="" /></p>
<h3 id="source-region-peerfsmdelegateon_ready_prepare_merge"><a class="header" href="#source-region-peerfsmdelegateon_ready_prepare_merge">Source Region <code>PeerFsmDelegate::on_ready_prepare_merge</code></a></h3>
<p><img src="tikv/./dot/PeerFsmDelegate__on_ready_prepare_merge.svg" alt="" /></p>
<p>source region raft 在收到ExecResult::PreapreMerge消息之后， 会调用<code>on_ready_prepare_merge</code> 处理该消息。
首先设置了<code>pending_merge_state</code>，在此之后，该region raft 对于proposal(RollbackMerge的除外)请求，会返回Error::ProposalInMergeMode.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    fn propose_normal&lt;T&gt;(
        &amp;mut self,
        poll_ctx: &amp;mut PollContext&lt;EK, ER, T&gt;,
        mut req: RaftCmdRequest,
    ) -&gt; Result&lt;Either&lt;u64, u64&gt;&gt; {
        if self.pending_merge_state.is_some()
            &amp;&amp; req.get_admin_request().get_cmd_type() != AdminCmdType::RollbackMerge
        {
            return Err(Error::ProposalInMergingMode(self.region_id));
        }
<span class="boring">}
</span></code></pre></pre>
<p>然调用<code>on_check_merge</code>, 经过一系列检查后， 向本地的target region Propose 一条CommitMergeRequest消息,
CommitMergeRequest 带上了source region一些peer要补齐的log entries.</p>
<p>其中比较重要的方法是<code>Peer::validate_merge_peer</code>, 会检查Source的MergeState 中的target region信息
和当前本地target region信息。如果merge state中的比本地的epoch小，则返回错误。</p>
<p>如果比本地的大，则需要等target region epoch 追上后再<code>schedule_merge</code>,
在下一次check merge tick中接着检查。</p>
<p>向本地target region发送AdminCmdType::CommitMerge类型的RaftCmd.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Please note that, here assumes that the unit of network isolation is store rather than
// peer. So a quorum stores of source region should also be the quorum stores of target
// region. Otherwise we need to enable proposal forwarding.
self.ctx
    .router
    .force_send(
        target_id,
        PeerMsg::RaftCommand(RaftCommand::new(request, Callback::None)),
    )
    .map_err(|_| Error::RegionNotFound(target_id))
<span class="boring">}
</span></code></pre></pre>
<p>处理Schedule Error: RegionNotFound, 以及target region epoch比merge state中的大。</p>
<p><img src="tikv/./dot/PeerFsmDelegate__schedule_merge_error.svg" alt="" /></p>
<h2 id="rollbackmerge"><a class="header" href="#rollbackmerge">RollbackMerge</a></h2>
<p>RollbackMerge执行后，会将<code>pending_merge_state</code>设置为none, 这样
就停止了<code>on_check_merge</code>, 并且<code>propose_normal</code>也可以正常工作了</p>
<p>RollbackMerge会将region epoch的version +1, 然后通过pd hearbeat
上报给pd server.</p>
<p><img src="tikv/./dot/source_region_rollback_merge.svg" alt="" /></p>
<h2 id="commitmerge"><a class="header" href="#commitmerge">CommitMerge</a></h2>
<h3 id="target-region-applydelegateexec_commit_merge"><a class="header" href="#target-region-applydelegateexec_commit_merge">Target Region <code>ApplyDelegate::exec_commit_merge</code></a></h3>
<p>CommitMerge消息由source region 发给本地的target region后，如果本地
的target region是leader， 则会像正常消息一样propose 到raft group,
如果target region不是leader, 则会slient drop掉该消息。</p>
<p>在target节点执行CommitMerge时，会先发送一个CatchUpLogs消息，给本地的source region
让它把日志补齐，CatchUpLogs里面带了一个<code>logs_up_to_date</code>是个AtomicU64.</p>
<p>如果source region补齐了log, 则会设置<code>logs_up_to_date</code>为自己的<code>region_id</code>。</p>
<p><code>ApplyDelegate::wait_merge_state</code> 也引用了<code>logs_up_to_date</code>，每次<code>resume_pending</code>
都会load <code>logs_up_to_date</code>，如果有值，则会继续重新执行<code>exec_commit_merge</code>.</p>
<p>最后返回结果<code>ExecResult::CommitMerge</code></p>
<p><img src="tikv/./dot/ApplyDelegate__exec_commit_merge.svg" alt="" /></p>
<p>等SourceRegion 已经CatchUpLogs后, 会修改atomic <code>logs_up_to_date</code>
从而影响<code>ApplyDelegate::wait_merge_state</code>, 在<code>resume_pending</code>
时重新执行<code>exec_commit_merge</code>。</p>
<p><img src="tikv/./dot/AppDelegate__exec_commit_merge2.svg" alt="" /></p>
<p>这次会将target region的key range扩大, 增加target region的version, 最后调用
<code>write_peer_state</code>将target region信息保存起来。</p>
<h3 id="source-region-peerfsmdelegateon_catch_up_logs_for_merge"><a class="header" href="#source-region-peerfsmdelegateon_catch_up_logs_for_merge">Source Region: <code>PeerFsmDelegate::on_catch_up_logs_for_merge</code></a></h3>
<p>使用CommitMergeRequest中的entries，补齐apply自己本地raft log.
，然后发送LogsUpToDate消息个ApplyFsm。</p>
<p>ApplyFsm中设置atomic 变量<code>CatchUpLogs::logs_up_to_date</code>值为
<code>source_region_id</code>, 然后发Noop消息给target region， 让target region接着处理自己的<code>wait_merge_state</code></p>
<p><img src="tikv/./dot/catchup_logs.svg" alt="" /></p>
<p>在执行<code>on_catch_up_logs_for_merge</code>时，如果<code>pending_merge_state</code>不为None,
说明source region可能已经过PreapreMerge消息了，直接发送<code>LogsUpToDate</code>消息给applyFsm.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn on_catch_up_logs_for_merge(&amp;mut self, mut catch_up_logs: CatchUpLogs) {

if let Some(ref pending_merge_state) = self.fsm.peer.pending_merge_state {
    if pending_merge_state.get_commit() == catch_up_logs.merge.get_commit() {
        assert_eq!(
            pending_merge_state.get_target().get_id(),
            catch_up_logs.target_region_id
        );
        // Indicate that `on_ready_prepare_merge` has already executed.
        // Mark pending_remove because its apply fsm will be destroyed.
        self.fsm.peer.pending_remove = true;
        // Just for saving memory.
        catch_up_logs.merge.clear_entries();
        // Send CatchUpLogs back to destroy source apply fsm,
        // then it will send `Noop` to trigger target apply fsm.
        self.ctx
            .apply_router
            .schedule_task(region_id, ApplyTask::LogsUpToDate(catch_up_logs));
        return;
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>同样在执行<code>on_ready_prepare_merge</code>中如果 <code>Peer.catch_up_logs</code>不为None，说明<code>on_catch_up_logs_for_merge</code>
这个先执行的，此时执行时的是被补齐的log中的PrepareMerge消息。</p>
<p>这时候Log已经补齐了，可以ApplyFsm发送LogsUpToDate消息了。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn on_ready_prepare_merge(&amp;mut self, region: metapb::Region, state: MergeState) {
//...
    if let Some(ref catch_up_logs) = self.fsm.peer.catch_up_logs {
        if state.get_commit() == catch_up_logs.merge.get_commit() {
            assert_eq!(state.get_target().get_id(), catch_up_logs.target_region_id);
            // Indicate that `on_catch_up_logs_for_merge` has already executed.
            // Mark pending_remove because its apply fsm will be destroyed.
            self.fsm.peer.pending_remove = true;
            // Send CatchUpLogs back to destroy source apply fsm,
            // then it will send `Noop` to trigger target apply fsm.
            self.ctx.apply_router.schedule_task(
                self.fsm.region_id(),
                ApplyTask::LogsUpToDate(self.fsm.peer.catch_up_logs.take().unwrap()),
            );
            return;
        }
    }
<span class="boring">}
</span></code></pre></pre>
<h3 id="target-region-peerfsmdelegateon_ready_commit_merge"><a class="header" href="#target-region-peerfsmdelegateon_ready_commit_merge">Target region: <code>PeerFsmDelegate::on_ready_commit_merge</code></a></h3>
<p>target region的PeerFsm 中更新StoreMeta中regions, readers， <code>region_ranges</code>信息，
删除<code>source_region</code>的，更新target region的</p>
<p>然后发送SignificantMsg::MergeResult消息给<code>source_region</code>.</p>
<p><img src="tikv/./dot/PeerFsmDelegate__on_ready_commit_merge.svg" alt="" /></p>
<h3 id="source-region-peerfsmdelegateon_merge_result"><a class="header" href="#source-region-peerfsmdelegateon_merge_result">Source Region: <code>PeerFsmDelegate::on_merge_result</code></a></h3>
<p>destory source regon PeerFsm和ApplyFsm.</p>
<p>如果ApplyFsm还没被注销的话，发送ApplyTask::destory 先destory ApplyFsm.</p>
<p><img src="tikv/./dot/PeerFsmDelegate__on_merge_result.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage-2"><a class="header" href="#storage-2">Storage</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="percolator"><a class="header" href="#percolator">Percolator</a></h1>
<blockquote>
<ul>
<li>分布式事务关键点在于所有的参与者对于事务状态(commit/abort) 达成共识，并且每个参与者保证最终可以完成该共识。</li>
<li><code>2PC</code> 在commit时候，只有coordinator知道事务的状态, 如果coordinator node fail stop，并且唯一收到commit消息的参与者也fail了，新起的coordinator(也无旧的coordinator的事务日志)无法通过询问存活的参与者来推算事务状态。</li>
<li><code>3PC</code> 增加了precommit 阶段, 在所有参与者收到precommit消息后(precommit 相当于告参与者投票结果)，才会进入commit阶，新起的coordinator 可以根据precommit来推算事务的状态。
但是无法解决network partition的问题。</li>
<li><code>Percolator</code> coordinator是无状态的，它将事务信息保存在BigTable中，使用primary key来存储事务状态，并且所有的参与者(secondary keys) 保存了指向primary key的指针，随时可以知道事务的状态。</li>
</ul>
</blockquote>
<!-- toc -->
<h2 id="2pctwo-phase-commit"><a class="header" href="#2pctwo-phase-commit">2PC(Two Phase Commit)</a></h2>
<blockquote>
<p>2PC is an atomic commit protocol meaning all participants will eventually commit if all voted “YES” or leave the system unchanged otherwise.</p>
</blockquote>
<h3 id="2pc-主要思想"><a class="header" href="#2pc-主要思想">2PC 主要思想</a></h3>
<p>2pc 中有两种角色，coordinator(协调者)和participant(参与者)</p>
<p>coordinator在prepare阶段, 先写入begin commit日志，
然后向所有的participant发送prepare消息。如果所有的participant 投票yes, 
则向所有的participant发送commit消息，participant完成commit.</p>
<p><img src="tikv/./dot/2pc.svg" alt="" /></p>
<p>如果在prepare阶段，某个participant投了no, coordinator则需要
向所有的participant发送rollback消息。</p>
<p><img src="tikv/./dot/2pc-rollback.svg" alt="" /></p>
<p>可以看到2PC模型中，事务的一致性依赖于coordinator, 也只有coordinator知道
prepare 阶段所有参与者的投票结果。</p>
<p>coordinator会把事务日志做<em>本地持久化</em>，并保证coordinator从crash恢复后，
重新读取事务日志，获取当前事务状态，然后接着发送commit/rollback 消息，
从而保证事务可以接着一致的执行。</p>
<h3 id="2pc缺陷"><a class="header" href="#2pc缺陷">2PC缺陷</a></h3>
<h4 id="无法处理fail-stop"><a class="header" href="#无法处理fail-stop">无法处理fail stop</a></h4>
<ul>
<li>fail stop Model: node can crash and never recover</li>
<li>fail recover Model: nodes could crash and may at some later date recover from the failure and continue executing. </li>
</ul>
<p>如果coordinator node fail stop了，新选择的coordinator，
没有旧的coordinator的事务日志。也就无法得知事务的状态，无法决定是rollback 还是commit。</p>
<p>新的coordinator可以重新查询让所有的参与者上次的投票，来推算事务的状态。</p>
<p>但是这时候，如果有个参与者挂了，这样新的coordinator无法知道
他之前投的是yes还是no，还是已经将事务commit了， 这样也就无法推算事务状态了。</p>
<p>假定这个挂的参与者已经将事务commit了，新的coordinator就无法决定事务状态为abort.
假定他投的是no, 新的coordinator 就无法决定事务状态为commit。（或者coordinator 只能等到这个挂掉
参与者恢复了才能接着判断事务的状态？)</p>
<p>我个人觉得根源上是coordinator的事务日志没有分布式的持久化?</p>
<h4 id="同步阻塞问题"><a class="header" href="#同步阻塞问题">同步阻塞问题</a></h4>
<ol>
<li>参与者的资源要一直lock，直到收到coordinator 的commit/rollback</li>
<li>如果coordinator和某个参与者都挂了，coordinator 要等到该参与者恢复了，才能判断事务状态.</li>
</ol>
<h2 id="3pc"><a class="header" href="#3pc">3PC</a></h2>
<h3 id="3p主要思想"><a class="header" href="#3p主要思想">3P主要思想</a></h3>
<p>3pc 将2pc的Commit 阶段拆分为PreCommit, Commit两个阶段。加入PreCommit状态[2], 是为了告诉所有的参与者，当前的投票结果。
这样新的coordinator 只要根据precommit这个状态，就能得知上投票结果了。</p>
<blockquote>
<p>The purpose of this phase is to communicate the result of the vote to every replica so that the state of the protocol can be recovered no matter which replica dies.
(if you are tolerating a fixed number f of failures, the co-ordinator can go ahead once it has received f+1 confirmations)</p>
</blockquote>
<p>这样coordinator 节点fail stop之后，新的coordinator询问所有的参与者状态，</p>
<p>如果有参与者没达到preCommit状态，说明之前coordinator还没有commit, 这时候
新的coordinator可以放心的abort事务了。</p>
<p>如果有的参与者已经在commit状态了，说明所有的参与者，应该都收到PreCommit了，
新的coordinator可以决定事务状态为commit.</p>
<p>考虑到2pc的case, 假定有两个参与者p1, p2, p1 挂了，</p>
<ol>
<li>如果p2是precommit或者commit状态，说明p1投的是yes, 并且p1有可能已经将事务commit了，事务状态应该为commit.</li>
<li>如果p2不是precommit，说明p肯定还没收到commit消息，可以安全的abort事务。</li>
</ol>
<h3 id="3pc缺陷"><a class="header" href="#3pc缺陷">3PC缺陷</a></h3>
<p>3PC 多了一轮消息，增加了延迟。 而且无法network partition 问题。</p>
<p>假定有三个参与者p1, p2, p3,
p1, p2处于precommit状态被分到一个一个network里面，p3还未收preCommit消息被分到了另外一个network里面，
这时候原有的coordinator 挂了，</p>
<p>p1, p2选举出coordinator c1, p3选举出coordinator c2。c1 判断事务应该为commit，c2判断事务应该为abort.
这样事务的状态就不一致了。</p>
<h2 id="percolator-主要思想"><a class="header" href="#percolator-主要思想">Percolator 主要思想</a></h2>
<h3 id="coordinator状态保存"><a class="header" href="#coordinator状态保存">coordinator状态保存</a></h3>
<p>Percolator 使用两阶段提交来完成事务，不过coordinator是client，coordinator的相关
状态也存放到bigtable中。这样作为coordinator的client就不用保存什状态了。</p>
<p>每个column 有data, write, lock 几个属性, 其中<code>start_ts</code> 是事务开始时时间戳,
<code>commit_ts</code>为事务提交时候的时间戳。<code>commit_ts</code> &gt; <code>start_ts</code>。
事务读写都会用<code>start_ts</code></p>
<ul>
<li>data  负责保存多版本的value: <code>(key, start_ts) -&gt; value</code></li>
<li>write 负责控制value可见的版本控制: <code>(key, commit_ts) -&gt; write_info</code></li>
<li>lock 表示事务的锁，表示有事务在写key: <code>key -&gt; lock_info</code></li>
</ul>
<h3 id="跨行事务"><a class="header" href="#跨行事务">跨行事务</a></h3>
<p>Percolator从key中任选一个作为<code>primary key</code>，事务的状态完全由<code>primary key</code>来决定。
其他的作为<code>secondary key</code>，<code>secondary key</code>的lock保存了对<code>primary key</code>引用。
事务的<code>primary key</code> 提交成功后，client就认为事务提交成功了，其他的<code>secondary key</code>可以异步提交。</p>
<p>事务在读取或者prewrite 某个key时，如果发现key的lock column 不为空，如果lock类型为secondary的,
则会根据里面保存的引用来找到<code>primary key</code>，使用<code>primay key</code>的write/lock来判断lock是否是stale的。</p>
<p>可以认为<code>primary key</code>的write/lock 为coordinator的状态，其他的<code>secondary key</code> write/lock是participant的状态。</p>
<h3 id="prewrite阶段"><a class="header" href="#prewrite阶段">prewrite阶段</a></h3>
<h3 id="commit-阶段"><a class="header" href="#commit-阶段">commit 阶段</a></h3>
<p>key的提交操作，就在在write中写入事务所做修改的版本(start_ts), 并清除key对应的lock.</p>
<h3 id="client-failure的处理"><a class="header" href="#client-failure的处理">client failure的处理</a></h3>
<p>client failure由读操作处理，假定client正在执行事务t1, client failure有两类，</p>
<p>一类情况是在<b>commit point之前</b>，client在prewrite阶段拿到了primary key和一些secondary key的lock，然后client 挂了，一直没提交。
后续这些key的read操作会检查lock的ttl(如果是secondary key则先根据位置信息找到primary key)，
如果超时了，就会clean up 清理掉lock，然后对事务做<code>rollback</code>。</p>
<p>一类是在<b>commit point 之后</b>，提交primary key成功，但在提交secondary lock时挂了。
这些提交失败的seconary key的读取操作首先定位到primary key, 然后
发现事务是成功的，就会对这些secondary lock做<code>roll forward</code></p>
<h2 id="伪代码"><a class="header" href="#伪代码">伪代码</a></h2>
<pre><code>CF_DEFAULT: (key, start_ts) -&gt; value
CF_LOCK: key -&gt; lock_info
CF_WRITE: (key, commit_ts) -&gt; write_info
</code></pre>
<blockquote>
<p>An instance of RocksDB may have multiple CFs, and each CF is a separated key namespace and has its own LSM-Tree. However different CFs in the same RocksDB instance uses a common WAL, providing the ability to write to different CFs atomically.</p>
</blockquote>
<pre><code>CF_DEFAULT: (key, start_ts) -&gt; value
CF_LOCK: key -&gt; lock_info
CF_WRITE: (key, commit_ts) -&gt; write_info
</code></pre>
<ul>
<li>LockColumn: 事务产生的锁，未提交的事务会写本项，记录primary lock的位置。事务成功提交后，该记录会被清理。记录内容格式</li>
<li>Data Column: 存储实际数据</li>
<li>Write Column: 已提交的数据信息，存储数据所对应的时间戳。</li>
</ul>
<p><img src="tikv/./dot/percolator_struct.svg" alt="" /></p>
<p>MvccReader封装了读操作
MvccTxn 封装了写操作。</p>
<p>An instance of RocksDB may have multiple CFs, and each CF is a separated key namespace and has its own LSM-Tree. However different CFs in the same RocksDB instance uses a common WAL, providing the ability to write to different CFs atomically.</p>
<p>事务lock冲突时候处理：</p>
<p>when a transaction T1 (either reading or writing) finds that a row R1 has a lock which belongs to an earlier transaction T0, T1 doesn’t simply rollback itself immediately. Instead, it checks the state of T0's primary lock.</p>
<ol>
<li>If the primary lock has disappeared and there’s a record data @ T0.start_ts in the write column, it means that T0 has been successfully committed. Then row R1's stale lock can also be committed. Usually we call this rolling forward. After this, the new transaction T1 resumes.</li>
<li>If the primary lock has disappeared with nothing left, it means the transaction has been rolled back. Then row R1's stale lock should also be rolled back. After this, T1 resumes.</li>
<li>If the primary lock exists but it’s too old (we can determine this by saving the wall time to locks), it indicates that the transaction has crashed before being committed or rolled back. Roll back T1 and it will resume.</li>
<li>Otherwise, we consider transaction T0 to be still running. T1 can rollback itself, or try to wait for a while to see whether T0 will be committed before T1.start_ts.</li>
</ol>
<p>memcomparable-encoded key</p>
<ol>
<li>Encode the user key to memcomparable</li>
<li>Bitwise invert the timestamp (an unsigned int64) and encode it into big-endian bytes.</li>
<li>Append the encoded timestamp to the encoded key.</li>
</ol>
<p>如何做rollback的</p>
<p>Doing rollback on a key will leave a Rollback record in CF_WRITE(Percolator’s write column)</p>
<p>Short Value in Write Column</p>
<p>直接把short value写到write column中。</p>
<h2 id="percolator-in-tikv"><a class="header" href="#percolator-in-tikv">Percolator in TiKV</a></h2>
<h3 id="memcomparable-format"><a class="header" href="#memcomparable-format">Memcomparable-format</a></h3>
<p><a href="https://github.com/facebook/mysql-5.6/wiki/MyRocks-record-format#memcomparable-format">MyRocks-record-format</a></p>
<h3 id="async-commit"><a class="header" href="#async-commit">async commit</a></h3>
<blockquote>
<p>判断 Async Commit 事务则需要知道所有 keys 的状态，所以我们需要能从事务的任意一个 key 出发，查询到事务的每一个 key。于是我们做了一点小的修改，保留从 secondary key 到 primary key 指针的同时，在 primary key 的 value 里面存储到到每一个 secondary key 的指针
对于 Async Commit 事务的每一个 key，prewrite 时会计算并在 TiKV 记录这个 key 的 Min Commit TS，事务所有 keys 的 Min Commit TS 的最大值即为这个事务的 Commit TS。</p>
</blockquote>
<h3 id="一阶段提交1pc"><a class="header" href="#一阶段提交1pc">一阶段提交(1pc)</a></h3>
<p>一阶段提交没有使用分布式提交协议，减少了写 TiKV 的次数。所以如果事务只涉及一个 Region，使用一阶段提交不仅可以降低事务延迟，还可以提升吞吐。4</p>
<h2 id="参考文献-15"><a class="header" href="#参考文献-15">参考文献</a></h2>
<h3 id="percolator-相关"><a class="header" href="#percolator-相关">Percolator 相关</a></h3>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/53197633">Google Percolator事务</a></li>
</ul>
<h3 id="pincap相关文档"><a class="header" href="#pincap相关文档">PinCap相关文档</a></h3>
<ul>
<li><a href="https://tikv.org/deep-dive/distributed-transaction/percolator/">TiKV Percolator</a></li>
<li><a href="https://mp.weixin.qq.com/s/m-22HHO3-9uyRxCfNNHLjA">Async commit</a></li>
<li><a href="https://github.com/tikv/sig-transaction/tree/master/design/async-commit">Async commit 设计文档</a></li>
<li><a href="https://pingcap.com/blog-cn/percolator-and-txn/">Percolator 和 TiDB 事务算法</a></li>
<li><a href="https://pingcap.com/blog-cn/pessimistic-transaction-the-new-features-of-tidb/">TiDB 新特性漫谈：悲观事务</a></li>
</ul>
<h3 id="two-phase-commit-1"><a class="header" href="#two-phase-commit-1">two phase commit</a></h3>
<ul>
<li><a href="https://tikv.org/deep-dive/distributed-transaction/distributed-algorithms/#two-phase-commit">Two phase commit</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/35298019">漫话分布式系统共识协议: 2PC/3PC篇</a></li>
<li><a href="https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/">Consensus Protocols: Two-Phase Commit</a></li>
<li><a href="https://www.the-paper-trail.org/post/2008-11-29-consensus-protocols-three-phase-commit/">Consensus Protocols: Three-phase Commit</a></li>
<li><a href="https://medium.com/@balrajasubbiah/consensus-two-phase-and-three-phase-commits-4e35c1a435ac">Consensus, Two Phase and Three Phase Commits</a></li>
<li><a href="https://exactly-once.github.io/posts/notes-on-2pc/">Notes on 2PC</a></li>
<li><a href="https://www.cs.rutgers.edu/%7Ebadri/553dir/notes/W9B-four.pdf">Sinfonia: A new Paradigm for Building scalable Distributed System</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="percolator-in-tikv-1"><a class="header" href="#percolator-in-tikv-1">Percolator in TiKV</a></h1>
<!-- toc -->
<h2 id="data--struct"><a class="header" href="#data--struct">data  struct</a></h2>
<p>在TiKV中，data/lock/write这些信息会写入不同的Column Family 中，由于Rocksdb 不同的column faimly 共享一个WAL,
所以不同CF的写入是原子性的。</p>
<ol>
<li>Data 信息写入<code>CF_DEFAULT</code>, key 为<code>raw_key start_ts</code>, 值为要写入的数据</li>
<li>Write 信息会写入<code>CF_WRITE</code>, key为<code>raw_key commit_ts</code>, 注意<b>Rollback类型的Write</b> 写入的key 为<b><code>raw_key start_ts</code></b>，值为Write, 使用WriteRef::tobytes序列化，WriteRef::parse反序列化。</li>
<li>Lock 信息会写入<code>CF_LOCK</code>, key 为<code>raw_key</code>, 值为LockInfo, 使用Lock::tobytes序列化，Lock::parse反序列化。</li>
</ol>
<p><img src="tikv/./dot/txn_types.svg" alt="" /></p>
<h2 id="mvcctxn"><a class="header" href="#mvcctxn">MvccTxn</a></h2>
<h3 id="put_lock"><a class="header" href="#put_lock"><code>put_lock</code></a></h3>
<p>加锁操作, 其中只有PrewriteMutation中是新创建lock的。</p>
<p><code>check_txn_status_lock_exists</code> 更新lock的<code>min_commit_ts</code></p>
<p><img src="tikv/./dot/txn_put_lock.svg" alt="" /></p>
<p><code>mark_rollback_on_mismatching_lock</code> 将事务的start_ts加入到
lock的 <code>rollback_ts</code> vec，该字段说明如下</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// In some rare cases, a protected rollback may happen when there's already another
// transaction's lock on the key. In this case, if the other transaction uses calculated
// timestamp as commit_ts, the protected rollback record may be overwritten. Checking Write CF
// while committing is relatively expensive. So the solution is putting the ts of the rollback
// to the lock.
pub rollback_ts: Vec&lt;TimeStamp&gt;,
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/lock_rollback_ts.svg" alt="" /></p>
<h3 id="unlock_key"><a class="header" href="#unlock_key"><code>unlock_key</code></a></h3>
<p><img src="tikv/./dot/unlock_key.svg" alt="" /></p>
<h3 id="put_write"><a class="header" href="#put_write"><code>put_write</code></a></h3>
<p>在commit或者rollback时，会创建write record，
commit时，保存的write record用的是<code>key commit_ts</code>
其中<code>commit_ts</code>,是事务提交的ts.</p>
<p>rollback时候，会创建一个WriteType::Rollback的WriteRecord。
对应的key是<code>key start_ts</code>, 其中<code>start_ts</code>是事务的自身的<code>start_ts</code></p>
<p><img src="tikv/./dot/txn_put_write.svg" alt="" /></p>
<h3 id="delete_write"><a class="header" href="#delete_write"><code>delete_write</code></a></h3>
<p><img src="tikv/./dot/txn_delete_write.svg" alt="" /></p>
<h3 id="modifies"><a class="header" href="#modifies">modifies</a></h3>
<p>由MvccTxn负责data/lock/write的写入, 会先将将改动保存在<code>MvccTxn::modifies</code> vec中。</p>
<p><img src="tikv/./dot/mvcc_txn_fn.svg" alt="" /></p>
<h3 id="modifiers-后续处理"><a class="header" href="#modifiers-后续处理">modifiers 后续处理</a></h3>
<p>modifiers会转换为WriteData, 然后放到WriteResult中，由<code>Schedule::process_write</code>负责将WriteResult异步
保存起来。</p>
<p><img src="tikv/./dot/mvcc_txn_modifiers3.svg" alt="" /></p>
<h3 id="writeresult-保存"><a class="header" href="#writeresult-保存">WriteResult 保存</a></h3>
<p><img src="tikv/./dot/Scheduler__process_write.svg" alt="" /></p>
<h2 id="mvccreader"><a class="header" href="#mvccreader">MvccReader</a></h2>
<p>在执行txn事务cmd时，由MvccReader的<code>load_lock</code>, <code>load_data</code>和<code>seek_write</code> 负责读取相应数据。</p>
<p><img src="tikv/./dot/mvcc_reader2.svg" alt="" /></p>
<h3 id="seek_write"><a class="header" href="#seek_write"><code>seek_write</code></a></h3>
<p>主要的方法为<code>seek_write</code>, 如果事务T(假设它ts为<code>start_ts</code>, 要读key的数据，首先要<code>seek_write</code>,
找到距离<code>start_ts</code> 最近的commit record。</p>
<p>然后使用它的<code>Write.start_ts</code> 去Data column中读取数据，或者对于short value, TiKV做了
一个优化，<code>short_value</code>直接保存在了Write中，直接返回<code>Write.short_value</code>就行了. 省去了一次读数据。</p>
<p><code>MvccReader::seek_write</code>和<code>MvccReader::load_data</code>的实现，体现了Percolator的思想，即使用Write column 来
控制事务所写数据的可见性，以及<code>start_ts</code>和<code>commit_ts</code>的作用。</p>
<p><img src="tikv/./dot/mvcc_reader_fn2.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="2pc"><a class="header" href="#2pc">2pc</a></h1>
<!-- toc -->
<h2 id="数据流程"><a class="header" href="#数据流程">数据流程</a></h2>
<p>TiDB中乐观事务提交流程如下(摘自<a href="https://pingcap.com/blog-cn/pessimistic-transaction-the-new-features-of-tidb/">TiDB 新特性漫谈：悲观事务</a>)</p>
<p><img src="tikv/./dot/Optimistic_pecolator.png" alt="" /></p>
<ol>
<li>首先Begin 操作会去TSO服务获取一个timestamp，作为事务的<code>startTS</code>，同时startTs也是事务的唯一标识。</li>
<li>DML阶段先KVTxn将写(Set, Delete)操作保存在<code>MemDB</code>中。</li>
<li>2PC提交阶段在<code>KVTxn::Commit</code>时创建<code>twoPhaseCommitter</code>, 并调用它的<code>initKeysAndMutations</code>
遍历<code>MemDB</code>, 初始化<code>memBufferMutations</code>.</li>
<li>在<code>twoPhaseCommitter::execute</code>中，首先对<code>memBufferMutations</code>先按照region做分组，然后每个分组内，按照size limit分批。</li>
<li>每批mutations，调用对应的action的<code>handleSignleBatch</code>，发送相应命令到TiKV.</li>
</ol>
<p><img src="tikv/./dot/batch_mutation.svg" alt="" /></p>
<h2 id="begin-transaction"><a class="header" href="#begin-transaction">Begin Transaction</a></h2>
<p>每个client connection 对应着一个session, 事务相关数据的放在了session中，
它包含了对kv.Storage和Txn接口的引用。</p>
<p><code>kv.Storage</code>接口定义了<code>Begin/BeginWithOption</code>接口，用来创建开始一个事务，它
主要实现者为<code>KVStore</code>。</p>
<p><code>kv.Transaction</code>定义了事务的接口，txn可以commit/rollback.
它主要实现者为<code>KVTxn</code>。</p>
<p>每个KVTxn有个对MemDB的引用，每个事务的set/delete等修改会先存放到<code>MemDB</code>中。</p>
<p><img src="tikv/./dot/tidb_session.svg" alt="" /></p>
<p>kv.Storage的Begin/BeginWithOption 调用图如下：如果startTS为nil, 则会去TOS(timestamp oracle service)也就是
PD服务获取一个时间戳，作为事务的startTS，同时也是事务的唯一标识。</p>
<p><img src="tikv/./dot/tikvstore_start_ts.svg" alt="" /></p>
<h2 id="数据dml-先保存到txn的memdb"><a class="header" href="#数据dml-先保存到txn的memdb">数据DML: 先保存到txn的MemDB</a></h2>
<p>table row的增删改，最终会调用<code>Table</code>的AddRecord, RemoveRecord, UpdateRecord接口来更新数据。</p>
<p><img src="tikv/./dot/tidb_dml_insert.svg" alt="" /></p>
<p>而Table的这些接口，会将改动保存在Txn.KVUnionStore.MemDB中。</p>
<p><img src="tikv/./dot/tidb_txn_dml.svg" alt="" /></p>
<h2 id="twophasecommitter"><a class="header" href="#twophasecommitter">twoPhaseCommitter</a></h2>
<p>像pecolator论文中描述的协议一样，两阶段提交步骤如下：</p>
<ol>
<li>先Prewrite获取Lock, TiDB中可以并发的发起Prewrite请求.</li>
<li>去TSO 服务获取commit ts, 保证获取的<code>commit_ts</code>比之前的事务的<code>start_ts</code>都大。</li>
<li>commit primary key, 提交完primary key后，就可以返回给client，事务提交成功了。</li>
<li>其它剩下的keys由go routine在后台异步提交。</li>
</ol>
<p>下图摘自[Async Commit 原理介绍][async-commit]</p>
<p><img src="tikv/./dot/tidb_2pc_normal.png" alt="" /></p>
<p>TiDB中会先根据region对MemDB中的keys做分组，然后每个分组内做分批，最后一批一批的向TiKV发请求。</p>
<h3 id="mutations"><a class="header" href="#mutations">mutations</a></h3>
<p>上面保存在txn的MemDB中的修改，在txn commit时，会被转变为<code>twoPhaseCommitter::mutations</code>，在两阶段提交的
Prewrite/Commit阶段会提交这些<code>mutations</code>.</p>
<p><img src="tikv/./dot/KVTxn__Commit.svg" alt="" /></p>
<h3 id="doactiononmutations"><a class="header" href="#doactiononmutations">doActionOnMutations</a></h3>
<pre><code class="language-go">// doActionOnMutations groups keys into primary batch and secondary batches, if primary batch exists in the key,
// it does action on primary batch first, then on secondary batches. If action is commit, secondary batches
// is done in background goroutine.
</code></pre>
<p>先调用groupMutations, 将mutations按照region分组，然后<code>doActionOnGroupMutations</code>对每个group分别做处理。</p>
<p><img src="tikv/./dot/doActionOnMuations.svg" alt="" /></p>
<h3 id="groupmutations-按照region分组"><a class="header" href="#groupmutations-按照region分组">groupMutations: 按照region分组</a></h3>
<p>先对mutations按照region分组，如果某个region的mutations 太多。
则会先发送CmdSplitRegion命令给TiKV, TiKV对那个region先做个split, 然后再开始提交，
这样避免对单个region too much write workload, 避免了不必要的重试。</p>
<p><img src="tikv/./dot/tidb_groupmutations.svg" alt="" /></p>
<h3 id="doactionongroupmutations-分批"><a class="header" href="#doactionongroupmutations-分批">doActionOnGroupMutations: 分批</a></h3>
<p>doActionOnGroupMutations 会对每个group的mutations 做进一步的分批处理。
对于actionCommit做了特殊处理，如果是NormalCommit, primay Batch要先提交，
然后其他的batch可以新起一个go routine在后台异步提交。</p>
<p><img src="tikv/./dot/tidb_doActionOnGroupMutations.svg" alt="" /></p>
<p>关键代码如下：</p>
<pre><code class="language-go">func (c *twoPhaseCommitter) doActionOnGroupMutations(bo *Backoffer, action twoPhaseCommitAction, groups []groupedMutations) error {
  // 1.每个分组内的再分批
	for _, group := range groups {
		batchBuilder.appendBatchMutationsBySize(group.region, group.mutations, sizeFunc, txnCommitBatchSize)
  }

  //2.commit先同步的提交primary key所在的batch
	if firstIsPrimary &amp;&amp;
		((actionIsCommit &amp;&amp; !c.isAsyncCommit()) || actionIsCleanup || actionIsPessimiticLock) {
		// primary should be committed(not async commit)/cleanup/pessimistically locked first
		err = c.doActionOnBatches(bo, action, batchBuilder.primaryBatch())
    //...
		batchBuilder.forgetPrimary()
	}
  //...

  //3. 其它的key由go routine后台异步的提交
	// Already spawned a goroutine for async commit transaction.
	if actionIsCommit &amp;&amp; !actionCommit.retry &amp;&amp; !c.isAsyncCommit() {
    //..
		go func() {
      //其它的action异步提交
			e := c.doActionOnBatches(secondaryBo, action, batchBuilder.allBatches())
    }
  }else {
		err = c.doActionOnBatches(bo, action, batchBuilder.allBatches())
  }
//...
</code></pre>
<h3 id="doactiononbatches-并发的处理batches"><a class="header" href="#doactiononbatches-并发的处理batches">doActionOnBatches: 并发的处理batches</a></h3>
<p><code>batchExecutor::process</code> 每个batch会启动一个go routine来并发的处理,
并通过channel等待batch的处理结果。当所有batch处理完了，再返回给调用者。</p>
<p>其中会使用令牌做并发控制, 启动goroutine前先去获取token, goroutine运行
完毕，归还token。</p>
<p><img src="tikv/./dot/tidb_doActionOnBatches.svg" alt="" /></p>
<h2 id="actionprewrite"><a class="header" href="#actionprewrite">actionPrewrite</a></h2>
<p>发送prewrite命令到TiKV, 如果prewrite阶段，遇到了lock error，
则尝试Resole lock， 然后重试；如果遇到了regionError， 则需要重新
调用doActionONMutations,重新分组，重新尝试。</p>
<p>如果没有keyError，并且Batch是primary. 则启动一个tllManager，给txn的
primary lock续命，ttlManager会定期的向TiKV发送txnHeartbeat, 更新primary lock的ttl。</p>
<p><img src="tikv/./dot/actionPrewrite_handleSingleBatch.svg" alt="" /></p>
<h3 id="tikv端处理prewrite"><a class="header" href="#tikv端处理prewrite">TiKV端处理Prewrite</a></h3>
<p>TiKV端PreWriteKind，分为悲观事务和乐观事务。</p>
<p><img src="tikv/./dot/Prewrite__process_write.svg" alt="" /></p>
<p>对单个key Muation的prewrite操作。</p>
<p><img src="tikv/./dot/tikv_prewrite2.svg" alt="" /></p>
<p>constraint check</p>
<p>should not write</p>
<p>PrewriteMutation</p>
<h3 id="tikv端处理txnheartbeat"><a class="header" href="#tikv端处理txnheartbeat">TiKV端处理TxnHeartBeat</a></h3>
<p>直接更新primary key lock的ttl.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//txn_heart_beat.rs
impl&lt;S: Snapshot, L: LockManager&gt; WriteCommand&lt;S, L&gt; for TxnHeartBeat {
    fn process_write(self, snapshot: S, context: WriteContext&lt;'_, L&gt;) -&gt; Result&lt;WriteResult&gt; {
    //...
    let lock = match reader.load_lock(&amp;self.primary_key)? {
        Some(mut lock) if lock.ts == self.start_ts =&gt; {
           if lock.ttl &lt; self.advise_ttl {
              lock.ttl = self.advise_ttl;
              txn.put_lock(self.primary_key.clone(), &amp;lock);
            }
            lock
        }
<span class="boring">}
</span></code></pre></pre>
<h2 id="actioncommit"><a class="header" href="#actioncommit">actionCommit</a></h2>
<p>TiDB向Tikv发起commit请求，CommitRequest中的Keys即为要提交的key.</p>
<p><img src="tikv/./dot/actionCommit_handleSingleBatch.svg" alt="" /></p>
<h3 id="tikv端处理commit"><a class="header" href="#tikv端处理commit">TiKV端处理commit</a></h3>
<p>TiKV会遍历Commit请求中的每个key, 尝试去commit key, 然后调用ReleasedLocks唤醒等待这些key的事务。</p>
<p><img src="tikv/./dot/Commit__process_write2.svg" alt="" /></p>
<p>单个key的commit过程如下, 分两种case:</p>
<ol>
<li><code>lock match</code>:  lock仍然被txn 所持有，则继续尝试提交, 提交如果commit_ts &lt; lock.min_commit_ts则报错，
<code>ErrorInner::CommitTsExpired</code>，如果lock.rollback_ts中有和commit_ts相同的ts, 则需要将
要写入的write.set_overlapped_rollback。最后unlock key, 提交write。</li>
<li><code>lock mismatch</code>: lock为None或者Lock已经被其他事务所持有，则需要<code>get_txn_commit_record</code>
读取commit record来判断事务的commit状态.</li>
</ol>
<p><img src="tikv/./dot/tikv_ResolveLock_commit.svg" alt="" /></p>
<h2 id="参考文献-16"><a class="header" href="#参考文献-16">参考文献</a></h2>
<p>1 <a href="https://pingcap.com/blog-cn/pessimistic-transaction-the-new-features-of-tidb/">TiDB 新特性漫谈：悲观事务</a></p>
<h1 id="draft-4"><a class="header" href="#draft-4">Draft</a></h1>
<h2 id="事务startts"><a class="header" href="#事务startts">事务startTS</a></h2>
<p>在执行start transaction时，会去TimmStamp Oracle服务获取时间戳，作为事务的startTS,
startTs会保存在TransactionContext中
startTS 是单调递增的，这样startT标识事务, 也可以用来表示事务之间的先后关系。</p>
<p><img src="tikv/./dot/txn_startTS.svg" alt="" /></p>
<p><img src="tikv/./dot/txn_start_ts.svg" alt="" /></p>
<p>在TiDB中，对应流程如下:</p>
<p><img src="tikv/./dot/twoPhaseCommitter_execute.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asynccommit"><a class="header" href="#asynccommit">AsyncCommit</a></h1>
<!-- toc -->
<p>AsyncCommit 等所有的key prewrite之后，就算成功了，TiDB即可返回告诉client事务提交成功了。
primary key 可以异步的commit.其流程如下(摘自<a href="https://pingcap.com/zh/blog/async-commit-principle">Async Commit 原理介绍</a>)</p>
<p><img src="tikv/./dot/tidb_async_commit.png" alt="" /></p>
<p>好处是在prewrite结束后，就可以返回结果给client, commit由tidb在后台异步提交，降低了事务的延迟。</p>
<p>需要解决的主要有两个：</p>
<ol>
<li>从如何确定所有 keys 已被prewrite，需要根据primary key找到所有的secondary keys.</li>
<li>如果确定<code>commit_ts</code></li>
</ol>
<p>对于问题1，primarylock中增加了<code>pub secondaries: Vec&lt;Vec&lt;u8&gt;&gt;</code>字段。</p>
<p>lock 包含了txn涉及到的所有的 secondaries keys</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(PartialEq, Clone)]
pub struct Lock {
    pub lock_type: LockType,
    pub primary: Vec&lt;u8&gt;,
    pub ts: TimeStamp,
    pub ttl: u64,
    pub short_value: Option&lt;Value&gt;,
    // If for_update_ts != 0, this lock belongs to a pessimistic transaction
    pub for_update_ts: TimeStamp,
    pub txn_size: u64,
    pub min_commit_ts: TimeStamp,
    pub use_async_commit: bool,
    // Only valid when `use_async_commit` is true, and the lock is primary. Do not set
    // `secondaries` for secondaries.
    pub secondaries: Vec&lt;Vec&lt;u8&gt;&gt;,
    // In some rare cases, a protected rollback may happen when there's already another
    // transaction's lock on the key. In this case, if the other transaction uses calculated
    // timestamp as commit_ts, the protected rollback record may be overwritten. Checking Write CF
    // while committing is relatively expensive. So the solution is putting the ts of the rollback
    // to the lock.
    pub rollback_ts: Vec&lt;TimeStamp&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>问题2，则使用每个key的<code>min_commit_ts</code>和TiKV的<code>max_ts</code>来确定事务的<code>commit_ts</code> TiDB的每次读都会更新Tikv的<code>max_ts</code>。</p>
<blockquote>
<p>对于 Async Commit 事务的每一个 key，prewrite 时会计算并在 TiKV 记录这个 key 的 Min Commit TS，事务所有 keys 的 Min Commit TS 的最大值即为这个事务的 Commit TS。</p>
</blockquote>
<h2 id="checkasynccommit"><a class="header" href="#checkasynccommit">checkAsyncCommit</a></h2>
<p>先关配置在Config.TiKVClient.AsyncCommit中, checkAsyncCommit 会遍历mutations
计算事务的total key size是否超过了限制。 最后结果保存在atomic变量<code>useAsyncCommit</code>中。</p>
<p>相关配置项如下：</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type AsyncCommit struct {
	// Use async commit only if the number of keys does not exceed KeysLimit.
	KeysLimit uint `toml:&quot;keys-limit&quot; json:&quot;keys-limit&quot;`
	// Use async commit only if the total size of keys does not exceed TotalKeySizeLimit.
	TotalKeySizeLimit uint64 `toml:&quot;total-key-size-limit&quot; json:&quot;total-key-size-limit&quot;`
	// The duration within which is safe for async commit or 1PC to commit with an old schema.
	// The following two fields should NOT be modified in most cases. If both async commit
	// and 1PC are disabled in the whole cluster, they can be set to zero to avoid waiting in DDLs.
	SafeWindow time.Duration `toml:&quot;safe-window&quot; json:&quot;safe-window&quot;`
	// The duration in addition to SafeWindow to make DDL safe.
	AllowedClockDrift time.Duration `toml:&quot;allowed-clock-drift&quot; json:&quot;allowed-clock-drift&quot;`
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/checkAsyncCommit.svg" alt="" /></p>
<h2 id="isasynccommit-caller"><a class="header" href="#isasynccommit-caller">isAsyncCommit caller</a></h2>
<p>对<code>isAsyncCommit</code>的调用, 最主要的有两个地方</p>
<ol>
<li><b>prewrite阶段</b>，是buildPrewriteRequest时，需要遍历事务的 mutations将所有的secondaries lock keys 放到request中</li>
<li><b>commit阶段</b>，commit时，启动一个go routine 异步提交，这样prewrite成功后，就可以向client返回事务结果，
不必向正常commit时等到primary key，提交成功才返回结果给client.</li>
</ol>
<p><img src="tikv/./dot/ref_isAsyncCommit.svg" alt="" /></p>
<h2 id="mincommitts"><a class="header" href="#mincommitts">minCommitTS</a></h2>
<blockquote>
<p>对于 Async Commit 事务的每一个 key，prewrite 时会计算并在 TiKV 记录这个 key 的 Min Commit TS，事务所有 keys 的 Min Commit TS 的最大值即为这个事务的 Commit TS。</p>
</blockquote>
<h3 id="client端更新min-commit-ts"><a class="header" href="#client端更新min-commit-ts">client端更新min commit ts</a></h3>
<p>minCommitTS更新逻辑如下，twoPhaseCommitter有个成员变量minCommitTS，记录事务的最小CommitTS.
每次prewrite request会带上该minCommitTS, 并且如果prewrite resp返回的minCommitTS比自己的大，
则更新twoPhaseCommitter的<code>minCOmmitTS</code></p>
<p>这样能保证所有prewrite 请求处理完后，twoPhaseCommitter的<code>minCommitTS</code>是所有key lock的minCommitTS
中最大的。</p>
<p>在后面resolve async commit lock中，也要遍历所有的lock的minCommitTS, 来确定最后的minCommitTS.</p>
<p><img src="tikv/./dot/client_minCommitTS.svg" alt="" /></p>
<ol>
<li>PreWrite前从TSO获取ts, 更新成员变量<code>minCommitTS</code></li>
</ol>
<pre><code class="language-go">func (c *twoPhaseCommitter) execute(ctx context.Context) (err error) {
//...
	if commitTSMayBeCalculated &amp;&amp; c.needLinearizability() {
		latestTS, err := c.store.oracle.GetTimestamp(ctx, &amp;oracle.Option{TxnScope: oracle.GlobalTxnScope})
    //...
		// Plus 1 to avoid producing the same commit TS with previously committed transactions
		c.minCommitTS = latestTS + 1
	}
//...
}
</code></pre>
<ol start="2">
<li>TiDB发送给TiKV的prewrite请求中带上minCommitTS，它收到c.minCommitTS, c.StartTS, c.forUpdateTS影响。</li>
</ol>
<pre><code class="language-go">func (c *twoPhaseCommitter) buildPrewriteRequest(batch batchMutations, txnSize uint64) *tikvrpc.Request {
 //...
	c.mu.Lock()
	minCommitTS := c.minCommitTS
	c.mu.Unlock()
	if c.forUpdateTS &gt; 0 &amp;&amp; c.forUpdateTS &gt;= minCommitTS {
		minCommitTS = c.forUpdateTS + 1
	} else if c.startTS &gt;= minCommitTS {
		minCommitTS = c.startTS + 1
	}
  //...
</code></pre>
<ol start="3">
<li>TiKV端根据maxTS,请求中的minCommitTS, forUpdateTs计算出最终MinCommitTS，并保存在<code>lock.min_commit_ts</code>字段中，
然后在prewriteResp.minCommitTS给TiDB client, TiDB client更新twoPhaseCommitter的minCommitTs.</li>
</ol>
<pre><code class="language-go">func (action actionPrewrite) handleSingleBatch(c *twoPhaseCommitter, bo *Backoffer, batch batchMutations) error {
//...
			if c.isAsyncCommit() {
				if prewriteResp.MinCommitTs == 0 {
        // fallback到normal commit
        }else {
					c.mu.Lock()
					if prewriteResp.MinCommitTs &gt; c.minCommitTS {
						c.minCommitTS = prewriteResp.MinCommitTs
					}
					c.mu.Unlock()
        }
</code></pre>
<h3 id="tikv端计算min-commit-ts"><a class="header" href="#tikv端计算min-commit-ts">TiKV端计算min commit ts</a></h3>
<p>每次TiDB的prewrite请求，TiKV都会返回一个minCommitTS, minCommitTS流程如下</p>
<p><img src="tikv/./dot/tikv_async_commit_min_commit_ts.svg" alt="" /></p>
<p>关键函数在<code>async_commit_timestamps</code>， 这个地方为什么要lock_key ?</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// The final_min_commit_ts will be calculated if either async commit or 1PC is enabled.
// It's allowed to enable 1PC without enabling async commit.
fn async_commit_timestamps(/*...*/) -&gt; Result&lt;TimeStamp&gt; {
    // This operation should not block because the latch makes sure only one thread
    // is operating on this key.
    let key_guard = CONCURRENCY_MANAGER_LOCK_DURATION_HISTOGRAM.observe_closure_duration(|| {
        ::futures_executor::block_on(txn.concurrency_manager.lock_key(key))
    });

    let final_min_commit_ts = key_guard.with_lock(|l| {
        let max_ts = txn.concurrency_manager.max_ts();
        fail_point!(&quot;before-set-lock-in-memory&quot;);
        let min_commit_ts = cmp::max(cmp::max(max_ts, start_ts), for_update_ts).next();
        let min_commit_ts = cmp::max(lock.min_commit_ts, min_commit_ts);

        lock.min_commit_ts = min_commit_ts;
        *l = Some(lock.clone());
        Ok(min_commit_ts)
    }
    ...
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="tikv-maxts"><a class="header" href="#tikv-maxts">TiKV MaxTS</a></h2>
<blockquote>
<p>TiDB 的每一次快照读都会更新 TiKV 上的 Max TS。Prewrite 时，Min Commit TS 会被要求至少比当前的 Max TS 大，也就是比所有先前的快照读的时间戳大，所以可以取 Max TS + 1 作为 Min Commit TS</p>
</blockquote>
<p>每次读操作，都会更新<code>concurrency_manager.max_ts</code></p>
<p><img src="tikv/./dot/tikv_update_max_ts.svg" alt="" /></p>
<p>值得注意的是replica read 也会更新max_ts。replica reader 在read之前会发readIndex消息给leader</p>
<h2 id="因果一致性"><a class="header" href="#因果一致性">因果一致性</a></h2>
<blockquote>
<p>循序性要求逻辑上发生的顺序不能违反物理上的先后顺序。具体地说，有两个事务 T1 和 T2，如果在 T1 提交后，T2 才开始提交，那么逻辑上 T1 的提交就应该发生在 T2 之前，也就是说 T1 的 Commit TS 应该小于 T2 的 Commit TS。 3</p>
</blockquote>
<blockquote>
<p>为了保证这个特性，TiDB 会在 prewrite 之前向 PD TSO 获取一个时间戳作为 Min Commit TS 的最小约束。由于前面实时性的保证，T2 在 prewrite 前获取的这个时间戳必定大于等于 T1 的 Commit TS，而这个时间戳也不会用于更新 Max TS，所以也不可能发生等于的情况。综上我们可以保证 T2 的 Commit TS 大于 T1 的 Commit TS，即满足了循序性的要求。</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="onepc一阶段提交"><a class="header" href="#onepc一阶段提交">OnePC(一阶段提交)</a></h1>
<p><img src="tikv/./dot/one-pc.png" alt="" /></p>
<p>只涉及一个region，且一个batch就能完成的事务，不使用分布式提交协议，只使用一阶段完成事务，
和AsyncCommit相比， 省掉了后面的commit步骤。</p>
<p><img src="tikv/./dot/twoPhaseCommitter_one_pc_execute.svg" alt="" /></p>
<p>对于batchCount &gt; 1的事务不会使用OnePC.</p>
<pre><code class="language-go">func (c *twoPhaseCommitter) checkOnePCFallBack(action twoPhaseCommitAction, batchCount int) {
	if _, ok := action.(actionPrewrite); ok {
		if batchCount &gt; 1 {
			c.setOnePC(false)
		}
	}
}
</code></pre>
<h4 id="tikv端-处理onepc"><a class="header" href="#tikv端-处理onepc">Tikv端 处理OnePC</a></h4>
<p>在TiKV端，OnePC 直接向Write Column 写write record, 提交事务，
省掉了写lock, 以及后续commit时候cleanup lock这些操作了。</p>
<p><img src="tikv/./dot/tikv_one_pc.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="悲观事务-1"><a class="header" href="#悲观事务-1">悲观事务</a></h1>
<!-- toc -->
<h2 id="数据流程-1"><a class="header" href="#数据流程-1">数据流程</a></h2>
<p>悲观事务将上锁时机从prewrite阶段提前到进行DML阶段,先acquire pessimistic lock，
此时并不会写value. 只是写入一个类型为Pessimistic 的lock 占位。</p>
<p><img src="tikv/./dot/pessimistic-lock1.png" alt="" /></p>
<p>在2pc commit阶段，先将lock类型改写为乐观锁，然后再commit</p>
<p><img src="tikv/./dot/pessimistic-lock2.png" alt="" /></p>
<p>上图中描述的代码调用流程如下:</p>
<p><img src="tikv/./dot/LockType__Pessimistic.svg" alt="" /></p>
<h2 id="lockkeys"><a class="header" href="#lockkeys">LockKeys</a></h2>
<p>悲观锁不包含数据，只有锁，只用于防止其他事务修改相同的 Key，不会阻塞读，但 Prewrite 后会阻塞读（和 Percolator 相同，但有了大事务支持后将不会阻塞 
(摘自[TiDB in Action, 6.2 悲观事务][3])</p>
<p>调用流程类似于上面的，也是先对mutation按照region分组，然后每个组内分批。</p>
<p><img src="tikv/./dot/KvTxn_LockKeys.svg" alt="" /></p>
<h3 id="client-acquirepessimisticlock"><a class="header" href="#client-acquirepessimisticlock">Client: AcquirePessimisticLock</a></h3>
<p>这个地方有LockWaitTime, 如果有key 冲突，TiKV会等待一段时间, 或者
等key 的lock被释放了，才会返回给TiDB key writeConflict或者deadlock</p>
<p><img src="tikv/./dot/tidb_actionPessimisticLock_handleSingleBatch.svg" alt="" /></p>
<p>LockKeys中对于<code>ErrDeadlock</code>特殊处理，等待已经lock的key都被rollback之后并且sleep 5ms, 才会向上返回。</p>
<p><img src="tikv/./dot/tidb_pessimitisticlock_handle_err.svg" alt="" /></p>
<p>悲观事务对于<code>ErrDeadlock</code>和<code>ErrWriteConflict</code>重试，重新创建executor, 重试statementContext 状态，更新ForUpdateTS。</p>
<p><img src="tikv/./dot/handle_pessimistic_dml.svg" alt="" /></p>
<p>做selectForUpdate 做了特殊处理，没看明白没什么要这么干。</p>
<h3 id="tikv-acquirepessimisticlock"><a class="header" href="#tikv-acquirepessimisticlock">TiKV: AcquirePessimisticLock</a></h3>
<p>TiKV端获取Pessimistic处理方法(摘自[TiDB 悲观锁实现原理][1])</p>
<ul>
<li>检查 TiKV 中锁情况，如果发现有锁
<ol>
<li>不是当前同一事务的锁，返回 KeyIsLocked Error</li>
<li>锁的类型不是悲观锁，返回锁类型不匹配（意味该请求已经超时）</li>
<li>如果发现 TiKV 里锁的 for_update_ts 小于当前请求的 for_update_ts(同一个事务重复更新)， 使用当前请求的 for_update_ts 更新该锁</li>
<li>其他情况，为重复请求，直接返回成功</li>
</ol>
</li>
<li>检查是否存在更新的写入版本，如果有写入记录
<ol>
<li>若已提交的 commit_ts 比当前的 for_update_ts 更新，说明存在冲突，返回 WriteConflict Error</li>
<li>如果已提交的数据是当前事务的 Rollback 记录，返回 PessimisticLockRollbacked 错误</li>
<li>若已提交的 commit_ts 比当前事务的 start_ts 更新，说明在当前事务 begin 后有其他事务提交过</li>
<li>检查历史版本，如果发现当前请求的事务有没有被 Rollback 过，返回 PessimisticLockRollbacked 错误</li>
</ol>
</li>
</ul>
<p><img src="tikv/./dot/acquire_pessimistic_lock.svg" alt="" /></p>
<h3 id="client-pessimisticlockrollback"><a class="header" href="#client-pessimisticlockrollback">Client: PessimisticLockRollback</a></h3>
<p>TiDB从 事务的MemBuffer中获取所有被枷锁的key，向tikv发送rollback key lock请求。</p>
<p><img src="tikv/./dot/tidb_pessimistic_rollback.svg" alt="" /></p>
<h3 id="tikv-pessimisticlockrollback"><a class="header" href="#tikv-pessimisticlockrollback">TiKV: PessimisticLockRollback</a></h3>
<p><img src="tikv/./dot/tikv_pessimistic_lock_rollback.svg" alt="" /></p>
<h2 id="forupdatets"><a class="header" href="#forupdatets">forUpdateTS</a></h2>
<p>ForUpdateTS 存放在SessionVar的TransactionContext中。
然后放到twoPhaseCommitter中，最后在actionIsPessimiticLock
向TiK发送请求时，放到PessimisticRequest请求参数中,发给TiKV.</p>
<p><img src="tikv/./dot/for_update_ts_var.svg" alt="" /></p>
<p>在buildDelete, buildInsert, buildUpdate, buildSelectLock
时会去TSO服务获取最新的ts作为ForUpdateTS.</p>
<pre><code class="language-go">// UpdateForUpdateTS updates the ForUpdateTS, if newForUpdateTS is 0, it obtain a new TS from PD.
func UpdateForUpdateTS(seCtx sessionctx.Context, newForUpdateTS uint64) error {
</code></pre>
<p><img src="tikv/./dot/for_update_ts.svg" alt="" /></p>
<h2 id="tidb加锁规则"><a class="header" href="#tidb加锁规则">TiDB加锁规则</a></h2>
<p>TiDB中加锁规则如下(摘自[TiDB 悲观锁实现原理][1])</p>
<ul>
<li>插入（ Insert）
<ul>
<li>如果存在唯一索引，对应唯一索引所在 Key 加锁</li>
<li>如果表的主键不是自增 ID，跟索引一样处理，加锁。</li>
</ul>
</li>
<li>删除（Delete）
<ul>
<li>RowID 加锁</li>
</ul>
</li>
<li>更新 (update)
<ul>
<li>对旧数据的 RowID 加锁</li>
<li>如果用户更新了 RowID, 加锁新的 RowID</li>
<li>对更新后数据的唯一索引都加锁</li>
</ul>
</li>
</ul>
<p>TODO: 没找到insert/delete/update这块的lock代码
<img src="tikv/./dot/tidb_tikv_lock_keys_caller.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="resolve-lock"><a class="header" href="#resolve-lock">Resolve Lock</a></h1>
<blockquote>
<ul>
<li>在事务(假定为t1) 在Prewrite阶段执行时，如果遇到Lock冲突，首先会先根据Lock.primaryKey
获取持有该lock事务（假定为t2) &gt; 状态，如果primary key的lock已过期， 则尝试清理t2遗留的lock(cleanup或者commit).</li>
<li>Asncy commit 需要check所有的secondaris keys判断事务(t2)的<code>commit_ts</code></li>
<li>WriteType::Rollback类型的Write,写入的key ts为事务的<code>start_ts</code>,可能和其他事务的<code>commit_ts</code>相等，
因此在commit或者rollback_lock时，需要特殊处理。</li>
</ul>
</blockquote>
<!-- toc -->
<h2 id="prewrite-阶段处理lock冲突"><a class="header" href="#prewrite-阶段处理lock冲突">Prewrite 阶段处理lock冲突</a></h2>
<p>在TiDB prewrite阶段，如果遇到lock，会尝试resolveLocks，resolveLocks会尝试获取
持有lock的事务的状态，然后去resolve lock. 如果lock 没有被resolve, 还被其他
事务所持有，则返回要sleep的时间。prewite BackoffWithMaxSleep后，重新尝试去resolve locks。</p>
<p><img src="tikv/./dot/client__actionPrewrite__resolve_lock.svg" alt="" /></p>
<p>TiDB resolve lock 流程如下</p>
<pre><code class="language-go">// ResolveLocks tries to resolve Locks. The resolving process is in 3 steps:
// 1) Use the `lockTTL` to pick up all expired locks. Only locks that are too
//    old are considered orphan locks and will be handled later. If all locks
//    are expired then all locks will be resolved so the returned `ok` will be
//    true, otherwise caller should sleep a while before retry.
// 2) For each lock, query the primary key to get txn(which left the lock)'s
//    commit status.
// 3) Send `ResolveLock` cmd to the lock's region to resolve all locks belong to
//    the same transaction.
</code></pre>
<p><img src="tikv/./dot/client__resolve_lock_for_write.svg" alt="" /></p>
<p>对于primary key已经过期的事务，则尝试去resolve locks，根据事务类型有不同的resolve 方法</p>
<ol>
<li><code>resolveLock</code>: resolve正常提交的乐观事务lock</li>
<li><code>resolveLocksAsync</code>: 处理async commit的乐观事务txn locks，需要checkAllSecondaris key的
<code>min_commit_ts</code>来计算最终的<code>commit_ts</code>.</li>
<li><code>resolvePessimisticLock</code>: resolve 悲观事务lock</li>
</ol>
<h2 id="获取事务状态"><a class="header" href="#获取事务状态">获取事务状态</a></h2>
<h3 id="client-gettxnstatusfromlock"><a class="header" href="#client-gettxnstatusfromlock">client getTxnStatusFromLock</a></h3>
<p>resolveLocks 首先会根据<code>lock.primarykey</code>, 调用<code>LockResolver::getTxnStatus</code>去获取持有这个lock的事务的状态。</p>
<p><img src="tikv/./dot/tidb__getTxnStatus.svg" alt="" /></p>
<h3 id="tikv-checktxnstatus"><a class="header" href="#tikv-checktxnstatus">TiKV CheckTxnStatus</a></h3>
<p>事务(假定为t2)，prewrite阶段遇到Lock(假定为事务t1的lock)冲突时，会发CheckTxnStatus GRPC请求到TiKV
该Cmd主要功能如下：</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    /// checks whether a transaction has expired its primary lock's TTL, rollback the
    /// transaction if expired, or update the transaction's min_commit_ts according to the metadata
    /// in the primary lock.
    /// When transaction T1 meets T2's lock, it may invoke this on T2's primary key. In this
    /// situation, `self.start_ts` is T2's `start_ts`, `caller_start_ts` is T1's `start_ts`, and
    /// the `current_ts` is literally the timestamp when this function is invoked; it may not be
    /// accurate.
<span class="boring">}
</span></code></pre></pre>
<p>CheckTxnStatus 根据<code>lock.primary_key</code>检查事务t1的状态，在检查过程中，如果t1的lock过期，则可能会rollback t1。</p>
<p>主要会调用<code>check_txn_status_lock_exists</code>和<code>check_txn_status_missing_lock</code>来处理lock的几种可能情况:</p>
<ol>
<li>
<p><code>check_txn_status_lock_exists</code>： 如果Lock存在且t1还持有该lock， 如果lock没过期，更新lock的<code>min_commit_ts</code>, 返回TxnStatus::Uncommitted状态；如果lock已过期，会<b>rollback_lock</b>, 并返回TxnStatus::Expire状态.</p>
</li>
<li>
<p><code>check_txn_status_missing_lock</code>：lock不存在或者lock.ts已经不是t1了，t1可能已经commited了，也可能被rollback了。
需要调用<code>get_txn_commit_record</code>，扫描从<code>max_ts</code>到<code>t1.start_ts</code>之间key的write record来判断t1状态。</p>
</li>
</ol>
<p>调用流程图如下，其中黄色的是GRPC请求中带上来的数据。</p>
<ol>
<li><code>primary_key</code> lock的primary key</li>
<li><code>caller_start_ts</code>  如果lock没被提交或者rollback，会用它来更新lock的min_commit_ts</li>
<li><code>current_ts</code> 调用getTxnStat接口时，传入的当前ts.</li>
</ol>
<p><img src="tikv/./dot/check_txn_status.svg" alt="" /></p>
<h4 id="rollback_lock"><a class="header" href="#rollback_lock">rollback_lock</a></h4>
<p>t1的primary lock过期时，rollback_lock调用流程如下:</p>
<p>如果locktype 为put, 并且value没有保存在Lock的short_value字段中，则需要删掉之前写入的value.</p>
<p><img src="tikv/./dot/tikv_ResolveLock_rollback.svg" alt="" /></p>
<p>主要是提交了Rollback类型的Write, 注意此处的key为 <code>key t1.start_ts</code>, 而不是<code>key t1.commit_ts</code>
这是和pecolator论文中不一样的地方，可能会出现t1.start_ts和其他事务commit_ts一样的情况。</p>
<h4 id="get_txn_commit_record"><a class="header" href="#get_txn_commit_record">get_txn_commit_record</a></h4>
<p>事务t2遇到持有lock时t1时，调用<code>get_txn_commit_record</code>  扫描从max_ts到t2.start_ts的所有write record，
获取事务t1的状态。</p>
<h5 id="txncommitrecordsinglerecord"><a class="header" href="#txncommitrecordsinglerecord"><code>TxnCommitRecord::SingleRecord</code></a></h5>
<p>找到了<code>write.start_ts = t1.ts1</code>的WriteRecord，可以根据
该record的WriteType来判断事务状态，如果为Rollback则事务状态为rollback. 否则就是Committed。</p>
<h5 id="txncommitrecordoverlappedrollback"><a class="header" href="#txncommitrecordoverlappedrollback"><code>TxnCommitRecord::OverlappedRollback</code></a></h5>
<p>找到了<code>t1.start_ts == t3.commit_ts</code>，t3的write record，并且t3 write record中
<code>has_overlapped_write</code>为<b>true</b>，这时候可以确定事务的状态为Rollback</p>
<p>事务t1.start_ts和事务t3.commit_ts相同，并且write columns中，t3的write已经提交了。如果
直接写入t1的rollback，会覆盖掉t3之前的提交。为了避免该情况，只用将t3 write record中的
<code>has_overlapped_rollback</code> 设置为true即可。</p>
<p><img src="tikv/./dot/overlap_rollback.svg" alt="" /></p>
<h5 id="txncommitrecordnonesomewrite"><a class="header" href="#txncommitrecordnonesomewrite"><code>TxnCommitRecord::None(Some(write))</code></a></h5>
<p>找到了<code>t1.start_ts == t3.commit_ts</code> t3的write record，并且
t3 WriteRecord的<code>has_overlapped_write</code> 为<b>false</b>，后续rollback_lock和check_txn_status_missing_lock 
会将该字段设置为true.</p>
<p>t1先写入write rollback, 然后t3 commit时，会覆盖掉t1的write rollback.</p>
<p><img src="tikv/./dot/overalp_commit.svg" alt="" /></p>
<h5 id="txncommitrecordnonenone"><a class="header" href="#txncommitrecordnonenone"><code>TxnCommitRecord::None(None)</code></a></h5>
<p>如果状态为<code>TxnCommitRecord::None(None)</code>,并且Lock 现在被t4所持有，则将t1.start_ts
加入到Lock.rollback_ts数组中，这样在t4被commit时，如果t4.commit_ts == t1.start_ts
会将t4的write record的has_overlapped_write设置为true.</p>
<p>从max_ts到<code>t2.start_ts</code>没找到相关的write record.</p>
<h4 id="check_txn_status_missing_lock"><a class="header" href="#check_txn_status_missing_lock"><code>check_txn_status_missing_lock</code></a></h4>
<p>check_txn_status_missing_lock会调用<code>get_txn_commit_record</code>计算t1的commit状态，</p>
<p><img src="tikv/./dot/check_txn_status_missing_lock.svg" alt="" /></p>
<p>另外一种情形是，t1.start_ts == t3.commit_ts, 并且t1先被rollback了, t3 commit时，
会覆盖掉t1的rollback write record，这种check_txn_status_missing_lock 更新t3 
commit 的has_overalpped rollback设为为true.</p>
<p>上图中绿色的就是最后返回的txn status, 对应的enum如下,在TiDB中对应于返回字段中的Action.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Represents the status of a transaction.
#[derive(PartialEq, Debug)]
pub enum TxnStatus {
    /// The txn was already rolled back before.
    RolledBack,
    /// The txn is just rolled back due to expiration.
    TtlExpire,
    /// The txn is just rolled back due to lock not exist.
    LockNotExist,
    /// The txn haven't yet been committed.
    Uncommitted {
        lock: Lock,
        min_commit_ts_pushed: bool,
    },
    /// The txn was committed.
    Committed { commit_ts: TimeStamp },
    /// The primary key is pessimistically rolled back.
    PessimisticRollBack,
    /// The txn primary key is not found and nothing is done.
    LockNotExistDoNothing,
}
<span class="boring">}
</span></code></pre></pre>
<pre><code class="language-go">type Action int32

const (
	Action_NoAction                     Action = 0
	Action_TTLExpireRollback            Action = 1
	Action_LockNotExistRollback         Action = 2
	Action_MinCommitTSPushed            Action = 3
	Action_TTLExpirePessimisticRollback Action = 4
	Action_LockNotExistDoNothing        Action = 5
)
</code></pre>
<h2 id="清理expired-lock"><a class="header" href="#清理expired-lock">清理expired lock</a></h2>
<h3 id="resolvelock"><a class="header" href="#resolvelock">resolveLock</a></h3>
<p>TiDB 获取根据Lock.primary key获取完txn状态后，
开始resolve secondary key的lock.向TiKV
发起resolve Lock request.</p>
<p><img src="tikv/./dot/LockResolver__resolveLock.svg" alt="" /></p>
<h4 id="tikv-执行cmdresolvelock"><a class="header" href="#tikv-执行cmdresolvelock">TiKV 执行CmdResolveLock</a></h4>
<p>TiKV收到ResolveLock Request后，有三种case</p>
<ol>
<li><code>commit_ts &gt; 0</code>, 并且txn还持有该lock，则commit</li>
<li><code>commit_ts == 0</code>, 并且txn还持有该lock, 则rollback.</li>
<li>如果lock为None, 或者lock.ts已经发生改变了，则<code>check_txn_status_missing_lock</code></li>
</ol>
<p><img src="tikv/./dot/tikv_ResolveLock.svg" alt="" /></p>
<p>其中rollback和 check_txn_status_missing_lock 逻辑和上面 CheckTxnStatus中的一致。</p>
<h4 id="tikv-commit-处理流程"><a class="header" href="#tikv-commit-处理流程">TiKV commit 处理流程:</a></h4>
<p><img src="tikv/./dot/tikv_ResolveLock_commit.svg" alt="" /></p>
<h3 id="resolvelocksasync"><a class="header" href="#resolvelocksasync">resolveLocksAsync</a></h3>
<p>TiDB 中首先调用<code>checkAllSecondaries</code>来获取txn的Status, 然后对所有的secondaries keys按照region分组，并且每个分组启动一个go routine,  并发的发送CmdResolveLock 请求给TiKV</p>
<p><img src="tikv/./dot/tidb_resolveLockAsync2.svg" alt="" /></p>
<h4 id="client-checkallsecondaries"><a class="header" href="#client-checkallsecondaries">client checkAllSecondaries</a></h4>
<p><img src="tikv/./dot/client__checkAllSeconaries.svg" alt="" /></p>
<h4 id="tikv-cmdchecksecondarylocks"><a class="header" href="#tikv-cmdchecksecondarylocks">TiKV CmdCheckSecondaryLocks</a></h4>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    /// Check secondary locks of an async commit transaction.
    ///
    /// If all prewritten locks exist, the lock information is returned.
    /// Otherwise, it returns the commit timestamp of the transaction.
    ///
    /// If the lock does not exist or is a pessimistic lock, to prevent the
    /// status being changed, a rollback may be written.
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, PartialEq)]
enum SecondaryLockStatus {
    Locked(Lock),
    Committed(TimeStamp),
    RolledBack,
}
<span class="boring">}
</span></code></pre></pre>
<h5 id="lock-match"><a class="header" href="#lock-match">lock match</a></h5>
<p>如果txn还持有该lock，对于乐观事务，会返回lock信息，而悲观事务，则会unlock key? 向write column
写入rollback信息。(为什么？）</p>
<p><img src="tikv/./dot/check_seconary_locks_loc_exists.svg" alt="" /></p>
<h5 id="lock-mismatch"><a class="header" href="#lock-mismatch">lock mismatch</a></h5>
<p>如果lock已经被其他事务所持有。或者Lock已经被resolve.</p>
<p><img src="tikv/./dot/check_secondary_lock_mismatch.svg" alt="" /></p>
<h3 id="resolvepessimisticlock"><a class="header" href="#resolvepessimisticlock">resolvePessimisticLock</a></h3>
<p>PessimisticLock事务的悲观锁，多了一个forUpdateTs, 而且是直接清理lock，不像乐观锁那样，要写入rollback 类型的Write， 这个是为什么呀？</p>
<p><img src="tikv/./dot/tidb_resolvePessimisticLock.svg" alt="" /></p>
<h1 id="todo-2"><a class="header" href="#todo-2">TODO</a></h1>
<p>研究下这个rollback must be protected。
// The rollback must be protected, see more on</p>
<p>OverlappedRollback 和overlapped write代表什么意思？</p>
<p>// <a href="https://github.com/tikv/tikv/issues/7364">issue #7364</a></p>
<blockquote>
<p>Assume that we have three clients {c1, c2, c3} and two keys {k1, k2}:</p>
<ol>
<li>Pessimistic client c1 acquires a pessimistic lock on k1(primary), k2. But the command for k1 is lost at this point.</li>
<li>Optimistic client c2 requires to clean up the lock on k2</li>
<li>k1 is rollbacked and a write record (&quot;rollback&quot;, c1_start_ts, not_protected) is written into k1 (not_protected because the lock on k1 is missing), and a cleanup(primary=k1, ts=c1_start_ts)(*1) is sent but lost at this point.</li>
<li>Client c3 prewrites k1</li>
<li>Client c2 requires to clean up the lock on k1</li>
<li>k1 is rollbacked and the rollback write record is collapsed to (&quot;rollback&quot;, c3_start_ts, protected/not_protected)</li>
<li>Client c1 retries to lock on k1</li>
<li>k1 is locked by c1</li>
<li>Client c1 prewrites k1, k2</li>
<li>k1, k2 are prewrited by c1, and c1 received the prewrite succeed response</li>
<li>The lost cleanup command (*1) in step 3 is received by k2, therefore k2 is rollbacked</li>
<li>Client c1 commit k1</li>
<li>k1 is committed, while k2 is rollbacked</li>
</ol>
<p>Then atomic guarantee is broken.</p>
</blockquote>
<p><code>get_txn_commit_record</code> 这方法需要仔细研究下。</p>
<p>rollback, make_rollback, collapse_prev_rollback 这几个关系是啥？</p>
<h3 id="lock-rollback-ts"><a class="header" href="#lock-rollback-ts">lock rollback ts</a></h3>
<p><code>commit_ts</code>和<code>start_ts</code> 相等的时候会出现的情况。</p>
<p>为什么会出现相等呢？</p>
<p>写WriteType::Rollback时候，用的是start_ts, 而key被commit时候，write record的
key为<code>key commit_ts</code>, 当<code>start_ts == commit_ts</code>时，事务的rollback可能被
<code>commit_ts</code>所覆盖掉。</p>
<p>按照pecolator论文，commit时候，commit_ts一定比之前所有的start_ts大呀，为什么还会出现
被覆盖掉的情况呢？</p>
<p>是不是和Pingcap引入了并发的prewrite有关呢？</p>
<p><img src="tikv/./dot/Lock__rollback_ts.svg" alt="" /></p>
<p><img src="tikv/./dot/resolveLocksForWriteAsyncCommit.svg" alt="" />
<img src="tikv/./dot/resolve_region_locks.svg" alt="" />
<img src="tikv/./dot/check_secondary_locks.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scheduler-1"><a class="header" href="#scheduler-1">Scheduler</a></h1>
<h3 id="schedule_txn_cmd"><a class="header" href="#schedule_txn_cmd">schedule_txn_cmd</a></h3>
<p>从service/kv.rs grpc接口handler处理函数中，首先会将 req::into会将request 转换成
对应的cmd, 然后创建一个oneshot channel, 并await oneshot channel返回的future.</p>
<p>然后由<code>Scheduler::sched_txn_command</code>调度执行该cmd, cmd执行完毕，或者
遇到error后，会调用callback, callback触发onshot channel,
然后grpc handler 从await future中获取的resp 返回给client.</p>
<p><img src="tikv/./dot/sched_txn_command2.svg" alt="" /></p>
<h3 id="taskslots"><a class="header" href="#taskslots">TaskSlots</a></h3>
<p>Scheduler command中，会将cmd 包装为一个TaskContext
TaskContext中则包含了Task, cb(向上的回到), ProcessResult cmd的执行结果.</p>
<p>对于每个cmd会分配一个唯一的cid, task_slot则用于从cid获取cmd 对应的taskContext.</p>
<p>task slots 会先找到cid 对应的的slot, 之后上mutex lock，获取slot中的hashmap，
做插入查找操作。这样的好处是检查mutex lock，增加了并发度。</p>
<p><img src="tikv/./dot/scheduler_task_slots.svg" alt="" /></p>
<h3 id="run_cmd"><a class="header" href="#run_cmd">run_cmd</a></h3>
<p>在run cmd之前，会尝试获取cmd的所有的key的latches, 如果成功了，就执行cmd
否则就放入latches等待队列中。latches和task slot一样，也对key hash做了slot.</p>
<p>在cmd执行结束或者遇到error了，会release lock，释放掉command获取的key laches.</p>
<p>然后唤醒等待key latch的command id.</p>
<p><img src="tikv/./dot/acquire_latches_lock.svg" alt="" /></p>
<h3 id="release-lock"><a class="header" href="#release-lock">release lock</a></h3>
<p>释放cid拥有的latches lock, 唤醒等待的task,
这些被唤醒的task 会尝试去获取lock
如果task的涉及的所有key 的latches都拿到了，
就去执行task.</p>
<p><img src="tikv/./dot/scheduler_release_lock2.svg" alt="" /></p>
<h3 id="scheduler-execute"><a class="header" href="#scheduler-execute">Scheduler execute</a></h3>
<p>Scheduler执行cmd</p>
<p><img src="tikv/./dot/scheduler_execute2.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wait-lock"><a class="header" href="#wait-lock">Wait Lock</a></h1>
<p>Lock冲突事后，TiKV会将lock, StorageCallback, ProcessResult等打包成waiter.
放入等待队列中，等lock释放了，或者timeout了，再调用callback(ProcessResult)
回调通知client ProcessResult.  相当于延迟等待一段时间，避免client 无效的重试</p>
<p><img src="tikv/./dot/wait_for_lock.svg" alt="" /></p>
<p>lock和cb还有ProcessResult会被打包成waiter, cb调用会触发向client返回结果吗？</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// If a pessimistic transaction meets a lock, it will wait for the lock
/// released in `WaiterManager`.
///
/// `Waiter` contains the context of the pessimistic transaction. Each `Waiter`
/// has a timeout. Transaction will be notified when the lock is released
/// or the corresponding waiter times out.
pub(crate) struct Waiter {
    pub(crate) start_ts: TimeStamp,
    pub(crate) cb: StorageCallback,
    /// The result of `Command::AcquirePessimisticLock`.
    ///
    /// It contains a `KeyIsLocked` error at the beginning. It will be changed
    /// to `WriteConflict` error if the lock is released or `Deadlock` error if
    /// it causes deadlock.
    pub(crate) pr: ProcessResult,
    pub(crate) lock: Lock,
    delay: Delay,
    _lifetime_timer: HistogramTimer,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="加入等待队列"><a class="header" href="#加入等待队列">加入等待队列</a></h3>
<p>将请求放入等待队列中，直到lock被cleanup了，调用StorageCallback, cb中返回WriteConflict错误给
client 让client重试。</p>
<p>在放入前还会将wait lock信息放入dead lock scheduler, 检测死锁.</p>
<p><img src="tikv/./dot/lock_manager_wait_for.svg" alt="" /></p>
<p>WaitManager 从channel中去取task, 放入lock的等待队列中。
并加个timeout, 等待超时了会调用cb。并从dead lock scheduler中去掉wait lock。</p>
<p><img src="tikv/./dot/wait_manager_handle_wait_for.svg" alt="" /></p>
<h3 id="wakeup"><a class="header" href="#wakeup">WakeUp</a></h3>
<p>lock被释放后, LockaManager::wake_up 唤醒等待该lock的waiter.</p>
<p>TODO: 需要对lock.hash做一些说明。
TODO: task的回调机制需要整理下。</p>
<p><img src="tikv/./dot/lock_manager_wake_up.svg" alt="" /></p>
<p>LockManager::Wakeup</p>
<p><img src="tikv/./dot/lock_manager_wake_up2.svg" alt="" /></p>
<p>WaiterManager::handle_wake_up</p>
<p><img src="tikv/./dot/wait_manager_handle_wake_up.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="死锁检测-1"><a class="header" href="#死锁检测-1">死锁检测</a></h1>
<p>在事务被加到lock的等待队列之前，会做一发一个rpc请求, 到deadlock detector服务做deadlock检测。</p>
<p>TiKV 会动态选举出一个 TiKV node 负责死锁检测。</p>
<p>(下图摘自[TiDB 新特性漫谈：悲观事务][6]):</p>
<p><img src="tikv/./dot/dead_lock_detect.png" alt="" /></p>
<p>死锁检测逻辑如下(摘自[TiDB 悲观锁实现原理][1])</p>
<ol>
<li>维护全局的 wait-for-graph，该图保证无环。</li>
<li>每个请求会尝试在图中加一条 <code>txn -&gt; wait_for_txn</code> 的 edge，若新加的导致有环则发生了死锁。</li>
<li>因为需要发 RPC，所以死锁时失败的事务无法确定。</li>
</ol>
<h4 id="deadlock-leader本地detect"><a class="header" href="#deadlock-leader本地detect">deadlock leader本地detect</a></h4>
<p>对应代码调用流程如下：</p>
<p><img src="tikv/./dot/deadloack_Detector_handle_detect.svg" alt="" /></p>
<p>其中比较关键的是wait_for_map ，保存了txn 之间的依赖关系DAG图。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Used to detect the deadlock of wait-for-lock in the cluster.
pub struct DetectTable {
    /// Keeps the DAG of wait-for-lock. Every edge from `txn_ts` to `lock_ts` has a survival time -- `ttl`.
    /// When checking the deadlock, if the ttl has elpased, the corresponding edge will be removed.
    /// `last_detect_time` is the start time of the edge. `Detect` requests will refresh it.
    // txn_ts =&gt; (lock_ts =&gt; Locks)
    wait_for_map: HashMap&lt;TimeStamp, HashMap&lt;TimeStamp, Locks&gt;&gt;,

    /// The ttl of every edge.
    ttl: Duration,

    /// The time of last `active_expire`.
    last_active_expire: Instant,

    now: Instant,
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="转发请求给deadlock-leader"><a class="header" href="#转发请求给deadlock-leader">转发请求给Deadlock leader</a></h4>
<p>如果当前Deadlock detector不是leader,则会把请求转发给Deadlock leader, 转发流程如下:</p>
<p>首先Deadlock client和leader 维持一个grpc stream, detect请求会发到一个channel中
然后由send_task异步的发送DeadlockRequest给Deadlock leader. </p>
<p>recv_task则从stream接口中去获取resp, 然后调用回调函数，最后调用waiter_manager的
deadlock函数来通知等待的事务死锁了。</p>
<p><img src="tikv/./dot/deadlock_send_request_to_leader.svg" alt="" /></p>
<h4 id="deadlock-service"><a class="header" href="#deadlock-service">Deadlock Service</a></h4>
<p>Deadlock leader会在<code>handle_detect_rpc</code>中处理deadlock detect请求，流程和leader处理本地的一样。</p>
<p><img src="tikv/./dot/deadlock_service.svg" alt="" /></p>
<h4 id="deadlock-service的高可用"><a class="header" href="#deadlock-service的高可用">Deadlock Service的高可用</a></h4>
<p>Detector在handle_detect,如果leader client为none,
则尝试先去pd server获取<code>LEADER_KEY</code>所在的region(Leader Key为空串，
所以leader region为第一region. </p>
<p>然后解析出leader region leader的
store addr, 创建和deadlock detect leader的grpc detect接口的stream 连接</p>
<p><img src="tikv/./dot/deadlock_service_leader_info.svg" alt="" /></p>
<p>注册了使用Coprocessor的Observer, RoleChangeNotifier, 当leader
region的信息发变动时, RoleChangeNotifier会收到回调
会将leader_client和leader_inf清空，下次handle_detect时会重新
请求leader信息。</p>
<p><img src="tikv/./dot/deadlock_service_change_role.svg" alt="" /></p>
<h3 id="问题-detecttable的wait_for_map需要保证高可用吗"><a class="header" href="#问题-detecttable的wait_for_map需要保证高可用吗">问题: DetectTable的wait_for_map需要保证高可用吗？</a></h3>
<p>DetectTable的wait_for_map这个信息在deadlock detect leader
变动时候，是怎么处理的？看代码是直接清空呀？这个之前的依赖关系丢掉了，
这样不会有问题吗？</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="分组提交"><a class="header" href="#分组提交">分组提交</a></h1>
<p>TiDB 提交事务时，会先将mutation按照key的region做分组，
然 每个分组会分批并发的提交。</p>
<p>doActionOnBatches 这个对primaryBatch的commit操作做了特殊处理。</p>
<p><img src="tikv/./dot/doActionOnMuations.svg" alt="" /></p>
<h3 id="groupmutations-按照region分组-1"><a class="header" href="#groupmutations-按照region分组-1">groupMutations: 按照region分组</a></h3>
<p>先对mutations做分组，如果某个region的mutations 太多。
则会先对那个region先做个split, 这样避免对单个region
too much write workload.</p>
<p><img src="tikv/./dot/tidb_groupmutations.svg" alt="" /></p>
<h3 id="doactionongroupmutations-分批-1"><a class="header" href="#doactionongroupmutations-分批-1">doActionOnGroupMutations: 分批</a></h3>
<p>doActionOnGroupMutations 会对每个group的mutations 做进一步的分批处理。
对于actionCommit做了特殊处理，如果是NormalCommit, primay Batch要先提交，
然后其他的batch可以新起一个go routine在后台异步提交。</p>
<p><img src="tikv/./dot/tidb_doActionOnGroupMutations.svg" alt="" /></p>
<h3 id="batchexecutor-并发的处理batches"><a class="header" href="#batchexecutor-并发的处理batches">batchExecutor: 并发的处理batches</a></h3>
<p><code>batchExecutor::process</code> 每个batch会启动一个go routine来并发的处理,
并通过channel等待batch的处理结果。当所有batch处理完了，再返回给调用者。</p>
<p>其中会使用令牌做并发控制, 启动goroutine前先去获取token, goroutine运行
完毕，归还token。</p>
<p><img src="tikv/./dot/tidb_doActionOnBatches.svg" alt="" /></p>
<h2 id="committermutations"><a class="header" href="#committermutations">CommitterMutations</a></h2>
<p>数据结构引用关系如下:</p>
<p><img src="tikv/./dot/commiter_mutations.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scanner"><a class="header" href="#scanner">Scanner</a></h1>
<blockquote>
<p>Scanner 使用归并排序的思路扫描CF_LOCK, CF_WRITE来做遍历</p>
</blockquote>
<!-- toc -->
<h2 id="pointgetter"><a class="header" href="#pointgetter">PointGetter</a></h2>
<p>假定事务t2,使用PointGetter::get读取user_key的value，t2的<code>start_ts</code> 保存在PointGetter::ts中。</p>
<p>get 一个user_key value过程如下：</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    pub fn get(&amp;mut self, user_key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt; {
<span class="boring">}
</span></code></pre></pre>
<ol>
<li>
<p>如果IsolationLevel为SI(Snapshot Isolation), 则需要检查先<code>load_and_check_lock</code>， 如果为RC(read commit), 则直接去<code>load_data</code>即可。</p>
</li>
<li>
<p>load_and_check_lock 会从<code>CF_LOCK</code>中读取<code>user_key</code>的lock， 然后检查lock和t2的时间戳。</p>
</li>
<li>
<p><code>load_data</code>从<code>CF_WRITE</code>中查找<code>[0, t2.start_ts]</code>之间最新事务(假设为t1)的Write, 其key为<code>{user_key}{t1.commit_ts}</code>, 如果write type为Rollback
或者Lock，就skip掉，接着查找下一个, 如果Write type 为delete，则直接返回None，如果是Put, 则从WriteRef中读取到事务t1的start_ts。
然后去读取数据。</p>
</li>
<li>
<p>从<code>CF_DEFAULT</code> 找到 <code>{user_key}{t1.start_ts}</code>对应的value。TiKV 对short_value做了优化，直接把value写在CF_WRITE中了，避免了一次再从<code>CF_DEFAULT</code>读取数据的过程。</p>
</li>
</ol>
<p><img src="tikv/./dot/PointGetter_get.svg" alt="" /></p>
<h2 id="scanner-主要struct"><a class="header" href="#scanner-主要struct">Scanner 主要struct</a></h2>
<p>Scanner主要分为<code>ForwardScanner </code>和<code>BackwardKvScanner</code>，它们公用信息保存在<code>ScannerConfig</code>中，它们使用<code>Cursor</code>来遍历CF_LOCK, CF_WRITE, CF_DEFAULT中的数据。</p>
<p><code>Cursor</code>主要在底层RocksDB的iter基础上，包装了一些near_seek, seek, valid等函数，并会将一些统计信息
写入到<code>CfStatistics</code>中。</p>
<p><code>ScannerConfig</code> 用于保存一些公用的信息，比如scan key的lower_bound和upper_bound, 另外它还负责使用CursorBuilder创建cursor.</p>
<p><code>Snapshot</code> 则提供了Iterator供Cursor使用。</p>
<p><code>ForwardKvScanner</code>在遇到lock/write时，使用<code>Trait ScanPolicy</code>来处理lock/write.
<code>Trait Policy</code> impl有:</p>
<ol>
<li>DeltaEntryPolicy </li>
<li>LatestKvPolicy </li>
<li>LatestEntryPolicy</li>
</ol>
<p><img src="tikv/./dot/storage_scanner_struct.svg" alt="" /></p>
<h2 id="cursor"><a class="header" href="#cursor">Cursor</a></h2>
<p>Cursor则在RocksSnapshot的ite基础上包装了一些seek, near_seek等功能。
并每次读取key,value, 都会在<code>CfStatistics</code>上加一些统计，</p>
<p>在iter上加了一个near_seek结合和scan_mode，每次Key,value 
会加一些key, value的统计。</p>
<p><img src="tikv/./dot/cursor.svg" alt="" /></p>
<h2 id="forwardscanner"><a class="header" href="#forwardscanner">ForwardScanner</a></h2>
<p>ForwardScanner 用于扫描range(对应<code>ScannerConfig</code>中的lower_bound和upper_bound)内所有key最新(<code>commit_ts &lt;=T.start_ts</code>) value</p>
<p>最简单粗暴的做法是像PointGetter那样，一个个扫描，但问题是对于<code>CF_WRITE</code>中扫描到的每个user_key，都需要到<code>CF_LOCK</code>中seek 查找它的lock信息。
但这样效率太低了.</p>
<p>TiKV采用了类似于归并排序的思路，同时移动 write cursor和 lock cursor. 使用最小的作为current_user_key。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//current_user_key, user_key, has_write, has_lock

// `current_user_key` is `min(user_key(write_cursor), lock_cursor)`, indicating
// the encoded user key we are currently dealing with. It may not have a write, or
// may not have a lock. It is not a slice to avoid data being invalidated after
// cursor moving.
//
// `has_write` indicates whether `current_user_key` has at least one corresponding
// `write`. If there is one, it is what current write cursor pointing to. The pointed
// `write` must be the most recent (i.e. largest `commit_ts`) write of
// `current_user_key`.
//
// `has_lock` indicates whether `current_user_key` has a corresponding `lock`. If
// there is one, it is what current lock cursor pointing to.
(Some(wk), Some(lk)) =&gt; {
    let write_user_key = Key::truncate_ts_for(wk)?;
    match write_user_key.cmp(lk) {
        Ordering::Less =&gt; {
            // Write cursor user key &lt; lock cursor, it means the lock of the
            // current key that write cursor is pointing to does not exist.
            (write_user_key, true, false)
        }
        Ordering::Greater =&gt; {
            // Write cursor user key &gt; lock cursor, it means we got a lock of a
            // key that does not have a write. In SI, we need to check if the
            // lock will cause conflict.
            (lk, false, true)
        }
        Ordering::Equal =&gt; {
            // Write cursor user key == lock cursor, it means the lock of the
            // current key that write cursor is pointing to *exists*.
            (lk, true, true)
        }
    }
<span class="boring">}
</span></code></pre></pre>
<p>然后调用<code>Trait ScanPolicy</code> 的<code>handle_lock</code>, <code>handle_write</code>来处理遇到的lock, write</p>
<p>ScanPolicy 有以下三种impl</p>
<ul>
<li><b> LatestKvPolicy</b>: outputs the latest key value pairs.</li>
<li><b> LatestEntryPolicy</b>: only outputs records whose commit_ts is greater than <code>after_ts</code>. It also supports outputting delete records if <code>output_delete</code> is set to <code>true</code>.</li>
<li><b>DeltaEntryPolicy</b>: The ScanPolicy for outputting <code>TxnEntry</code> for every locks or commits in specified ts range.  The <code>ForwardScanner</code> with this policy scans all entries whose <code>commit_ts</code>s (or locks' <code>start_ts</code>s) in range (<code>from_ts</code>, <code>cfg.ts</code>].</li>
</ul>
<h3 id="latestkvpolicy"><a class="header" href="#latestkvpolicy">LatestKvPolicy</a></h3>
<p><img src="tikv/./dot/LastKvPolicy.svg" alt="" /></p>
<h3 id="latestentrypolicy"><a class="header" href="#latestentrypolicy">LatestEntryPolicy</a></h3>
<h3 id="deltaentrypolicy"><a class="header" href="#deltaentrypolicy">DeltaEntryPolicy</a></h3>
<h2 id="backwardkvscanner"><a class="header" href="#backwardkvscanner">BackwardKvScanner</a></h2>
<h2 id="questions-3"><a class="header" href="#questions-3">Questions</a></h2>
<ol>
<li>bypass_locks ? 这个作用是什么?</li>
<li>check_ts_conflict 为啥lockType为Pessimistic 就可以返回OK?</li>
</ol>
<h2 id="参考-10"><a class="header" href="#参考-10">参考</a></h2>
<ol>
<li><a href="https://tikv.org/deep-dive/distributed-transaction/percolator/#percolator-in-tikv">percolator-in-tikv</a></li>
<li><a href="https://pingcap.com/blog-cn/tikv-source-code-reading-13/">MVCC 数据读取</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coprocessor-1"><a class="header" href="#coprocessor-1">Coprocessor</a></h1>
<h2 id="draft-5"><a class="header" href="#draft-5">draft</a></h2>
<p><img src="tikv/./dot/coprocessor-draft.svg" alt="" /></p>
<p>BatchExecutor</p>
<pre><code>{agg;selection} -&gt;  BatchTableScanner;BatchIndexScanner -&gt; Scanner -&gt; RangeScanner -&gt; Storage.scan_next;
</code></pre>
<p>分为三类 scanner, selection, agg</p>
<p>其中scanner是作为基础数据源的，selection/agg就是在这个基础数据源上做filter和agg
scanner 又依赖于RangesScanner</p>
<p><img src="tikv/./dot/coprocessor_BatchExecutor.svg" alt="" /></p>
<h3 id="rangesscanner"><a class="header" href="#rangesscanner">RangesScanner</a></h3>
<p>TiKVStorage</p>
<p>这块需要先把Storage/mvcc的scanner先研究透了.</p>
<p><img src="tikv/./dot/coprocessor_storage.svg" alt="" /></p>
<h3 id="scanexecutor"><a class="header" href="#scanexecutor">ScanExecutor</a></h3>
<p>impl负责process_kv_pair, RangeScanner扫描获取kv</p>
<p>ranges_iterator感觉像获取多个range的数据？</p>
<p>把多个range chain起来？</p>
<p><img src="tikv/./dot/coprocessor_ScanExecutor.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rpnexpression"><a class="header" href="#rpnexpression">RpnExpression</a></h1>
<!-- toc -->
<h2 id="rpnexpressionbuilder"><a class="header" href="#rpnexpressionbuilder">RpnExpressionBuilder</a></h2>
<p>Expr 定义在tipb repo的<code>proto/expression.proto</code>文件中。</p>
<pre><code class="language-proto">// Evaluators should implement evaluation functions for every expression type.
message Expr {
	optional ExprType tp = 1 [(gogoproto.nullable) = false];
	optional bytes val = 2;
	repeated Expr children = 3;
	optional uint32 rpn_args_len = 6;
	optional ScalarFuncSig sig = 4 [(gogoproto.nullable) = false];
	optional FieldType field_type = 5;
	optional bool has_distinct = 7 [(gogoproto.nullable) = false];
}
</code></pre>
<p>ExprType主要分为三类，value类型的，agg函数，scalar函数。
scalar函数，在TiKV中会build对应的<code>RpnFnMeta</code>
agg函数也对应的AggregateFunction和AggregateState.</p>
<p><img src="tikv/./dot/tipb_expr.svg" alt="" /></p>
<p><code>RpnExpressionBuilder</code> 将expr tree转换为RpnExpression, 
在handle_node_fn_call, 处理ScalarFunc时候，会使用后续遍历方式，先递归
处理ScalarFunc的args,最后再处理ScalarFunc节点。</p>
<p>其中比较重要的是调用<code>map_expr_node_to_rpn_func</code>，
生成函数对应的RpnFnMeta.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn map_expr_node_to_rpn_func(expr: &amp;Expr) -&gt; Result&lt;RpnFnMeta&gt; {
    let value = expr.get_sig();
    let children = expr.get_children();
    let ft = expr.get_field_type();
    Ok(match value {
        // impl_arithmetic
        ScalarFuncSig::PlusInt =&gt; map_int_sig(value, children, plus_mapper)?,
        ScalarFuncSig::PlusIntUnsignedUnsigned =&gt; arithmetic_fn_meta::&lt;UintUintPlus&gt;(),
        //...
    }
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/rpn_expression_builder.svg" alt="" /></p>
<h3 id="过程宏-rpn_fn"><a class="header" href="#过程宏-rpn_fn">过程宏 rpn_fn</a></h3>
<blockquote>
<p>Coprocessor 直接实现了向量与标量的运算，rpn_expr_codegen 提供了过程宏 #[rpn_fn] ，我们只需定义标量逻辑，过程宏将自动生成剩下带有向量的逻辑。</p>
</blockquote>
<p>rpn 代码生成</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The `rpn_fn` attribute.
#[proc_macro_attribute]
pub fn rpn_fn(attr: TokenStream, input: TokenStream) -&gt; TokenStream {
    match rpn_function::transform(attr.into(), input.into()) {
        Ok(tokens) =&gt; TokenStream::from(tokens),
        Err(e) =&gt; TokenStream::from(e.to_compile_error()),
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>生成对应vector代码调用地方如下, 循环的调用scalar函数, 生成vector版本的rpn function.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//...
let chunked_push = if self.writer {
    quote! {
        let writer = result.into_writer();
        let guard = #fn_ident #ty_generics_turbofish ( #(#captures,)* #(#call_arg),* , writer)?;
        result = guard.into_inner();
    }
} else {
    quote! {
        result.push( #fn_ident #ty_generics_turbofish ( #(#captures,)* #(#call_arg),* )?);
    }
};

//循环loop调用标量的func
let nullable_loop = quote! {
    for row_index in 0..output_rows {
<span class="boring">        (let (#extract, arg) = arg.extract(row_index));*;
</span><span class="boring">        chunked_push
</span>    }
};
<span class="boring">}
</span></code></pre></pre>
<h2 id="rpnexpression-struct"><a class="header" href="#rpnexpression-struct">RpnExpression struct</a></h2>
<p>RpnExpression是逆波兰表达式，比如 <code>2 + a</code> 的RPN表达式为<code>2 a +</code> ，RpnExpressionNodeNode有三种类型: <code>Const</code>和<code>Column Ref</code>, <code>Fn</code> 
比如对于表达式 <code>2 a + </code>，其中2为Const, a 为ColumnRef，+ 为Fn。</p>
<p><img src="tikv/./dot/rpn_expression_struct_relationship.svg" alt="" /></p>
<p>ColumnRef只记录了一个offset, 表示引用了input_physical_columns index为offset的列.</p>
<h2 id="lazybatchcolumn-decode"><a class="header" href="#lazybatchcolumn-decode">LazyBatchColumn decode</a></h2>
<p>在eval之前需要对column数据做解码，从<code>Vec&lt;8&gt;</code> decode对应的<code>field_type</code> 类型的数据。</p>
<p><code>LazyBatchColumn</code> 中<code>Raw</code> 存放了原始数据，<code>Decode</code> 存放了解码后的数据。</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Clone, Debug)]
pub enum LazyBatchColumn {
//原始数据
    Raw(BufferVec),
//Decode之后的数据
    Decoded(VectorValue),
}
<span class="boring">}
</span></code></pre></pre>
<p><code>LazyBatchColumn::ensure_decoded</code> 会根据传进来的LogicalRows 对需要的rows做解码</p>
<p><img src="tikv/./dot/logical_rows.svg" alt="" /></p>
<h2 id="vectorvalue"><a class="header" href="#vectorvalue">VectorValue</a></h2>
<p>VectorValue包含各种具体type的enum. LazyBatchColumn decode后，会从原始的
vec&lt;8&gt; 数据，<code>decode</code>为<code>field_type</code>对应的具体类型。</p>
<p><img src="tikv/./dot/vector_value.svg" alt="" /></p>
<h2 id="rpnstacknode"><a class="header" href="#rpnstacknode">RpnStackNode</a></h2>
<p>RpnExpression eval时候，会使用一个stack, stack中的元素即为<code>RpnStackNode</code>
有两种类型，<code>scalar</code>表示标量，<code>vector</code>表示向量.  比如上面表达式<code>2 + a</code> 中2就是标量， 
a为向量(column a 那一列值)</p>
<p><img src="tikv/./dot/rpn_stack_node.svg" alt="" /></p>
<h2 id="rpnexpression-eval"><a class="header" href="#rpnexpression-eval">RpnExpression eval</a></h2>
<p>RpnExpression eval时，会对遍历RpnExpressionNode, 遇到const或者column ref就压入stack，
遇到<code>Fn</code>节点的，就从stack顶上pop出N个args。</p>
<p>执行完Fn后将结果再push到stack中，stack中最后元素即为RpnExpression的结果。</p>
<p><img src="tikv/./dot/rpn_expression_eval.svg" alt="" /></p>
<h2 id="draft-6"><a class="header" href="#draft-6">draft</a></h2>
<h3 id="rpn-宏相关代码分析"><a class="header" href="#rpn-宏相关代码分析">rpn 宏相关代码分析</a></h3>
<p><img src="tikv/./dot/rpn_fn.svg" alt="" /></p>
<p><img src="tikv/./dot/rpn_fn_struct.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aggrfunction"><a class="header" href="#aggrfunction">AggrFunction</a></h1>
<p>在build_executors时，会将DagRequest中的tipb::Executors 解析为AggrFunction
存放在Entities.each_aggr_fn Vec中.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait AggrFunction: std::fmt::Debug + Send + 'static {
    /// The display name of the function.
    fn name(&amp;self) -&gt; &amp;'static str;

    /// Creates a new state instance. Different states aggregate independently.
    fn create_state(&amp;self) -&gt; Box&lt;dyn AggrFunctionState&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/aggr_function_parse.svg" alt="" /></p>
<h3 id="aggrfunctionstate"><a class="header" href="#aggrfunctionstate">AggrFunctionState</a></h3>
<p>AggrFunctionState 由AggrFunction::create_state创建.
定义了一个derive, aggr_function, 用来自动生成create_state</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, AggrFunction)]
#[aggr_function(state = AggrFnStateAvg::&lt;T&gt;::new())]
pub struct AggrFnAvg&lt;T&gt;
where
    T: Summable,
    VectorValue: VectorValueExt&lt;T&gt;,
{
    _phantom: std::marker::PhantomData&lt;T&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/aggr_function_derive.svg" alt="" /></p>
<p><img src="tikv/./dot/AggrFunctionState.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="batchexecutor"><a class="header" href="#batchexecutor">BatchExecutor</a></h1>
<!-- toc -->
<h2 id="executor-proto"><a class="header" href="#executor-proto">Executor proto</a></h2>
<p>tipb proto中定义的Executor 关系如下</p>
<p>其中TableScan和IndexScan是最底层的Executor, 从Storage can key range的数据，供上层(Selection等）其他Executor使用。</p>
<p><img src="tikv/./dot/tipb_executor.svg" alt="" /></p>
<h2 id="build_executors"><a class="header" href="#build_executors">build_executors</a></h2>
<p>build_executors 根据tipb中定义的Executor 创建对应的BatchExecutor</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[allow(clippy::explicit_counter_loop)]
pub fn build_executors&lt;S: Storage + 'static&gt;(
    executor_descriptors: Vec&lt;tipb::Executor&gt;,
    storage: S,
    ranges: Vec&lt;KeyRange&gt;,
    config: Arc&lt;EvalConfig&gt;,
    is_scanned_range_aware: bool,
) -&gt; Result&lt;Box&lt;dyn BatchExecutor&lt;StorageStats = S::Statistics&gt;&gt;&gt; {

    match first_ed.get_tp() {

        ExecType::TypeTableScan =&gt; {
        //...
<span class="boring">}
</span></code></pre></pre>
<p>参数中的<code>executor_descriptors</code>数组，第i个是第i+1个的child Executor, 
且第一个为TableScan或者IndexScan。</p>
<p><img src="tikv/./dot/build_executors.svg" alt="" /></p>
<h2 id="batchexecutor-trait"><a class="header" href="#batchexecutor-trait">BatchExecutor Trait</a></h2>
<p>BatchExecutor定义了Executor的基本接口, 其中的<code>next_batch</code>用来
从child Executor中获取数据。</p>
<p>数据格式为<code>LazyBatchColumnVec</code></p>
<p><img src="tikv/./dot/BatchExecutor_next_batch.svg" alt="" /></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;C: ExecSummaryCollector + Send, T: BatchExecutor&gt; BatchExecutor
    for WithSummaryCollector&lt;C, T&gt;
{
    type StorageStats = T::StorageStats;

    fn schema(&amp;self) -&gt; &amp;[FieldType] {
        self.inner.schema()
    }

    fn next_batch(&amp;mut self, scan_rows: usize) -&gt; BatchExecuteResult {
        let timer = self.summary_collector.on_start_iterate();
        let result = self.inner.next_batch(scan_rows);
        self.summary_collector
            .on_finish_iterate(timer, result.logical_rows.len());
        result
    }

    fn collect_exec_stats(&amp;mut self, dest: &amp;mut ExecuteStats) {
        self.summary_collector
            .collect(&amp;mut dest.summary_per_executor);
        self.inner.collect_exec_stats(dest);
    }

    fn collect_storage_stats(&amp;mut self, dest: &amp;mut Self::StorageStats) {
        self.inner.collect_storage_stats(dest);
    }

    fn take_scanned_range(&amp;mut self) -&gt; IntervalRange {
        self.inner.take_scanned_range()
    }

    fn can_be_cached(&amp;self) -&gt; bool {
        self.inner.can_be_cached()
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="batchexecutorsrunner"><a class="header" href="#batchexecutorsrunner">BatchExecutorsRunner</a></h2>
<h3 id="call-next_batch"><a class="header" href="#call-next_batch">call next_batch</a></h3>
<p><img src="tikv/./dot/call_next_batch.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rangesscanner-1"><a class="header" href="#rangesscanner-1">RangesScanner</a></h1>
<p>提供了统一的next接口，从Storage中遍历多个Key Range</p>
<p><img src="tikv/./dot/ranges_scanner.svg" alt="" /></p>
<h2 id="tikvstorage"><a class="header" href="#tikvstorage">TiKVStorage</a></h2>
<p><img src="tikv/./dot/TiKVStorage.svg" alt="" /></p>
<h2 id="snapshot-2"><a class="header" href="#snapshot-2">Snapshot</a></h2>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A Snapshot is a consistent view of the underlying engine at a given point in time.
///
/// Note that this is not an MVCC snapshot, that is a higher level abstraction of a view of TiKV
/// at a specific timestamp. This snapshot is lower-level, a view of the underlying storage.
pub trait Snapshot: Sync + Send + Clone {
    type Iter: Iterator;

    /// Get the value associated with `key` in default column family
    fn get(&amp;self, key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt;;

    /// Get the value associated with `key` in `cf` column family
    fn get_cf(&amp;self, cf: CfName, key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt;;

    /// Get the value associated with `key` in `cf` column family, with Options in `opts`
    fn get_cf_opt(&amp;self, opts: ReadOptions, cf: CfName, key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt;;
    fn iter(&amp;self, iter_opt: IterOptions) -&gt; Result&lt;Self::Iter&gt;;
    fn iter_cf(&amp;self, cf: CfName, iter_opt: IterOptions) -&gt; Result&lt;Self::Iter&gt;;
    // The minimum key this snapshot can retrieve.
    #[inline]
    fn lower_bound(&amp;self) -&gt; Option&lt;&amp;[u8]&gt; {
        None
    }
    // The maximum key can be fetched from the snapshot should less than the upper bound.
    #[inline]
    fn upper_bound(&amp;self) -&gt; Option&lt;&amp;[u8]&gt; {
        None
    }

    /// Retrieves a version that represents the modification status of the underlying data.
    /// Version should be changed when underlying data is changed.
    ///
    /// If the engine does not support data version, then `None` is returned.
    #[inline]
    fn get_data_version(&amp;self) -&gt; Option&lt;u64&gt; {
        None
    }

    fn is_max_ts_synced(&amp;self) -&gt; bool {
        // If the snapshot does not come from a multi-raft engine, max ts
        // needn't be updated.
        true
    }
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/range_scanner_snapshot.svg" alt="" /></p>
<p>调用RaftEngine的<code>async_snapshot</code>获取snapshot</p>
<p><img src="tikv/./dot/range_scanner_snaphost_from.svg" alt="" /></p>
<h2 id="tls-engine"><a class="header" href="#tls-engine">tls engine</a></h2>
<p><img src="tikv/./dot/range_scanner_tls_engine.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scanexecutor-1"><a class="header" href="#scanexecutor-1">ScanExecutor</a></h1>
<!-- toc -->
<p>ScanExecutor 使用RangesScanner从底层的Storage，扫描读取Ranges内的key, value Pair,
然后由<code>TableScanExectuorImpl</code>或者<code>IndexScanExecutorImpl</code>
根据ColumnInfo信息，将key,value pair, 组装成 LazyBatchColumnVec, 
供上层Executor 使用。</p>
<p>对于TableScan来说， key中包含了intHandle或者commonHandle, 而value则是一些columns Id和column的值</p>
<p>value有两种编码方式v1版本的，是普通的datum方式, <code>col_id1 value1 col_id2 value2</code>. 
V2版本的是<code>RowSlice</code>, 具体格式信息见下文.</p>
<p>对于IndexScan来说，key中包含了建索引的columns的columnsValues(编码方式为datum), 如果是unique index的话，key 中则还
包含了intHandle或者commonHandle 信息。</p>
<p><img src="tikv/./dot/batch_executor_scan.svg" alt="" /></p>
<h2 id="scanexecutornext_batch"><a class="header" href="#scanexecutornext_batch">ScanExecutor::next_batch</a></h2>
<p>迭代读取<code>scan_rows</code>行数据，每次调用<code>RangesScanner::next</code>从
<code>Storage</code>中读取kv数据, 然后调用impl的<code>process_kv_pair</code>处理kv数据.
放入<code>LazyBatchColumnVec</code>中，返回给上层Executor。</p>
<p><img src="tikv/./dot/ScanExecutor_next_batch.svg" alt="" /></p>
<h2 id="tablescanexectuorimpl"><a class="header" href="#tablescanexectuorimpl">TableScanExectuorImpl</a></h2>
<h3 id="primary-key"><a class="header" href="#primary-key">primary key</a></h3>
<p>primary key可能是两个column compose起来才是primary key, 比如这样：</p>
<p>multi column compose的primay key 应该有个unique index吧.</p>
<p>primay index在column之上又搞了啥？</p>
<pre><code class="language-sql">CREATE TABLE table_name(
    primary_key_column datatype PRIMARY KEY,
    --...
);

CREATE TABLE IF NOT EXISTS tasks (
    task_id INT AUTO_INCREMENT PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    start_date DATE,
    --...
)  ENGINE=INNODB;


CREATE TABLE
    product (
        category INT NOT NULL,
        id INT NOT NULL,
        price DECIMAL,
        PRIMARY KEY(category, id)
    );
</code></pre>
<p>handle_indices 和primary column 这两个是什么概念？</p>
<h3 id="tablescanexecutor-输入输出"><a class="header" href="#tablescanexecutor-输入输出">TableScanExecutor 输入输出</a></h3>
<p>不明白的的是为什么要handle_indices push 同一个handle值</p>
<p><img src="tikv/./dot/table_scan_io.svg" alt="" /></p>
<h3 id="tablescanexecutor-数据结构关系"><a class="header" href="#tablescanexecutor-数据结构关系">TableScanExecutor 数据结构关系</a></h3>
<p><img src="tikv/./dot/BatchTableScanExecutor.svg" alt="" /></p>
<h3 id="处理key"><a class="header" href="#处理key">处理key</a></h3>
<p>解析key中的intHandle或者commonHandle.</p>
<p><img src="tikv/./dot/table_scan_process_kv_pair.svg" alt="" /></p>
<h3 id="v1版本value"><a class="header" href="#v1版本value">v1版本value</a></h3>
<p><img src="tikv/./dot/table_scan_process_v1.svg" alt="" /></p>
<h3 id="v2版本value"><a class="header" href="#v2版本value">v2版本value</a></h3>
<p><img src="tikv/./dot/table_scan_process_v2.svg" alt="" /></p>
<h2 id="indexscanexecutorimpl"><a class="header" href="#indexscanexecutorimpl">IndexScanExecutorImpl</a></h2>
<h3 id="indexscanexecutor的输入输出"><a class="header" href="#indexscanexecutor的输入输出">IndexScanExecutor的输入输出</a></h3>
<p>TiKV中 index 的key layout布局如下：</p>
<p>Unique index</p>
<blockquote>
<p>Key: tablePrefix_tableID_indexPrefixSep_indexID_indexedColumnsValue
Value: value</p>
</blockquote>
<p>非unique index</p>
<blockquote>
<p>Key: tablePrefix_idxPrefix_tableID_indexID_ColumnsValue_handle, value: null</p>
</blockquote>
<p>其中的handle可以为IntHandle或者commonHandle</p>
<p>IndexScanExecutor的输入输出如下:</p>
<p><img src="tikv/./dot/indexvalue_layout.svg" alt="" /></p>
<p>其中输出的columns 是在<code>IndexScanExecutorImpl::build_column_vec</code>
方法中创建的。</p>
<h3 id="indexscan-数据结构关系"><a class="header" href="#indexscan-数据结构关系">IndexScan 数据结构关系</a></h3>
<p><img src="tikv/./dot/BatchIndexScanExecutor.svg" alt="" /></p>
<h3 id="process_old_collation_kv"><a class="header" href="#process_old_collation_kv">process_old_collation_kv</a></h3>
<p><img src="tikv/./dot/IndexScan_process_old_collation_kv.svg" alt="" /></p>
<h3 id="process_kv_general"><a class="header" href="#process_kv_general">process_kv_general</a></h3>
<p><img src="tikv/./dot/IndexScan_process_kv_pair.svg" alt="" /></p>
<h2 id="rangesscannernext"><a class="header" href="#rangesscannernext">RangesScanner::next</a></h2>
<p>从storage中读取数据</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="selection"><a class="header" href="#selection">Selection</a></h1>
<p>调用Src BatchExecutor的next_batch, 获取数据，然后对于自己的每个condition
调用 RpnExpression::eval, 计算condition的结果，然后只保留condition为true的
logical rows.</p>
<p><img src="tikv/./dot/batch_executor_selection.svg" alt="" /></p>
<h3 id="next_batch"><a class="header" href="#next_batch">next_batch</a></h3>
<p>这里面RpnExpression是逆波兰表达式，比如2 *（3 + 4）+ 5 会被
表示为: 2 3 4 + * 5 +。</p>
<p><img src="tikv/./dot/batch_executor_selection_next_batch.svg" alt="" /></p>
<p>RpnExpression eval时，从左到右遍历表达式，遇到操作数(比如数字2,3)，
就push到stack中，遇到operator(比如+号)就从Stack中pop出operator需要的参数
比如+就pop 3和4，然后将 3 4 +的执结果7push到stack中。最后stack中就是执行的结果。</p>
<p>对应的执行逻辑在代码<code>RpnExpression::eval_decoded</code>函数中</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    pub fn eval_decoded&lt;'a&gt;(
        &amp;'a self,
        ctx: &amp;mut EvalContext,
        schema: &amp;'a [FieldType],
        input_physical_columns: &amp;'a LazyBatchColumnVec,
        input_logical_rows: &amp;'a [usize],
        output_rows: usize,
    ) -&gt; Result&lt;RpnStackNode&lt;'a&gt;&gt; {

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="agg-executor"><a class="header" href="#agg-executor">Agg executor</a></h1>
<p><img src="tikv/./dot/batch_executor_agg.svg" alt="" /></p>
<h2 id="next_batch-1"><a class="header" href="#next_batch-1">next_batch</a></h2>
<p><img src="tikv/./dot/Aggregator_next_batch.svg" alt="" /></p>
<h2 id="aggregationexecutorimpl"><a class="header" href="#aggregationexecutorimpl">AggregationExecutorImpl</a></h2>
<p>对应四种实现，每个里面都有个states 是<code>Vec&lt;Box&lt;dyn AggrFunctionState&gt;&gt;</code>
用来保存aggr state (比如avg 的state需要保存sum和count).</p>
<p><img src="tikv/./dot/agg_impl.svg" alt="" /></p>
<p><code>SimpleAggregationImpl</code> 是没有group by 的，比如下面这种SQL。</p>
<pre><code class="language-sql">select count(*) from table
</code></pre>
<h2 id="simpleaggregationimpl"><a class="header" href="#simpleaggregationimpl">SimpleAggregationImpl</a></h2>
<p>这个没有groupby</p>
<p><img src="tikv/./dot/simple_agg_impl.svg" alt="" /></p>
<h2 id="fasthashaggregationimpl"><a class="header" href="#fasthashaggregationimpl">FastHashAggregationImpl</a></h2>
<p>这个只有一个group by expr</p>
<p><img src="tikv/./dot/fast_hash_aggr.svg" alt="" /></p>
<h2 id="slowhashaggregationimpl"><a class="header" href="#slowhashaggregationimpl">SlowHashAggregationImpl</a></h2>
<p>有多个group by expr</p>
<p><img src="tikv/./dot/slow_hash_agg.svg" alt="" /></p>
<p>假设数据有四列<code>a</code>,<code>b</code>,<code>c</code>,<code>d</code>, 执行</p>
<pre><code class="language-sql">select 
  exp_1(a), exp_2(b), avg(c), sum(d) 
from t 
group by 
  exp_1(a), exp_2(b)
</code></pre>
<p>slow hash agg中相关数据结构关系如下:</p>
<p><img src="tikv/./dot/slow_hash_map.svg" alt="" /></p>
<h2 id="batchstreamaggregationimpl"><a class="header" href="#batchstreamaggregationimpl">BatchStreamAggregationImpl</a></h2>
<p>假定已排好序</p>
<p><img src="tikv/./dot/stream_agg_impl.svg" alt="" /></p>
<p>stream agg中相关数据结构关系如下:</p>
<p><img src="tikv/./dot/stream_agg_struct_relationship.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance"><a class="header" href="#performance">Performance</a></h1>
<p>本地使用tipu 启动了一个cluster, 跑了bench。</p>
<pre><code class="language-bash">tiup bench  tpcc -H 127.0.0.1 -P 4000 -D tpcc --warehouses 10000 run
</code></pre>
<p>然后在TiDB的dashboard上，做了一个profile，下载打开后，tikv对应的profile如下(可以在新的tab页打开下面的svg，看到交互的火焰图)</p>
<p><img src="tikv/./dot/profiling_7_11_tikv_127_0_0_1_20160528561294.svg" alt="" /></p>
<ol>
<li>apply: 20.33%, apply fsm poller, 主要负责将k数据写入到rocksdb.</li>
<li>raft store: 24.15%, peer fsm poller</li>
<li>grpc: 16.45%</li>
<li>sched-worker-pro：20.39%</li>
<li>unified-read-pro: 14.2%</li>
</ol>
<h2 id="batch-system-apply-poller"><a class="header" href="#batch-system-apply-poller">Batch System: Apply Poller</a></h2>
<p>主要负责将k数据写入到rocksdb，主要时间用在了数据flush上。</p>
<p><code>raftstore::store::fsm::apply::ApplyContext&lt;EK,W&gt;::flush</code>上</p>
<p><img src="tikv/./dot/performance_apply.png" alt="" /></p>
<h2 id="batch-system-raft-poller"><a class="header" href="#batch-system-raft-poller">Batch System: Raft Poller</a></h2>
<p>raft store: 24.15%, peer fsm poller, 可以看到其中有将近1/3的时间用在了write_opt上，主要是
write raft state( 比如hard state(term, votefor), raft log等）</p>
<p><img src="tikv/./dot/performance_raftstore.png" alt="" /></p>
<h2 id="storage-事务"><a class="header" href="#storage-事务">Storage: 事务</a></h2>
<p>sched-worker-pro 占总体时间的20.39%</p>
<p>主要时间用在<code>Command:process_write</code>上, 其中MvccReader::seek_write和MvccReader::load_data占据大部分时间.</p>
<p><img src="tikv/./dot/performance_sched.png" alt="" /></p>
<h2 id="coprocessor-2"><a class="header" href="#coprocessor-2">Coprocessor</a></h2>
<ol start="5">
<li>unified-read-pro: 14.2%</li>
</ol>
<p>主要时间用在了Cusor::Seek上</p>
<p><img src="tikv/./dot/performance_unified-read-po.png" alt="" /></p>
<h2 id="grpc-接口"><a class="header" href="#grpc-接口">Grpc 接口</a></h2>
<ol start="3">
<li>grpc: 16.45%</li>
</ol>
<p><img src="tikv/./dot./dot/performance_grpc.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yatp"><a class="header" href="#yatp">yatp</a></h1>
<!-- toc -->
<h2 id="数据结构关系"><a class="header" href="#数据结构关系">数据结构关系</a></h2>
<p><img src="tikv/./dot/yatp_draft.svg" alt="" /></p>
<h2 id="worker-thread"><a class="header" href="#worker-thread">worker thread</a></h2>
<h3 id="run"><a class="header" href="#run">run</a></h3>
<p><code>WorkerThread::run</code> 线程主循环, 不断的去队列中
获取task, handle task。</p>
<p><img src="tikv/./dot/yatp_worker_thread_run.svg" alt="" /></p>
<h2 id="callback-1"><a class="header" href="#callback-1">callback</a></h2>
<h2 id="future"><a class="header" href="#future">future</a></h2>
<h3 id="task-status"><a class="header" href="#task-status">task status</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>const NOTIFIED: u8 = 1;
const IDLE: u8 = 2;
const POLLING: u8 = 3;
const COMPLETED: u8 = 4;
<span class="boring">}
</span></code></pre></pre>
<p>如果<code>task.poll</code>调用了<code>ctx.wake</code>, 并且返回了<code>Pending</code>，就会出现poll结束后，task
的<code>NOTIFIED</code>状态, 这时候可以重新调用<code>task.poll</code>，如果超过了<code>repoll_limit</code>
则会调用<code>wake_task</code>, 把任务放入调度队列中。</p>
<p><img src="tikv/./dot/yatp_task_status.svg" alt="" /></p>
<h3 id="reschedule"><a class="header" href="#reschedule">reschedule</a></h3>
<p>yatp 提供了reschedule, task自己主动让出time slice.</p>
<p>比如下面, while循环中会自己计算自己的time_slice
如果超过了<code>MAX_TIME_SLICE</code>就会调用<code>reschedule.await</code>
并重置<code>time_slice_start</code></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>//src/coprocessor/statistics/analyze.rs#L101
async fn handle_index( //...
//...
let mut time_slice_start = Instant::now();
while let Some((key, _)) = scanner.next()? {
    row_count += 1;
    if row_count &gt;= BATCH_MAX_SIZE {
        if time_slice_start.elapsed() &gt; MAX_TIME_SLICE {
            reschedule().await;
            time_slice_start = Instant::now();
        }
        row_count = 0;
    }
//...
<span class="boring">}
</span></code></pre></pre>
<p><code>reschedule</code> async 定义如下</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Gives up a time slice to the task scheduler.
///
/// It is only guaranteed to work in yatp.
pub async fn reschedule() {
    Reschedule { first_poll: true }.await
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/yatp_reschedule.svg" alt="" /></p>
<h3 id="waker"><a class="header" href="#waker">waker</a></h3>
<p>实现了<code>RawWakerVtable</code>中的几个函数, 最后都会调用<code>wake_task</code> 
将task队列中.</p>
<p>这里面<code>RawWaker.data</code> data指针指向的是<code>task_cell</code></p>
<p><code>task_cell</code>中包含了指向<code>QueueCore</code>的weake指针.</p>
<p><img src="tikv/./dot/yatp_future_waker.svg" alt="" /></p>
<h3 id="wake_task"><a class="header" href="#wake_task">wake_task</a></h3>
<p>如果是在<code>polling</code>中被wake的，可使用
thread 局部变量<code>LOCAL</code> 指针，它指向了worker自己的<code>Local</code>。</p>
<p>该指针由<code>Scope</code>来设置, 在进入<code>future::Runner:handle</code>时
会被设置好,离开该函数时 由<code>Scope</code>的drop函数将LOCAL指针设置为<code>null</code>.</p>
<p><img src="tikv/./dot/yatp_future_wake_task.svg" alt="" /></p>
<h2 id="localqueue"><a class="header" href="#localqueue">LocalQueue</a></h2>
<p><img src="tikv/./dot/yatp_local_queue.svg" alt="" /></p>
<h3 id="singlelevellocalqueue"><a class="header" href="#singlelevellocalqueue">SingleLevel::LocalQueue</a></h3>
<p><img src="tikv/./dot/yatp_local_queue_single_level.svg" alt="" /></p>
<p><code>spawn</code> 和<code>pop</code> task过程</p>
<p>task 的<code>spawn</code>有两处，一处是<code>ThreadPool::spawn</code> 外部线程
来执行async task，另一处是Future等待的事件ready后
调用的ctx.wake 将task 放回到队列中。</p>
<p>在<code>WorkerThread</code>主线程中会不断地去pop task, 先从
自己本地的local queue取task，为空的话，再去
global queue steal 一个batch的task.</p>
<p>如果还是没有的话， 就去别的worker那steal一批task.
最后如果没有可执行的task, 就进入sleep状态.</p>
<p><img src="tikv/./dot/yatp_single_level_spawn.svg" alt="" /></p>
<h3 id="multilevellocalqueue"><a class="header" href="#multilevellocalqueue">Multilevel::LocalQueue</a></h3>
<p>multilevel 为了避免long run的task阻塞了其他async task的执行。
使用了多个Injector level。Injector level越高，injector 会被pop的
优先级(概率）就越低。</p>
<p>在task被reschedule时，会根据task的<code>running_time</code> 放到不同的level injector中。
task运行时越长，被放入的level就越高</p>
<p>默认level0 用于跑小任务，时间在<code>&lt;5ms</code>, level1 是 <code>5ms ~ 100ms</code>, level2是<code>&gt;100ms</code>的</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Default for Config {
    fn default() -&gt; Config {
        Config {
            name: None,
            level_time_threshold: [Duration::from_millis(5), Duration::from_millis(100)],
            level0_proportion_target: 0.8,
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>在worker去Injector获取任务时，会根据一定概率来选择某个level的injector, 计算方式如下:</p>
<p><code>expected_level</code>的计算
先根据<code>level0_chance</code> 概率level0，
然后从level1 ~ Level_NUM -1 依次按照概率 <code>CHANCE_RATIO/(CHANCE_RATIO +1)</code>
选择level.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let expected_level = if rng.gen::&lt;f64&gt;() &lt; level0_chance {
    0
} else {
    (1..LEVEL_NUM - 1)
        .find(|_| rng.gen_ratio(CHANCE_RATIO, CHANCE_RATIO + 1))
        .unwrap_or(LEVEL_NUM - 1)
};

const LEVEL_NUM: usize = 3;
/// The chance ratio of level 1 and level 2 tasks.
const CHANCE_RATIO: u32 = 4;
<span class="boring">}
</span></code></pre></pre>
<p><code>prepare_before_push</code>, 会根据task的running_time, 和每个level的level_time_threshold,
设置task的<code>current_level</code>, 后面task 会push到<code>current_level</code>对应的injector.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let running_time = extras
    .running_time
    .get_or_insert_with(|| self.task_elapsed_map.get_elapsed(task_id));
let running_time = running_time.as_duration();
self.level_time_threshold
    .iter()
    .enumerate()
    .find(|(_, &amp;threshold)| running_time &lt; threshold)
    .map(|(level, _)| level)
    .unwrap_or(LEVEL_NUM - 1) as u8
<span class="boring">}
</span></code></pre></pre>
<p>在每次<code>MultilevelRunner::handle</code> task时，都会更新task的<code>running_time</code></p>
<p>也会每隔一段时间调用<code>maybe_adjust_chance</code> 更新level0_chance.</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;R, T&gt; Runner for MultilevelRunner&lt;R&gt;
    fn handle(&amp;mut self, local: &amp;mut Local&lt;T&gt;, mut task_cell: T) -&gt; bool {
        let extras = task_cell.mut_extras();
        let running_time = extras.running_time.clone();
        //...
        let begin = Instant::now();
        let res = self.inner.handle(local, task_cell);
        let elapsed = begin.elapsed();

        //更新task的runnig_time
        if let Some(running_time) = running_time {
            running_time.inc_by(elapsed);
        }

        //...
        if local_total &gt; FLUSH_LOCAL_THRESHOLD_US {
        //...
        //调整level0_chance
            self.manager.maybe_adjust_chance();
        }

<span class="boring">}
</span></code></pre></pre>
<p><img src="tikv/./dot/yatp_multi_level.svg" alt="" /></p>
<p>multilevel spawn 和pop</p>
<p><img src="tikv/./dot/yatp_multi_level_spawn.svg" alt="" /></p>
<h2 id="参考文献-17"><a class="header" href="#参考文献-17">参考文献</a></h2>
<ol>
<li><a href="https://cfsamson.github.io/books-futures-explained/introduction.html">Futures Explained in 200 Lines of Rust</a></li>
<li><a href="https://tmandry.gitlab.io/blog/posts/optimizing-await-1/">optimizing-await-1</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bevy"><a class="header" href="#bevy">Bevy</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="初次印象"><a class="header" href="#初次印象">初次印象</a></h1>
<!-- toc -->
<h2 id="questions-4"><a class="header" href="#questions-4">Questions</a></h2>
<ul>
<li>texture和camera是咋搞的。</li>
</ul>
<h2 id="bevy-sprite"><a class="header" href="#bevy-sprite">Bevy Sprite</a></h2>
<p>Sprite/Sprite_sheet</p>
<p>了解下bevy的一个sprite是怎么画出来的</p>
<p>sprite的vertex/fragment shader的数据是怎么传过去的？</p>
<h3 id="shader-defs"><a class="header" href="#shader-defs">shader defs</a></h3>
<p>先初始化vertex shader和fragment shader</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let pipeline_handle = pipelines.add(PipelineDescriptor::default_config(ShaderStages {
        vertex: shaders.add(Shader::from_glsl(ShaderStage::Vertex, VERTEX_SHADER)),
        fragment: Some(shaders.add(Shader::from_glsl(ShaderStage::Fragment, FRAGMENT_SHADER))),
    }));
<span class="boring">}
</span></code></pre></pre>
<p>PipelineSpecialization</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>  commands
        .spawn(MeshComponents {
            mesh: cube_handle,
            render_pipelines: RenderPipelines::from_pipelines(vec![RenderPipeline::specialized(
                pipeline_handle,
                // NOTE: in the future you wont need to manually declare dynamic bindings
                PipelineSpecialization {
                    dynamic_bindings: vec![
                        // Transform
                        DynamicBinding {
                            bind_group: 1,
                            binding: 0,
                        },
                        // MyMaterial_color
                        DynamicBinding {
                            bind_group: 1,
                            binding: 1,
                        },
                    ],
                    ..Default::default()
                },
            )]),
            transform: Transform::from_translation(Vec3::new(-2.0, 0.0, 0.0)),
            ..Default::default()
<span class="boring">}
</span></code></pre></pre>
<p>RenderGraph的概念是咋样的</p>
<p>自动derive的RenderResources和ShaderDefs是啥</p>
<p>shadersource 貌似用的是Spirv</p>
<p>compile_pipeline -&gt; compile_shader</p>
<p>reflect_layout 貌似这块是上传数据到shader里面的。</p>
<p>draw.rs中的set_bind_groups_from_bindings </p>
<pre><code class="language-dot">set_bind_group -&gt; render_command 
set_index_buffer
set_vertex_buffer
SetBindGroup
set_vertex_buffers_from_bindings -&gt; set_vertex_buffer
</code></pre>
<p>ColorMaterial 包含了color和texture</p>
<h3 id="rendergraph"><a class="header" href="#rendergraph">RenderGraph</a></h3>
<p>RenderGraph 是干啥的，类似于blender中的matrial node吗？</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct NodeState {
    pub id: NodeId,
    pub name: Option&lt;Cow&lt;'static, str&gt;&gt;,
    pub node: Box&lt;dyn Node&gt;,
    pub input_slots: ResourceSlots,
    pub output_slots: ResourceSlots,
    pub edges: Edges,
}
<span class="boring">}
</span></code></pre></pre>
<p>Slot如下</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Default, Debug, Clone)]
pub struct ResourceSlots {
    slots: Vec&lt;ResourceSlot&gt;,
}

#[derive(Debug, Clone)]
pub struct ResourceSlot {
    pub resource: Option&lt;RenderResourceId&gt;,
    pub info: ResourceSlotInfo,
}

#[derive(Clone, Debug)]
pub struct ResourceSlotInfo {
    pub name: Cow&lt;'static, str&gt;,
    pub resource_type: RenderResourceType,
}

#[derive(Debug, Clone, Eq, PartialEq)]
pub enum RenderResourceType {
    Buffer,
    Texture,
    Sampler,
}
<span class="boring">}
</span></code></pre></pre>
<p>SpriteRenderGraphBuilder
add_sprite_graph -&gt; build_sprite_pipeline;</p>
<h3 id="render-graph"><a class="header" href="#render-graph">Render Graph？</a></h3>
<p>https://ourmachinery.com/post/high-level-rendering-using-render-graphs/</p>
<h3 id="render-pipeline"><a class="header" href="#render-pipeline">Render Pipeline</a></h3>
<p>在stage DRAW 阶段生成所有的RenderCommand,放入render_commands vec,  然后在stage Render阶段，遍历它，
执行这个render commands.</p>
<p><img src="bevy/./dot/pipline.svg" alt="render pipeline" /></p>
<h3 id="render-nodes"><a class="header" href="#render-nodes">Render Nodes</a></h3>
<ul>
<li>PassNode</li>
<li>CameraNode</li>
<li>RenderResourcesNode</li>
<li>AssetRenderResourcesNode;</li>
</ul>
<h4 id="renderresourcesnode"><a class="header" href="#renderresourcesnode">RenderResourcesNode</a></h4>
<p>RenderResourcesNode 负责绑定uniform</p>
<p><img src="bevy/./dot/render-resource-node.svg" alt="" /></p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait RenderResource {
    fn resource_type(&amp;self) -&gt; Option&lt;RenderResourceType&gt;;
    fn write_buffer_bytes(&amp;self, buffer: &amp;mut [u8]);
    fn buffer_byte_len(&amp;self) -&gt; Option&lt;usize&gt;;
    // TODO: consider making these panic by default, but return non-options
    fn texture(&amp;self) -&gt; Option&lt;Handle&lt;Texture&gt;&gt;;
}

pub trait RenderResources: Send + Sync + 'static {
    fn render_resources_len(&amp;self) -&gt; usize;
    fn get_render_resource(&amp;self, index: usize) -&gt; Option&lt;&amp;dyn RenderResource&gt;;
    fn get_render_resource_name(&amp;self, index: usize) -&gt; Option&lt;&amp;str&gt;;
    fn get_render_resource_hints(&amp;self, _index: usize) -&gt; Option&lt;RenderResourceHints&gt; {
        None
    }
    fn iter(&amp;self) -&gt; RenderResourceIterator;
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="derive-renderresources"><a class="header" href="#derive-renderresources">derive RenderResources</a></h4>
<p>derive 自动实现RenderResources接口</p>
<h5 id="sprite"><a class="header" href="#sprite">Sprite</a></h5>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Default, RenderResources)]
pub struct Sprite {
    pub size: Vec2,
    #[render_resources(ignore)]
    pub resize_mode: SpriteResizeMode,
}
<span class="boring">}
</span></code></pre></pre>
<p>在sprite.vert中定义该uniform</p>
<pre><code class="language-glsl">layout(set = 2, binding = 1) uniform Sprite_size {
    vec2 size;
};
</code></pre>
<h5 id="colormaterial"><a class="header" href="#colormaterial">ColorMaterial</a></h5>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, RenderResources, ShaderDefs)]
pub struct ColorMaterial {
    pub color: Color,
    #[shader_def]
    pub texture: Option&lt;Handle&lt;Texture&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<pre><code class="language-glsl">layout(set = 1, binding = 0) uniform ColorMaterial_color {
    vec4 Color;
};

# ifdef COLORMATERIAL_TEXTURE 
layout(set = 1, binding = 1) uniform texture2D ColorMaterial_texture;
layout(set = 1, binding = 2) uniform sampler ColorMaterial_texture_sampler;
# endif
</code></pre>
<h3 id="texturecopynode"><a class="header" href="#texturecopynode">TextureCopyNode</a></h3>
<p>AssetEvent是由谁来emit?</p>
<p>监听AssetEvent，创建texture 在RendererContext中buffer</p>
<h2 id="events"><a class="header" href="#events">Events</a></h2>
<h2 id="texture"><a class="header" href="#texture">Texture</a></h2>
<p>texture 一个vec u8数据 + size + 数据格式</p>
<ol>
<li>texture是怎么和shader中的buffer关联起来的?</li>
<li>texture的数据是怎么实现hot reload的？</li>
</ol>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Texture {
    pub data: Vec&lt;u8&gt;,
    pub size: Vec2,
    pub format: TextureFormat,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="texture_resource_system"><a class="header" href="#texture_resource_system">texture_resource_system</a></h3>
<p>texture_resource_system 会轮询AssetEvent, 处理AssetEvent::Created/Modified/Remove等事件</p>
<p><img src="bevy/./dot/asset-texture.svg" alt="" /></p>
<p>Handle是啥？</p>
<h2 id="asset"><a class="header" href="#asset">Asset</a></h2>
<p>AssetServer load相关函数，返回的都是Handle, 由channelAssetHandler加载资源，
加载完毕后，放入channel中，然后update_asset_storage_system会去设置全局
Assets和更新AssetServer中的load状态。
<img src="bevy/./dot/asset.svg" alt="" /></p>
<h2 id="pbr"><a class="header" href="#pbr">PBR</a></h2>
<h2 id="ecs"><a class="header" href="#ecs">ECS</a></h2>
<p>Entiy, component, system
Resources 资源</p>
<p>thread local 或者global的</p>
<pre><pre class="playground"><code class="language-rust edition2021">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Resources {
    pub(crate) resource_data: HashMap&lt;TypeId, ResourceData&gt;,
    thread_local_data: HashMap&lt;TypeId, Box&lt;dyn ResourceStorage&gt;&gt;,
    main_thread_id: ThreadId,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blender-学习笔记"><a class="header" href="#blender-学习笔记">Blender 学习笔记</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blender-manual-notes"><a class="header" href="#blender-manual-notes">Blender Manual notes</a></h1>
<!-- toc -->
<h2 id="user-interface"><a class="header" href="#user-interface">User Interface</a></h2>
<p>Blender 界面分为三块</p>
<ul>
<li>TopBar 主要是菜单和workspace的tab</li>
<li>Areas: 中间的是Areas，由各种Editor组成</li>
<li>StatusBar: 最下面, 记录鼠标，键盘等状态</li>
</ul>
<p><img src="blender/./images/interface_window-system_introduction_layout-workspace.png" alt="" /></p>
<h3 id="statusbar"><a class="header" href="#statusbar">StatusBar</a></h3>
<p>左边记录了keymap information, 右边记录了Resource Information.</p>
<p><img src="blender/./images/interface_window-system_status-bar.png" alt="" /></p>
<h3 id="workspace"><a class="header" href="#workspace">WorkSpace</a></h3>
<p>WorkSpace, 将几个editor组合起来，便于完成建模，动画等工作，</p>
<ul>
<li>Modeling:	For modification of geometry by modeling tools.</li>
<li>Sculpting:	For modification of meshes by sculpting tools.</li>
<li>UV Editing:	Mapping of image texture coordinates to 3D surfaces.</li>
<li>Texture Paint:	Tools for coloring image textures in the 3D View.</li>
<li>Shading:	Tools for specifying material properties for rendering.</li>
<li>Animation:	Tools for making properties of objects dependent on time.</li>
<li>Rendering:	For viewing and analyzing rendering results.</li>
<li>Compositing:	Combining and post-processing of images and rendering information.</li>
<li>Scripting:	Programming workspace for writing scripts.</li>
</ul>
<h3 id="areas"><a class="header" href="#areas">Areas</a></h3>
<p><img src="blender/./images/interface_window-system_areas_borders.png" alt="" /></p>
<h4 id="split-area"><a class="header" href="#split-area">Split Area</a></h4>
<p>鼠标放在Area左上角，出现+号时候，按住鼠标左键拖动(方向如下图所示）</p>
<p><img src="blender/./images/interface_window-system_areas_split.png" alt="" /></p>
<h4 id="join-area"><a class="header" href="#join-area">Join Area</a></h4>
<p>鼠标放在Area左上角，出现+号时候，按住鼠标左键拖动(方向和Split的相反)
<img src="blender/./images/interface_window-system_areas_join.png" alt="" /></p>
<h4 id="swap-area"><a class="header" href="#swap-area">Swap Area</a></h4>
<p>两个area交换空间，鼠标放在area左上角，然后按住ctl + LMB 从src area拖动到target area</p>
<h3 id="regions"><a class="header" href="#regions">Regions</a></h3>
<p>blender 的每个editor包含不同的region.</p>
<p>一般Region有</p>
<ul>
<li>Header </li>
<li>ToolBar(快捷键T), 在编辑器左边, 当前active tool的设置</li>
<li>SideBar(快捷键N)，在编辑器右边, editor中Object的settings和editor自身的settings</li>
</ul>
<p><img src="blender/./images/interface_window-system_regions_3d-view.png" alt="" /></p>
<h3 id="common-shortcuts"><a class="header" href="#common-shortcuts">Common shortcuts</a></h3>
<h2 id="ref-8"><a class="header" href="#ref-8">Ref</a></h2>
<ol>
<li>https://docs.blender.org/manual/en/2.80/index.html</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python"><a class="header" href="#python">python</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="records"><a class="header" href="#records">records</a></h1>
<p>[github records]https://github.com/kennethreitz-archive/records</p>
<pre><code class="language-python">import records

db = records.Database('postgres://...')
rows = db.query('select * from active_users')    # or db.query_file('sqls/active-users.sql')
</code></pre>
<p><img src="python/records/./dot/records.svg" alt="records" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="react"><a class="header" href="#react">react</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h2 id="react中state-render到html-dom的流程分析"><a class="header" href="#react中state-render到html-dom的流程分析">React中state render到html dom的流程分析</a></h2>
<h3 id="questions-5"><a class="header" href="#questions-5">Questions</a></h3>
<ol>
<li>React的component的lifecycle 在react中是怎么被调到的.</li>
<li>分析jsx =&gt; element tree =&gt; fiber tree =&gt; html dom在react中的流程.</li>
<li>react中的fiber tree的建立和执行, 以及异步的schedule.</li>
</ol>
<p><img src="react/./images/react-questions.jpeg" alt="react-questions" /></p>
<h3 id="研究工具和方法"><a class="header" href="#研究工具和方法">研究工具和方法</a></h3>
<ul>
<li>chrome debug 打断点</li>
<li><a href="https://github.com/ggreer/the_silver_searcher">ag the silver searcher</a>, 源代码全局搜索.</li>
<li>猜测它的实现原理，打log, call trace验证, console.log, console.trace;</li>
</ul>
<h3 id="准备工作"><a class="header" href="#准备工作">准备工作</a></h3>
<p>代码下载,编译</p>
<pre><code class="language-bash">$ git clone git@github.com:facebook/react.git
$ cd react
$ yarn install
$ gulp react:extract-errors
$ yarn build
</code></pre>
<h3 id="component-lifecycle-callback"><a class="header" href="#component-lifecycle-callback">Component lifeCycle callback</a></h3>
<p>准备最简单的组件HelloWorld</p>
<pre><code class="language-jsx">import React from &quot;react&quot;
import ReactDom from &quot;react-dom&quot;

class HelloWorld extends React.Component{
    constructor(props){
        super(props);
        this.state = {
            message: &quot;hello, world&quot;
        }
    }

    componentWillMount(){
        console.log(&quot;component will mount&quot;);
    }

    componentWillUpdate(){
        console.log(&quot;component will update&quot;);
    }

    componentDidUpdate(){
        console.log(&quot;component did update&quot;);
    }

    componentDidMount(){
        console.log(&quot;componentDidMount&quot;);
    }

    render(){
        return &lt;span className={this.state.message}&gt;
            {this.state.message}
        &lt;/span&gt;;
    }
}
ReactDom.render(&lt;HelloWorld/&gt;, document.getElementById(&quot;app&quot;));
</code></pre>
<p>在<code>componentWillMount</code>, <code>componentDidMount</code>, <code>componentWillUpdate</code>, <code>componentDidUpdate</code>中打个断点</p>
<p><img src="react/./images/react-component-life-cycle-callback.jpeg" alt="" /></p>
<h3 id="创建html-dom的callstack"><a class="header" href="#创建html-dom的callstack">创建html dom的callstack</a></h3>
<p>react中最后一定会去调用<code>document.createElement</code>去创建html的dom节点，所以把document.createElement这个方法覆盖了，加了一层log.</p>
<pre><code class="language-javascript">var originCreateElement = document.createElement;
document.createElement = function() {
    if (arguments[0] === 'span'){
        console.log('create span');
    }
   return originCreateElement.apply(document, arguments);
}
</code></pre>
<p>然后打断点，得到的callstack如下:</p>
<p><img src="react/./images/react-create-dom.jpeg" alt="" /></p>
<h3 id="call-flow-整理"><a class="header" href="#call-flow-整理">call flow 整理</a></h3>
<p>函数间的callflow 整理如下
<img src="react/./images/react-lifecycle-call-flow.jpeg" alt="" /></p>
<p>函数所属模块之间的call flow 整理如下</p>
<p><img src="react/./images/react-module-call-flow.jpeg" alt="" /></p>
<h2 id="fiber"><a class="header" href="#fiber">Fiber</a></h2>
<h4 id="fiber的设计思想"><a class="header" href="#fiber的设计思想">fiber的设计思想</a></h4>
<p>在<a href="https://github.com/acdlite/react-fiber-architecture">react-fiber-artchitecture</a> 中作者描述了fiber的设计思想，简单来说，每个fiber就是一个执行单元，可以任意的修改它的优先级，可以pause 它，之后再继续执行（感觉很像进程线程的概念）。</p>
<p>实际中执行一个fiber可以生成下一步要执行的fiber，然后fiber执行之前可以检查时候js跑的时间时候用完了，如果用完了，就挂起来，等待下次requestIdleCallback/requestAnimationFrame的callback, schedule 开始接着上次结束的地方继续执行js code.</p>
<p>相当于把以前的js function 的call stack 改成fiber chain了。</p>
<p><img src="react/./images/fiber-flow.jpeg" alt="" /></p>
<p><code>workLoop</code> 函数主要逻辑如下（注，删除了错误处理和其他不相干的<code>if else</code> 分支)
performWork</p>
<pre><code class="language-javascript">// ReactScheduler.js workLoop
if (deadline !== null &amp;&amp; priorityLevel &gt; TaskPriority) {
      // The deferred work loop will run until there's no time left in
      // the current frame.
      while (nextUnitOfWork !== null &amp;&amp; !deadlineHasExpired) {
        if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
          nextUnitOfWork = performUnitOfWork(nextUnitOfWork);
          if (nextUnitOfWork === null &amp;&amp; pendingCommit !== null) {
           // If we have time, we should commit the work now.
           if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
             commitAllWork(pendingCommit);
             nextUnitOfWork = findNextUnitOfWork();
             // Clear any errors that were scheduled during the commit phase.
           }
         }
       }
   }
  }
</code></pre>
<h4 id="schedule-1"><a class="header" href="#schedule-1">schedule</a></h4>
<p>schedule 有同步和异步的，同步的会一直执行，直到fiber tree被执行结束，不会去检查time限制和priorityLevel的问题，异步的有两类权限，一个是animation的，一类是HighPriority, OffScreen Priority这个会有个deadline.</p>
<p><img src="react/./images/schedule-update.jpeg" alt="schedule-update" /></p>
<p>在preformwork的末尾会去检查nextLevelPriority的优先权，然后根据优先权异步的schedule.</p>
<pre><code class="language-javascript">switch (nextPriorityLevel) {
      case SynchronousPriority:
      case TaskPriority:
        // Perform work immediately by switching the priority level
        // and continuing the loop.
        priorityLevel = nextPriorityLevel;
        break;
      case AnimationPriority:
        scheduleAnimationCallback(performAnimationWork);
        // Even though the next unit of work has animation priority, there
        // may still be deferred work left over as well. I think this is
        // only important for unit tests. In a real app, a deferred callback
        // would be scheduled during the next animation frame.
        scheduleDeferredCallback(performDeferredWork);
        break;
      case HighPriority:
      case LowPriority:
      case OffscreenPriority:
        scheduleDeferredCallback(performDeferredWork);
        break;
    }
</code></pre>
<h4 id="fiber类型"><a class="header" href="#fiber类型">fiber类型</a></h4>
<p>FunctionalComponent, ClassComponent 对应着用户创建的Component, HostRoot, HostComponent, HostPortal, HostText这些是和平台相关的组件。对于web来说就是 div, span这些dom元素了。</p>
<pre><code class="language-javascript">// ReactTypeOfWork.js
module.exports = {
  IndeterminateComponent: 0, // Before we know whether it is functional or class
  FunctionalComponent: 1,
  ClassComponent: 2,
  HostRoot: 3, // Root of a host tree. Could be nested inside another node.
  HostPortal: 4, // A subtree. Could be an entry point to a different renderer.
  HostComponent: 5,
  HostText: 6,
  CoroutineComponent: 7,
  CoroutineHandlerPhase: 8,
  YieldComponent: 9,
  Fragment: 10,
};
</code></pre>
<h3 id="fiber执行的三个阶段"><a class="header" href="#fiber执行的三个阶段">fiber执行的三个阶段</a></h3>
<p><code>react</code>中的<code>fiber</code>执行的执行主要分为三个阶段</p>
<ol>
<li>
<p><code>beginWork</code>: fiber展开（把ClassComponent render开来，最后展开到fiber tree的叶子节点都是hostComponent)</p>
</li>
<li>
<p><code>completeWork</code>: 计算fiber之间的diff, 底层的dom元素的创建，以及dom tree的建立，还有事件绑定。</p>
</li>
<li>
<p><code>commitWork</code>: 调用host接口，把fiber的diff更新到host上去</p>
</li>
</ol>
<h4 id="begin-work-fiber-tree-的展开"><a class="header" href="#begin-work-fiber-tree-的展开">begin work: fiber tree 的展开</a></h4>
<p>每次的beginWork(fiber), 会把fiber的所有直接子节点展开（这里只展开一层, 不会递归的去展开子节点的子节点）</p>
<pre><code class="language-javascript">function performUnitOfWork(workInProgress: Fiber): Fiber | null {
   const current = workInProgress.alternate;
   let next = beginWork(current, workInProgress, nextPriorityLevel);

   if (next === null) {
     next = completeUnitOfWork(workInProgress);
   }
   return next;
 }
</code></pre>
<p>在workloop里面会把beginWork创建的子节点接着传给beginWork，继续展开fiber tree</p>
<pre><code class="language-javascript">//workLoop
while (nextUnitOfWork !== null &amp;&amp; !deadlineHasExpired) {
       if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
         nextUnitOfWork = performUnitOfWork(nextUnitOfWork);
</code></pre>
<p><img src="react/./images/begin-work-create-fiber.jpeg" alt="" /></p>
<h4 id="completework-创建dom元素计算diff"><a class="header" href="#completework-创建dom元素计算diff">completeWork 创建dom元素，计算diff</a></h4>
<p>创建的<code>instance</code>(对于html来说，就是dom节点), 存储在<code>workInProgress.stateNode</code> 里面, 计算好的props diff存放在了<code>workInProgress.updateQueue</code>，在下一个阶段commitWork 会把这个updateQueue里面的patch提交到host。</p>
<p><img src="react/./images/completework-flow.jpeg" alt="" /></p>
<h4 id="commitwork-提交diff"><a class="header" href="#commitwork-提交diff">commitWork 提交diff</a></h4>
<p>在commitUpdate中取WorkInprogress.updateQueue,然后调用Dom操作把diff apply上去</p>
<p><img src="react/./images/commit-work.jpeg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="godot"><a class="header" href="#godot">Godot</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="godot-学习笔记"><a class="header" href="#godot-学习笔记">godot 学习笔记</a></h1>
<!-- toc -->
<h2 id="node-tree"><a class="header" href="#node-tree">node tree</a></h2>
<ol>
<li>在tree中怎么快速定位到某个Node? 并转换为相应类型？</li>
<li>node之间怎么互相调用？</li>
<li>scene之间的过渡场景怎么搞？</li>
<li>目前有哪些node 各自负责干啥？</li>
</ol>
<p><img src="godot/./node-tree.svg" alt="node" /></p>
<h2 id="node2d"><a class="header" href="#node2d">Node2D</a></h2>
<p><img src="godot/./node-type.svg" alt="node type" /></p>
<h2 id="node-虚函数"><a class="header" href="#node-虚函数">Node 虚函数</a></h2>
<p>Rust中没有虚函数，是咋搞的</p>
<pre><code class="language-c#">public override void _EnterTree()
{
    // When the node enters the Scene Tree, it becomes active
    // and  this function is called. Children nodes have not entered
    // the active scene yet. In general, it's better to use _ready()
    // for most cases.
    base._EnterTree();
}

public override void _Ready()
{
    // This function is called after _enter_tree, but it ensures
    // that all children nodes have also entered the Scene Tree,
    // and became active.
    base._Ready();
}

public override void _ExitTree()
{
    // When the node exits the Scene Tree, this function is called.
    // Children nodes have all exited the Scene Tree at this point
    // and all became inactive.
    base._ExitTree();
}

public override void _Process(float delta)
{
    // This function is called every frame.
    base._Process(delta);
}

public override void _PhysicsProcess(float delta)
{
    // This is called every physics frame.
    base._PhysicsProcess(delta);
}
</code></pre>
<p><img src="godot/./node-callback.svg" alt="node callback" /></p>
<h2 id="instance-scene"><a class="header" href="#instance-scene">Instance Scene</a></h2>
<p>先load scene, 然后将scene instance为node，可以放在场景里面</p>
<pre><code>var scene = GD.Load&lt;PackedScene&gt;(&quot;res://myscene.tscn&quot;); // Will load when the script is instanced.

//preload
var scene = preload(&quot;res://myscene.tscn&quot;) # Will load when parsing the script.

//instance
var node = scene.Instance();
AddChild(node);
</code></pre>
<h2 id="signal"><a class="header" href="#signal">Signal</a></h2>
<p>可以在editor中connect. 也可以在代码中connect 信号和handler </p>
<p>带参数的Signal</p>
<pre><code>extends Node

signal my_signal(value, other_value)

func _ready():
    emit_signal(&quot;my_signal&quot;, true, 42)
</code></pre>
<h3 id="connect-signal"><a class="header" href="#connect-signal">Connect signal</a></h3>
<pre><code>// &lt;source_node&gt;.connect(&lt;signal_name&gt;, &lt;target_node&gt;, &lt;target_function_name&gt;)
extends Node2D


func _ready():
    $Timer.connect(&quot;timeout&quot;, self, &quot;_on_Timer_timeout&quot;)


func _on_Timer_timeout():
    $Sprite.visible = !$Sprite.visible
</code></pre>
<h3 id="emit-signal"><a class="header" href="#emit-signal">Emit signal</a></h3>
<p>定义和发射signal</p>
<pre><code>extends Node2D


signal my_signal


func _ready():
    emit_signal(&quot;my_signal&quot;)
</code></pre>
<h3 id="animatedsprite"><a class="header" href="#animatedsprite">AnimatedSprite</a></h3>
<p><img src="godot/./dot/animated-sprite.svg" alt="" /></p>
<h2 id="kinematicbody2d"><a class="header" href="#kinematicbody2d">KinematicBody2D</a></h2>
<p>控制角色运动和碰撞检测， 和RigionBody2D有什么区别？</p>
<ol>
<li>CollisionShape2D: 碰撞检测, Geometric Shape: New RectTangeShape2D</li>
<li>Modulation: 调制，在inspector中可以改变collisionShape的颜色, 在debug模式比较有用.</li>
<li>Sprite, render顺序，从上到下，下面的覆盖上面的.</li>
<li>snaping feature: 用于精确放置图片到0点, snap to grid, 快捷键G Snap to pixel</li>
<li>collision shape比sprite稍微小一点.</li>
<li>Dector Monitorble是干啥的, 为啥要把Dector的physical Layer去掉？这样就不会发生碰撞检测了吗？只会做dector?</li>
<li>stampdector检测的时候，比较global_y 来判断player是否不是在头顶</li>
<li>player身上的enemyDector，jump更高一些, onAreaEnter和onBodyEnter有啥区别啊</li>
</ol>
<pre><code>src/Actors/Player.tscn
</code></pre>
<pre><code>func _physic_process(delta: float) -&gt; void:
  velocity.y = gravity * delta;
  velocity.y = max(velocity.y, speed.y)
  velocity = move_and_slide(velocity)

</code></pre>
<p>is_on_wall
is_on_floor</p>
<p>Actor.gd, player和enemy共享公用的代码</p>
<h2 id="tilemap"><a class="header" href="#tilemap">TileMap</a></h2>
<p>tileset.tres 这个制作细节需要去学习下</p>
<p>Cell collision, 给每个cell添加collsion，这个画完tilemap之后，自动就有了collision.
snapOptions: Step</p>
<p>CellSize</p>
<p>可以直接将Player drag到tilemap里面</p>
<p>physics layers
masks: 需要检测碰撞的layer</p>
<p>Input Mapping</p>
<p>get_action_strength</p>
<h2 id="camera2d控制"><a class="header" href="#camera2d控制">Camera2D控制</a></h2>
<p>Camera Limit: Top, left, Right, Bottom, Smoothed
Drag Margin, H,V enable</p>
<h3 id="leveltemplate"><a class="header" href="#leveltemplate">LevelTemplate</a></h3>
<p>rules: pixels, shiyong rules来衡量位置，然后修改camera的limit</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>TextureRectangle, Layout, FullRect</p>
<p>CanvasLayer-&gt; Background, Layer -100, </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="make-your-first-2d-game-with-godot"><a class="header" href="#make-your-first-2d-game-with-godot">Make Your First 2D Game With Godot</a></h1>
<!-- toc -->
<h2 id="part-1"><a class="header" href="#part-1">Part 1</a></h2>
<h3 id="kinematicbody2d-1"><a class="header" href="#kinematicbody2d-1">KinematicBody2D</a></h3>
<p>控制角色运动和碰撞检测， 和RigionBody2D有什么区别？</p>
<ol>
<li>CollisionShape2D: 碰撞检测, Geometric Shape: New RectTangeShape2D</li>
<li>Modulation: 调制，在inspector中可以改变collisionShape的颜色, 在debug模式比较有用.</li>
<li>Sprite, render顺序，从上到下，下面的覆盖上面的.</li>
<li>snaping feature: 用于精确放置图片到0点, snap to grid, 快捷键G Snap to pixel</li>
<li>collision shape比sprite稍微小一点.</li>
<li>Dector Monitorble是干啥的, 为啥要把Dector的physical Layer去掉？这样就不会发生碰撞检测了吗？只会做dector?</li>
<li>stampdector检测的时候，比较global_y 来判断player是否不是在头顶</li>
<li>player身上的enemyDector，jump更高一些, onAreaEnter和onBodyEnter有啥区别啊</li>
</ol>
<pre><code>src/Actors/Player.tscn
</code></pre>
<pre><code>func _physic_process(delta: float) -&gt; void:
  velocity.y = gravity * delta;
  velocity.y = max(velocity.y, speed.y)
  velocity = move_and_slide(velocity)

</code></pre>
<p>is_on_wall
is_on_floor</p>
<p>Actor.gd, player和enemy共享公用的代码</p>
<h3 id="tilemap-1"><a class="header" href="#tilemap-1">TileMap</a></h3>
<p>tileset.tres 这个制作细节需要去学习下</p>
<p>Cell collision, 给每个cell添加collsion，这个画完tilemap之后，自动就有了collision.
snapOptions: Step</p>
<p>CellSize</p>
<p>可以直接将Player drag到tilemap里面</p>
<p>physics layers
masks: 需要检测碰撞的layer</p>
<p>Input Mapping</p>
<p>get_action_strength</p>
<h3 id="camera2d控制-1"><a class="header" href="#camera2d控制-1">Camera2D控制</a></h3>
<p>Camera Limit: Top, left, Right, Bottom, Smoothed
Drag Margin, H,V enable</p>
<h3 id="leveltemplate-1"><a class="header" href="#leveltemplate-1">LevelTemplate</a></h3>
<p>rules: pixels, shiyong rules来衡量位置，然后修改camera的limit</p>
<h3 id="background-1"><a class="header" href="#background-1">Background</a></h3>
<p>TextureRectangle, Layout, FullRect</p>
<p>CanvasLayer-&gt; Background, Layer -100, 
使用canvaslayer作为背景，这样背景图就这一直在了</p>
<h2 id="part2-coins-portals-and-levels"><a class="header" href="#part2-coins-portals-and-levels">Part2: Coins, Portals and levels</a></h2>
<h3 id="coins"><a class="header" href="#coins">Coins</a></h3>
<h4 id="animationplayer"><a class="header" href="#animationplayer">AnimationPlayer</a></h4>
<h5 id="制作coin的bouncing动画"><a class="header" href="#制作coin的bouncing动画">制作Coin的bouncing动画</a></h5>
<p>length: 1.8s Add TracK
automatic key insert, 
uncheck rot </p>
<p>AnimationTrackKeyEdit: Easing, 插值, 修改曲线
shirtf + D： player animation</p>
<p>AutoPlay on Load</p>
<p>coin.position</p>
<h5 id="制作coin的fadeout动画"><a class="header" href="#制作coin的fadeout动画">制作Coin的Fadeout动画</a></h5>
<p>修改CanvasItem/Visibility/Modulate的颜色, colorRange, 修改alpha值</p>
<p>在boucing动画中需要: Reset Modulate color</p>
<h4 id="callmethodtrack"><a class="header" href="#callmethodtrack">CallMethodTrack</a></h4>
<p>在动画结束的时候，调用node的某个方法，比如fadeout动画结束了，call node的queueFree方法</p>
<h3 id="portals-下一个关卡的入口"><a class="header" href="#portals-下一个关卡的入口">Portals: 下一个关卡的入口</a></h3>
<p>CapsuleShape2D</p>
<p>TransitionLayer: CanvasLayer, ColorRectangle</p>
<p>canvas layer的行为和普通node不一样？render的时候.</p>
<h4 id="fade-in-animation"><a class="header" href="#fade-in-animation">fade in animation</a></h4>
<p>animation player, canvaslayer ,visible 选择这个对性能影响比较大</p>
<h2 id="gui-menuspausescore"><a class="header" href="#gui-menuspausescore">GUI: Menus/Pause/Score</a></h2>
<p>代码目录结构</p>
<ul>
<li>Actors: 放player, enemies</li>
<li>Levels: 各种关卡</li>
<li>Objects: coins, portal 之类的小道具</li>
<li>UserInterface: Menu/Title</li>
<li>Screens: dialog</li>
</ul>
<p>add node dialog中，green icon是gui node</p>
<p>control node: anchor/margin/rect/hint/focus etc.
ProjectSettings/MainScene/</p>
<h3 id="background-2"><a class="header" href="#background-2">Background</a></h3>
<p>Background: TextureRectangle, resizable texture for intereface.</p>
<p>背景和gui  Layout: FullRect, backgroud fits the parent</p>
<p>Label: basic text box, layout: centerTop</p>
<h3 id="vboxcontainer"><a class="header" href="#vboxcontainer">VBoxContainer</a></h3>
<p>Container: 将两个button放到container中, VBoxContainer</p>
<p>Button: Play,quit</p>
<p>Text, Ctl+D 复制一个button
同事选中buttons,然后在inspector中修改： Size Flags: Horizontal/Vectical Expand</p>
<p>两个button填充满VboxContainer</p>
<p>save branch as sencne, 将tree中部分node 保存成scene.</p>
<h3 id="font"><a class="header" href="#font">Font</a></h3>
<p>Theme: apply一个theme到node时候，所有的child都是用那个theme</p>
<p>在theme中修改font</p>
<p>DynamicFont, otf 文件拖到font data, 修改size</p>
<p>customerFonts, </p>
<p>get_configuration_warning</p>
<p>ChangeSecneButton: reuseable button</p>
<p>MainScreen/EndScreen/PauseScreen</p>
<h4 id="playerdata"><a class="header" href="#playerdata">PlayerData</a></h4>
<p>PlayerData相当于gameState, AutoLoad 单例模式</p>
<p>rest
set_score 之后发送一个singal score_updated</p>
<p>AutoLoad在ready中可以用</p>
<h4 id="pause"><a class="header" href="#pause">Pause</a></h4>
<p>score:%s</p>
<p>retrybutton: get_tree().reload_current_scene() 重新加载当前scene</p>
<p>game_tree().paused = false, 使用这个来pause, pause时候所有东西都pause了</p>
<p>Inspector: Node/PauseMode: process 这样这个node就不会被pause了</p>
<p>get_tree().set_input_as_handled(); stop event的传播</p>
<p>player die的时候，就显示pause</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka"><a class="header" href="#kafka">Kafka</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-client-producer"><a class="header" href="#kafka-client-producer">Kafka client: producer</a></h1>
<p>producer client端发送消息过程</p>
<p><img src="kafka/./client-producer.svg" alt="client-producer" /></p>
<p>更新元数据过程: updateMetadata</p>
<p><img src="kafka/./client-producer-udatemetadata.svg" alt="client-update-metadata" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-groupcoordinator"><a class="header" href="#kafka-groupcoordinator">Kafka GroupCoordinator</a></h1>
<p>GroupCoordinator handles general group membership and offset management.</p>
<h2 id="consumergroup"><a class="header" href="#consumergroup">ConsumerGroup</a></h2>
<p>consumer group是kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费</p>
<ol>
<li>consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程</li>
<li>group.id是一个字符串，唯一标识一个consumer group</li>
<li>consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer(当然该分区还可以被分配给其他group)</li>
</ol>
<p><code>__consumer_offsets</code> 中的消息保存了每个consumer group某一时刻提交的offset信息。 这个key是consumer-group-id-topic-partition- 这样？
谁来提交offsets?</p>
<p>group与coordinator共同使用它来完成group的rebalance。目前kafka提供了5个协议来处理与consumer group coordination相关的问题：</p>
<ol>
<li>Heartbeat请求：consumer需要定期给coordinator发送心跳来表明自己还活着</li>
<li>JoinGroup请求：成员请求加入组</li>
<li>LeaveGroup请求：主动告诉coordinator我要离开consumer group</li>
<li>SyncGroup请求：group leader把分配方案告诉组内所有成员</li>
<li>DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等。通常该请求是给管理员使用</li>
</ol>
<h2 id="joinleave-group"><a class="header" href="#joinleave-group">join/leave group</a></h2>
<p>reblance的时候发生了啥？parition 和consumer之间的分配？？谁负责把partition给各个consumer?</p>
<p>staticMember 是client指定的groupInsanceID</p>
<p>staticMember 和PendingMember是啥？作用是啥？</p>
<p>GroupInstanceId用户指定的consumerid,每个group中这些ID必须是唯一的。</p>
<p>和member.id不同的是，每次成员重启回来后，其静态成员ID值是不变的，因此之前分配给该成员的所有分区也是不变的，而且在没有超时前静态成员重启回来是不会触发Rebalance的。</p>
<pre><code>Static Membership: the membership protocol where the consumer group will not trigger rebalance unless 
  * A new member joins
  * A leader rejoins (possibly due to topic assignment change)
  * An existing member offline time is over session timeout
  * Broker receives a leave group request containing alistof `group.instance.id`s (details later)

Group instance id: the unique identifier defined by user to distinguish each client instance.
</code></pre>
<p><img src="kafka/./coordinator-join-group.svg" alt="join-leave-group-coordinator" /></p>
<h2 id="sync-group"><a class="header" href="#sync-group">Sync group</a></h2>
<p>总体而言，rebalance分为2步：Join和Sync</p>
<ol>
<li>Join， 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求入组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。</li>
<li>Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。</li>
</ol>
<p><img src="kafka/./sync-group.svg" alt="sync-group" /></p>
<h2 id="fetchcommit-offset"><a class="header" href="#fetchcommit-offset">Fetch/Commit Offset</a></h2>
<p><img src="kafka/./commit-offset.svg" alt="commit-offset" /></p>
<h2 id="heartbeat"><a class="header" href="#heartbeat">heartbeat</a></h2>
<p><img src="kafka/./group-heartbeat.svg" alt="group-heartbeat" /></p>
<h2 id="group状态"><a class="header" href="#group状态">Group状态</a></h2>
<p>group状态，以及group各个状态下对join/leave/sync/offset_commit等行为的反应</p>
<p><img src="kafka/./group-state.svg" alt="group-state" /></p>
<h2 id="ref-9"><a class="header" href="#ref-9">Ref</a></h2>
<ol>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances">static memeber</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Client-side+Assignment+Proposal">Kafka Client-side Assignment Proposal</a></li>
<li><a href="https://www.cnblogs.com/huxi2b/p/11386847.html">Kafka消费者组静态成员</a></li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6223228.html">Kafka消费组(consumer group)</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-读写消息"><a class="header" href="#kafka-读写消息">Kafka 读写消息</a></h1>
<h2 id="消息的produce-and-consume"><a class="header" href="#消息的produce-and-consume">消息的produce and consume</a></h2>
<p><img src="kafka/./kafkaServer.svg" alt="kafka-produce-fetch" /></p>
<h2 id="partition-对应log对象创建"><a class="header" href="#partition-对应log对象创建">partition 对应log对象创建</a></h2>
<p>log对象是什么时候创建的？parition创建时候就创建吗？</p>
<p><img src="kafka/./partition-log-create.svg" alt="kafka-log-create" /></p>
<h2 id="replicamanager-partion信息维护"><a class="header" href="#replicamanager-partion信息维护">ReplicaManager Partion信息维护</a></h2>
<p>ReplicaManager的allPartions是存放在zk中的吗？不同broker server之间这个信息是怎么同步的?</p>
<pre><code class="language-java">public final class TopicPartition implements Serializable {
//other code
    private final int partition;
    private final String topic;
}
</code></pre>
<pre><code class="language-scala">class ReplicaManager{
/* other code */
  private val allPartitions = new Pool[TopicPartition, HostedPartition](
    valueFactory = Some(tp =&gt; HostedPartition.Online(Partition(tp, time, this)))
  )
/* other code */
}
</code></pre>
<p>当zk中broker,topic, partion, controller等发生变动时候，由kafka controller通过ControllerChannelManager
向每个kafka broker发送<code>LEADER_AND_ISR</code>消息, broker收到消息以后，会更新ReplicaManager中的allPartitions信息。</p>
<p><img src="kafka/./allpartion-overview.svg" alt="allpartionsoverview" /></p>
<p>具体细节如下
<img src="kafka/./getpartition.svg" alt="getPartition" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-logmanager"><a class="header" href="#kafka-logmanager">Kafka LogManager</a></h1>
<h2 id="kafka日志层级"><a class="header" href="#kafka日志层级">Kafka日志层级</a></h2>
<p>在kafka中每个topic可以有多个partition,　每个partition存储时候分为多个segment。</p>
<p>每个parition有多个副本，副本分布在不同的broker上，其中一个broker被选为该partition的leader,
消息是写到kafka partition leader副本中的，而follower通过fetchmessage，同步该partition的消息。</p>
<p><img src="kafka/./log_struct.svg" alt="logstruct" /></p>
<h2 id="日志文件加载和创建"><a class="header" href="#日志文件加载和创建">日志文件加载和创建</a></h2>
<p>启动时候，会打开log所有segment log file, Lazy的加载他们对应的index.</p>
<p><img src="kafka/./logmanager_loadlog.svg" alt="loadlog" /></p>
<h2 id="日志读写"><a class="header" href="#日志读写">日志读写</a></h2>
<p>写的message个数超过了配置也会触发flush，将cache中msg刷新到磁盘中。</p>
<p><img src="kafka/./log_read_write.svg" alt="load-read-write" /></p>
<h2 id="日志后台清理和压缩"><a class="header" href="#日志后台清理和压缩">日志后台清理和压缩</a></h2>
<h3 id="清理过期日志"><a class="header" href="#清理过期日志">清理过期日志</a></h3>
<p>后台线程根据配置定期清理过期或者超过大小的日志segment</p>
<p><img src="kafka/./log_clean.svg" alt="log-clean" /></p>
<h3 id="日志缓存flush"><a class="header" href="#日志缓存flush">日志缓存flush</a></h3>
<p>后台线程定期将cache刷新到磁盘.</p>
<p><img src="kafka/./log_flush.svg" alt="log-flush" /></p>
<h3 id="日志compact"><a class="header" href="#日志compact">日志compact</a></h3>
<p>有相同key的msg按照时间顺序只用保留最后一条。</p>
<p><img src="kafka/./kafka-log-compaction-process.png" alt="kafka-log-compact-process" /></p>
<p>首先会创建key -&gt; offset的映射，然后在遍历records的时候，只保留offset最大的那个。</p>
<pre><code class="language-scala">  private def buildOffsetMapForSegment(topicPartition: TopicPartition,
                                       segment: LogSegment,
                                       map: OffsetMap,
                                       startOffset: Long,
                                       maxLogMessageSize: Int,
                                       transactionMetadata: CleanedTransactionMetadata,
                                       stats: CleanerStats): Boolean = {
      //other code
      val records = MemoryRecords.readableRecords(readBuffer)
      throttler.maybeThrottle(records.sizeInBytes)
      for (batch &lt;- records.batches.asScala) {
        //other code...
        map.put(record.key, record.offset)
      }
}
</code></pre>
<p>在memory records的filter中根据这个OffsetMap 过滤掉相同key下offset小的record</p>
<pre><code class="language-scala">  private def shouldRetainRecord(map: kafka.log.OffsetMap,
                                 retainDeletes: Boolean,
                                 batch: RecordBatch,
                                 record: Record,
                                 stats: CleanerStats): Boolean = {
    val pastLatestOffset = record.offset &gt; map.latestOffset
    if (pastLatestOffset)
      return true

    if (record.hasKey) {
      val key = record.key
      val foundOffset = map.get(key)
      /* First,the message must have the latest offset for the key
       * then there are two cases in which we can retain a message:
       *   1) The message has value
       *   2) The message doesn't has value but it can't be deleted now.
       */
      val latestOffsetForKey = record.offset() &gt;= foundOffset
      val isRetainedValue = record.hasValue || retainDeletes
      latestOffsetForKey &amp;&amp; isRetainedValue
    } else {
      stats.invalidMessage()
      false
    }
  }
</code></pre>
<p><img src="kafka/./log_compact.svg" alt="log-compact" /></p>
<h2 id="ref-10"><a class="header" href="#ref-10">Ref</a></h2>
<ol>
<li><a href="http://cloudurable.com/blog/kafka-architecture-log-compaction/index.html">Kafka Architecture: Log Compaction</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-partition"><a class="header" href="#kafka-partition">Kafka Partition</a></h1>
<h2 id="partionstate"><a class="header" href="#partionstate">PartionState</a></h2>
<p>PartionState中重要信息为当前partion的leader和ISR(in sync replica)的replicaId， PartitionState最终是存储在zk中的。
isr信息有<code>maybeShrinkIsr</code>和<code>maybeExpandIsr</code>这两个函数维护.</p>
<p>每个parition的replica follower都有一个replicaFetcher 线程，该线程负责从partition的leader中
获取消息，在parition leader中处理fetchMessage请求时，判断该follower是否达到in sync标准，将该replicaId加入到该partiton中的ISR中。</p>
<p>另外ReplicaManager后台会周期性的调用<code>maybeShrinkIsr</code>将outOfSync的replica从ISR中踢掉。</p>
<p><img src="kafka/./partition-isr.svg" alt="isr" /></p>
<h3 id="replica-inout-sync-state"><a class="header" href="#replica-inout-sync-state">replica in/out sync state</a></h3>
<p>判断replica是否处于in/out sync状态</p>
<pre><code class="language-scala">  private def isFollowerOutOfSync(replicaId: Int,
                                  leaderEndOffset: Long,
                                  currentTimeMs: Long,
                                  maxLagMs: Long): Boolean = {
    val followerReplica = getReplicaOrException(replicaId)
    followerReplica.logEndOffset != leaderEndOffset &amp;&amp;
      (currentTimeMs - followerReplica.lastCaughtUpTimeMs) &gt; maxLagMs
  }

  private def isFollowerInSync(followerReplica: Replica, highWatermark: Long): Boolean = {
    val followerEndOffset = followerReplica.logEndOffset
    followerEndOffset &gt;= highWatermark &amp;&amp; leaderEpochStartOffsetOpt.exists(followerEndOffset &gt;= _)
  }
</code></pre>
<h3 id="partition-对应log对象创建-1"><a class="header" href="#partition-对应log对象创建-1">partition 对应log对象创建</a></h3>
<p>在成为leader或者follower时会创建相应的log对象</p>
<p>log对象是什么时候创建的？parition创建时候就创建吗？
<img src="kafka/./partition-log-create.svg" alt="kafka-log-create" /></p>
<h3 id="partition-sate-在zk中的存储"><a class="header" href="#partition-sate-在zk中的存储">Partition sate 在zk中的存储</a></h3>
<h4 id="存储路径"><a class="header" href="#存储路径">存储路径</a></h4>
<p>Partition 的ISR信息存储在zk下</p>
<pre><code>/broker/topics/{topic}/partitions/{partition}/state，
</code></pre>
<p>具体对应代码在<code>zkData.scala</code>中</p>
<pre><code class="language-scala">// tp partition状态在zk中存储路径
object TopicPartitionStateZNode {
  def path(partition: TopicPartition) = s&quot;${TopicPartitionZNode.path(partition)}/state&quot;
  //other code
}

//tp路径
object TopicPartitionsZNode {
  def path(topic: String) = s&quot;${TopicZNode.path(topic)}/partitions&quot;
}

object TopicZNode {
  def path(topic: String) = s&quot;${TopicsZNode.path}/$topic&quot;
  //othercode
}

//topics路径
object TopicsZNode {
  def path = s&quot;${BrokersZNode.path}/topics&quot;
}

</code></pre>
<h4 id="存储信息"><a class="header" href="#存储信息">存储信息</a></h4>
<p>paritionstate中存储信息如下</p>
<pre><code class="language-scala">  def decode(bytes: Array[Byte], stat: Stat): Option[LeaderIsrAndControllerEpoch] = {
    Json.parseBytes(bytes).map { js =&gt;
      val leaderIsrAndEpochInfo = js.asJsonObject
      val leader = leaderIsrAndEpochInfo(&quot;leader&quot;).to[Int]
      val epoch = leaderIsrAndEpochInfo(&quot;leader_epoch&quot;).to[Int]
      val isr = leaderIsrAndEpochInfo(&quot;isr&quot;).to[List[Int]]
      val controllerEpoch = leaderIsrAndEpochInfo(&quot;controller_epoch&quot;).to[Int]
      val zkPathVersion = stat.getVersion
      LeaderIsrAndControllerEpoch(LeaderAndIsr(leader, epoch, isr, zkPathVersion), controllerEpoch)
    }
  }
</code></pre>
<p>LeaderAndIsrPartitionState定义在LeaderAndIsrRequest.json中,定义如下</p>
<pre><code class="language-json">  &quot;commonStructs&quot;: [
    { &quot;name&quot;: &quot;LeaderAndIsrPartitionState&quot;, &quot;versions&quot;: &quot;0+&quot;, &quot;fields&quot;: [
      { &quot;name&quot;: &quot;TopicName&quot;, &quot;type&quot;: &quot;string&quot;, &quot;versions&quot;: &quot;0-1&quot;, &quot;entityType&quot;: &quot;topicName&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The topic name.  This is only present in v0 or v1.&quot; },
      { &quot;name&quot;: &quot;PartitionIndex&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The partition index.&quot; },
      { &quot;name&quot;: &quot;ControllerEpoch&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The controller epoch.&quot; },
      { &quot;name&quot;: &quot;Leader&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;, &quot;entityType&quot;: &quot;brokerId&quot;,
        &quot;about&quot;: &quot;The broker ID of the leader.&quot; },
      { &quot;name&quot;: &quot;LeaderEpoch&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The leader epoch.&quot; },
      { &quot;name&quot;: &quot;Isr&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The in-sync replica IDs.&quot; },
      { &quot;name&quot;: &quot;ZkVersion&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The ZooKeeper version.&quot; },
      { &quot;name&quot;: &quot;Replicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The replica IDs.&quot; },
      { &quot;name&quot;: &quot;AddingReplicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;3+&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The replica IDs that we are adding this partition to, or null if no replicas are being added.&quot; },
      { &quot;name&quot;: &quot;RemovingReplicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;3+&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The replica IDs that we are removing this partition from, or null if no replicas are being removed.&quot; },
      { &quot;name&quot;: &quot;IsNew&quot;, &quot;type&quot;: &quot;bool&quot;, &quot;versions&quot;: &quot;1+&quot;, &quot;default&quot;: &quot;false&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;Whether the replica should have existed on the broker or not.&quot; }
    ]}
  ]
</code></pre>
<h2 id="replica-sync副本同步"><a class="header" href="#replica-sync副本同步">Replica sync(副本同步)</a></h2>
<p>在broker成为一个follower时候，会启动一个fetchThread，用于和partition leader同步消息
<img src="kafka/./replica-sync.svg" alt="replica-sync" /></p>
<h2 id="replica-leader-election"><a class="header" href="#replica-leader-election">Replica Leader Election</a></h2>
<p>partition replica leader是由KafkaController来分配的.</p>
<p><img src="kafka/./replica-leader-election.svg" alt="replica-leader-election" /></p>
<p><img src="kafka/./elecLeaderForPartitions.svg" alt="electLeaderForPartitions" /></p>
<p>partion leader选择策略</p>
<pre><code class="language-scala">  def offlinePartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], uncleanLeaderElectionEnabled: Boolean, controllerContext: ControllerContext): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id)).orElse {
      if (uncleanLeaderElectionEnabled) {
        val leaderOpt = assignment.find(liveReplicas.contains)
        if (leaderOpt.isDefined)
          controllerContext.stats.uncleanLeaderElectionRate.mark()
        leaderOpt
      } else {
        None
      }
    }
  }
  def reassignPartitionLeaderElection(reassignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    reassignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  def preferredReplicaPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    assignment.headOption.filter(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  def controlledShutdownPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], shuttingDownBrokers: Set[Int]): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id) &amp;&amp; !shuttingDownBrokers.contains(id))
  }
</code></pre>
<h1 id="ref-11"><a class="header" href="#ref-11">Ref</a></h1>
<ol>
<li><a href="http://objcoding.com/2019/11/05/kafka-isr/">Kafka ISR 副本同步机制</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-controller-主要功能"><a class="header" href="#kafka-controller-主要功能">Kafka Controller 主要功能</a></h1>
<p>kafka中会从broker server中选取一个作为controller，该controller通过ControllerChannelManager管理和每个broker通信的线程。</p>
<p>当zk中broker,topic, partion 等发生变动时，controller向每个broker发送消息, replica和partition 主要是通过replicaStateMachine和PartitionStateMachine来管理的
当replica或者partition leaderAndISR信息发生变动时候，controller通过这两个状态机，将状态的转换改为
相应的request请求，发送给broker。 </p>
<p>其中比较重要的请求是LeaderAndISR, 它指定了partition的leader和paritition in sync的replica list。</p>
<p>每个broker在zk中注册了ControllerChangeHandler，如果controller挂了，broker就会尝试去选举新的controller.</p>
<p><img src="kafka/./allpartion-overview.svg" alt="allpartionsoverview" /></p>
<p>controller会向broker发送三类请求: </p>
<ul>
<li>UpdateMetadataRequest: 更新元数据</li>
<li>LeaderAndIsrRequest: 创建分区，副本，leader和follower</li>
<li>StopReplicaRequest: 停止副本。</li>
</ul>
<p>controller和broker之间同步metadata</p>
<p>三类请求broker主要处理逻辑如下：</p>
<p><img src="kafka/./broker_update_metadata.svg" alt="broker_update_metadata" /></p>
<p>controller和broker之间处理IsrAndLeader请求</p>
<p><img src="kafka/./broker_handle_isr.svg" alt="borker-handle-isr" /></p>
<p>controller向broker发送stopReplica请求</p>
<p><img src="kafka/./broker-stop-replica.svg" alt="broker-stop-relica" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-controller-channelmanager"><a class="header" href="#kafka-controller-channelmanager">Kafka Controller: channelManager</a></h1>
<p>Controller和Broker之间采用队列来做异步通信,有专门的线程负责网络数据收发。</p>
<p>每次broker上线，Conntroller会新建一个RequestSendThread线程，当broker下线时候，会销毁该线程。</p>
<p>Controller和每个broker之间都有个RequestSendThread, Controller 将请求放到broker对应的请求队列中。
在RequestSendThread发送完请求，收到broker的响应之后，通过预先设置好的<code>sendEvent</code>回调，通过eventManager 采用异步的方式通知controller。</p>
<p><img src="kafka/./channel-manager.svg" alt="channel-manager" /></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="kafka-controller-选举"><a class="header" href="#kafka-controller-选举">Kafka Controller 选举</a></h2>
<p>每个kafka broker启动后, 会去zk中尝试创建ControllerZNode, 如果成功就会当选为controller。然后调用<code>onControllerFailover</code>开始controller的工作</p>
<ul>
<li>从zk中加载数据，刷新controllerContext中的各种cache.</li>
<li>在zk中注册broker, topic, patition等zk处理函数.</li>
<li>启动channelManager, 建立和其他broker之间通信channel</li>
<li>启动PartitionStateMachine和ReplicaStateMachine管理分区和副本状态.</li>
<li>启动kafkaScheduler，启动后台调度等</li>
</ul>
<p><img src="kafka/./controller-elect.svg" alt="controller-elect" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-controller-zk监听"><a class="header" href="#kafka-controller-zk监听">Kafka Controller zk监听</a></h1>
<p>在broker当选为controller之后，controller会在zk上注册一堆的handler， 处理broker/topic/partions等变化</p>
<pre><code class="language-scala">  private def onControllerFailover(): Unit = {
    info(&quot;Registering handlers&quot;)

    // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks
    val childChangeHandlers = Seq(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,
      isrChangeNotificationHandler)
    childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)
    val nodeChangeHandlers = Seq(preferredReplicaElectionHandler, partitionReassignmentHandler)
    nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)
    //...other code
  }
</code></pre>
<h2 id="broker"><a class="header" href="#broker">Broker</a></h2>
<p><code>BrokerChangeHandler</code>, 处理broker上线下线</p>
<p><img src="kafka/./controler-failover-zk-broker.svg" alt="controller-failover-zk-broker" /></p>
<h2 id="topic"><a class="header" href="#topic">Topic</a></h2>
<h3 id="topic-change"><a class="header" href="#topic-change">topic change</a></h3>
<p><img src="kafka/./topic-change.svg" alt="topic-change" /></p>
<h3 id="topic-delete"><a class="header" href="#topic-delete">topic delete</a></h3>
<p><img src="kafka/./topic-delete.svg" alt="topic-delete" /></p>
<h2 id="isrchange"><a class="header" href="#isrchange">Isrchange</a></h2>
<p>主要更新controller中的cache，并且controller发送sendUpdateMetadata通知所有的borker更新metadata.
<img src="kafka/./isr-change.svg" alt="isr-change" /></p>
<h2 id="logdirevent"><a class="header" href="#logdirevent">LogDirEvent</a></h2>
<p><img src="kafka/./logdir-event.svg" alt="logdir-event" /></p>
<h2 id="replicaleaderelection"><a class="header" href="#replicaleaderelection">ReplicaLeaderElection</a></h2>
<p><img src="kafka/./controller-process-replica-leader-election.svg" alt="replica-leader-election" /></p>
<h2 id="partitionreassignment"><a class="header" href="#partitionreassignment">PartitionReassignment</a></h2>
<p><img src="kafka/./controller-partition-reasignment.svg" alt="partion-reassignment" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-replica-assignment"><a class="header" href="#kafka-replica-assignment">Kafka Replica Assignment</a></h1>
<h2 id="replica-迁移过程"><a class="header" href="#replica-迁移过程">Replica 迁移过程</a></h2>
<p>缩写说明</p>
<ol>
<li>RS: replica set 所有的replica set</li>
<li>AR: add replica, 需要添加的replica</li>
<li>RR: remove replica,　需要删除的replica</li>
<li>TRS: target replica set, 要达到目标的replica set</li>
<li>ORS: target replica set, 原有的replica set</li>
</ol>
<p>具体迁移过程kafka代码中注释写的比较详细, 主要分为俩个阶段:</p>
<h4 id="phase-a"><a class="header" href="#phase-a">Phase A</a></h4>
<p>如果AR没有在partition的ISR中，controller会发送NewReplica请求给AR的broker, 这些broker开始调用
replicaManager的makeFollowers, 启动Replicafetch线程和parititon leader同步，达到in-sync条件后，partition leader会将该broker加入到ISR中。</p>
<p>然后会触发controller在zk中注册的handler,开始下一步的迁移</p>
<h4 id="phase-b"><a class="header" href="#phase-b">Phase B</a></h4>
<p>删除RR中的replica, 更新zk, 如果leader不在TRS中，controller需要发送LeaderAndIsr request给broker, 指定新的leader.</p>
<pre><code>   * Phase A (when TRS != ISR): The reassignment is not yet complete
   *
   *   A1. Bump the leader epoch for the partition and send LeaderAndIsr updates to RS.
   *   A2. Start new replicas AR by moving replicas in AR to NewReplica state.
   *
   * Phase B (when TRS = ISR): The reassignment is complete
   *
   *   B1. Move all replicas in AR to OnlineReplica state.
   *   B2. Set RS = TRS, AR = [], RR = [] in memory.
   *   B3. Send a LeaderAndIsr request with RS = TRS. This will prevent the leader from adding any replica in TRS - ORS back in the isr.
   *       If the current leader is not in TRS or isn't alive, we move the leader to a new replica in TRS.
   *       We may send the LeaderAndIsr to more than the TRS replicas due to the
   *       way the partition state machine works (it reads replicas from ZK)
   *   B4. Move all replicas in RR to OfflineReplica state. As part of OfflineReplica state change, we shrink the
   *       isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr.
   *       After that, we send a StopReplica (delete = false) to the replicas in RR.
   *   B5. Move all replicas in RR to NonExistentReplica state. This will send a StopReplica (delete = true) to
   *       the replicas in RR to physically delete the replicas on disk.
   *   B6. Update ZK with RS=TRS, AR=[], RR=[].
   *   B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it if present.
   *   B8. After electing leader, the replicas and isr information changes. So resend the update metadata request to every broker.
   *
   * In general, there are two goals we want to aim for:
   * 1. Every replica present in the replica set of a LeaderAndIsrRequest gets the request sent to it
   * 2. Replicas that are removed from a partition's assignment get StopReplica sent to them
   *
   * For example, if ORS = {1,2,3} and TRS = {4,5,6}, the values in the topic and leader/isr paths in ZK
   * may go through the following transitions.
   * RS                AR          RR          leader     isr
   * {1,2,3}           {}          {}          1          {1,2,3}           (initial state)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3}           (step A2)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3,4,5,6}     (phase B)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {1,2,3,4,5,6}     (step B3)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {4,5,6}           (step B4)
   * {4,5,6}           {}          {}          4          {4,5,6}           (step B6)
   *
   * Note that we have to update RS in ZK with TRS last since it's the only place where we store ORS persistently.
   * This way, if the controller crashes before that step, we can still recover.
</code></pre>
<p><img src="kafka/./partition-replica-assignment.svg" alt="repartition-replica-assignment" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-partitionreplica-state-machine"><a class="header" href="#kafka-partitionreplica-state-machine">Kafka partition/replica state machine</a></h1>
<h2 id="replica-状态机"><a class="header" href="#replica-状态机">Replica 状态机</a></h2>
<p><img src="kafka/./replica-statemachine.svg" alt="replica state machine" /></p>
<h3 id="replica-状态入口"><a class="header" href="#replica-状态入口">replica 状态入口</a></h3>
<p><img src="kafka/./replica-target-state.svg" alt="replica target state" /></p>
<h2 id="partition-状态机"><a class="header" href="#partition-状态机">Partition 状态机</a></h2>
<p><img src="kafka/./partition-statemachine.svg" alt="partition_statemachine" /></p>
<h3 id="partition-状态入口"><a class="header" href="#partition-状态入口">partition 状态入口</a></h3>
<p><img src="kafka/./partition-target-state.svg" alt="partition_target_state" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="txn-coordinator"><a class="header" href="#txn-coordinator">Txn coordinator</a></h1>
<p>kafka streams中实现exactly once处理
read process write cycle</p>
<pre><code class="language-java">KafkaProducer producer = createKafkaProducer(
  “bootstrap.servers”, “localhost:9092”,
  “transactional.id”, “my-transactional-id”);

producer.initTransactions();

KafkaConsumer consumer = createKafkaConsumer(
  “bootstrap.servers”, “localhost:9092”,
  “group.id”, “my-group-id”,
  &quot;isolation.level&quot;, &quot;read_committed&quot;);

consumer.subscribe(singleton(“inputTopic”));

while (true) {
  ConsumerRecords records = consumer.poll(Long.MAX_VALUE);
  producer.beginTransaction();
  for (ConsumerRecord record : records)
    producer.send(producerRecord(“outputTopic”, record));
  producer.sendOffsetsToTransaction(currentOffsets(consumer), group);  
  producer.commitTransaction();
}
</code></pre>
<h2 id="dataflow"><a class="header" href="#dataflow">Dataflow</a></h2>
<p><a href="https://www.confluent.io/blog/transactions-apache-kafka/">Transactions in Apache Kafka</a>中从整体上介绍了kafka中的事务处理流程,
摘抄如下：</p>
<ol>
<li>在A中producer和txn coordinator交互，获取唯一producerId，注册涉及到的partition等。主要发送请为InitProducerId, AddPartitionsToTxn, AddOffsetsToPartitions</li>
<li>在B中txn coordinator将事务各种状态写入日志中。</li>
<li>在C中producer正常向各个topic paritition写数据。</li>
<li>在D中coordinator开始两阶段提交，coordinator确保每个paritition将WriteMark写入成功。</li>
</ol>
<p><img src="kafka/./txn-dataflow.png" alt="tx-dataflow" /></p>
<h2 id="findcoordinator"><a class="header" href="#findcoordinator">FindCoordinator</a></h2>
<p>首先producer发送FindCoordinator请求找到transcationId对应的coordinator. transactionId由client端提供，保证唯一性。
服务端会根据transactionId做hash，分配到相应的topic state 的paritition 中。
该parition 的leader即为这个事务的coordinator.</p>
<p><img src="kafka/./txn-find-coordinator.svg" alt="txn-find-coordinator" /></p>
<h2 id="initproducerid"><a class="header" href="#initproducerid">InitProducerId</a></h2>
<p>生成全局唯一producerId, 每个transactionId对应着一个TransactionMetadata,
其中的topicPartitions 该事务涉及到的topic partition set.</p>
<p>服务端生成ProducerID时候，有个producerManager每次向zk申请一段的producerId区间，请求来了，先用改区间的id，如果用完了
就像zk再申请。这里面使用了expect zk version 来做分布式控制。避免申请的block被其他的txn coordinator覆盖了。</p>
<p><img src="kafka/./txn-producer-id.svg" alt="txn-producer-id" /></p>
<h2 id="addpartitionstotxn"><a class="header" href="#addpartitionstotxn">AddPartitionsToTxn</a></h2>
<p>向事务添加Partitions, 或者提交当前消费的offset, 由于提交offset也是一种写入topic paritition行为，所以这边统一处理了。</p>
<p><img src="kafka/./txn-add-partition.svg" alt="txn-addPartitions" /></p>
<h2 id="endtxn"><a class="header" href="#endtxn">endTxn</a></h2>
<p>最后producer发送endTxn请求， commit/abort 事务, coordinator开始两阶段提交。</p>
<h3 id="准备阶段preparecommitprepareabort"><a class="header" href="#准备阶段preparecommitprepareabort">准备阶段：PrepareCommit/PrepareAbort</a></h3>
<p>将prepareCommit/PrepareAbort写入日志中, 写成功之后，coordinator会保证事务一定会被commit或者abort.</p>
<p><img src="kafka/./txn-prepare.svg" alt="txn-prepare" /></p>
<h3 id="提交阶段"><a class="header" href="#提交阶段">提交阶段</a></h3>
<p>prepareCommit/preapreAbort日志写入成功后调用<code>sendTxnMarkersCallback</code>, coordinator 向事务中涉及到的broker发送WriteTxnMarker 请求，coordinator会一直尝试发送直到成功。
所有broker都响应成功后，会写入日志，并迁移到complete状态。</p>
<p><code>SendTxnMarkers</code>将请求放入队列中, 有个单独的InterBrokerThread线程负责从队列, 以及处理失败的队列中取出这些消息，然后将相同broker的请求batch起来，统一发送。</p>
<p><img src="kafka/./txn-commit.svg" alt="txn-commit" /></p>
<h3 id="broker对writemarkers请求的处理"><a class="header" href="#broker对writemarkers请求的处理">broker对WriteMarkers请求的处理</a></h3>
<p><img src="kafka/./txn-write-markers.svg" alt="txn-write-markers" /></p>
<h2 id="txnimmigration"><a class="header" href="#txnimmigration">TxnImmigration</a></h2>
<p>txn coordinator partition leader发生了变化，新的leader读取事务日志，加载到内存中，保存在变量<code>transactionMetadataCache</code>中.
对于PrepareCommit/PrepareAbort状态的事务会重新<code>SendTxnMarkers</code>请求</p>
<p><img src="kafka/./txn-immigration.svg" alt="txn-immigration" /></p>
<h2 id="事务状态机迁移"><a class="header" href="#事务状态机迁移">事务状态机迁移</a></h2>
<p>状态迁移时候先<code>prepareTransionTo</code> 设置要转移到的Metadata状态, 然后调用appendTransactionToLog将事务写入日志，日志写入成功后
调用completeTransitionTo 迁移到目标状态</p>
<p><img src="kafka/./txn-state.svg" alt="txn-state" /></p>
<h2 id="事务日志消息格式"><a class="header" href="#事务日志消息格式">事务日志消息格式</a></h2>
<p>事务日志中消息格式如下, 启动了log compaction</p>
<p><img src="kafka/./txn-message.svg" alt="txn-message" /></p>
<h2 id="ref-12"><a class="header" href="#ref-12">Ref</a></h2>
<ol>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/">Transactions in Apache Kafka</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Transactional Messaging in Kafka</a></li>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.i4ub5zye01nh">Exactly Once Delivery and Transactional Messaging in Kafka</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Transactional Messaging in Kafka</a></li>
<li><a href="https://zhmin.github.io/2019/05/20/kafka-transaction/">Kafka 事务实现原理</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1149669">Kafka设计解析8</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="draft-stream"><a class="header" href="#draft-stream">Draft: Stream</a></h1>
<h2 id="streamgraphnode"><a class="header" href="#streamgraphnode">StreamGraphNode</a></h2>
<p><img src="kafka/./stream-graph-node.svg" alt="stream graph node" /></p>
<h3 id="processor"><a class="header" href="#processor">Processor</a></h3>
<p><img src="kafka/./stream-processor.svg" alt="stream processor" /></p>
<h3 id="processorcontext"><a class="header" href="#processorcontext">ProcessorContext</a></h3>
<p><img src="kafka/./stream-processor-context.svg" alt="stream processor context" /></p>
<h2 id="stream-start"><a class="header" href="#stream-start">Stream start</a></h2>
<p><img src="kafka/./stream-start.svg" alt="stream start" /></p>
<h2 id="questions-6"><a class="header" href="#questions-6">Questions</a></h2>
<ol>
<li>DAG图是怎么建立起来的。</li>
<li>Kafka怎么调度DAG？怎么在不同线程，不同机器上部署？</li>
<li>DAG节点之间是怎么通信的？单纯通过kafka topic ?</li>
<li>怎么处理节点之间的依赖关系的?</li>
<li>Stream中的localstate, sharestate是怎么搞得，怎么保证故障恢复的。状态存储实现快速故障恢复和从故障点继续处理</li>
<li>Window join 具体指的是啥</li>
<li>KStream和KTable在kafka中是怎么表示的。</li>
<li>Kafka中的window有哪些？分别是怎么实现的？</li>
<li>through方法提供了类似Spark的Shuffle机制，为使用不同分区策略的数据提供了Join的可能</li>
</ol>
<p>KTable, KStream, KGroupedTable</p>
<p>StreamsBuilder</p>
<p>StreamGraphNode;
GlobalStoreNode;
StateStoreNode;
storeBuilder</p>
<p>writeToTopology</p>
<p>map/filter/groupBy/join(leftJoin, outerJoin)
queryableStoreName;</p>
<p>context.getStateStore</p>
<p>kafka Stream的并行模型中，最小粒度为Task，而每个Task包含一个特定子Topology的所有Processor。因此每个Task所执行的代码完全一样，唯一的不同在于所处理的数据集互补。</p>
<p>这里要保证两个进程的<code>StreamsConfig.APPLICATION_ID_CONFIG</code>完全一样。因为Kafka Stream将<code>APPLICATION_ID_CONFIG</code>作为隐式启动的Consumer的Group ID。只有保证APPLICATION_ID_CONFIG相同，才能保证这两个进程的Consumer属于同一个Group，从而可以通过Consumer Rebalance机制拿到互补的数据集。</p>
<p>State store被用来存储中间状态。它可以是一个持久化的Key-Value存储，也可以是内存中的HashMap，或者是数据库。Kafka提供了基于Topic的状态存储。</p>
<p>Topic中存储的数据记录本身是Key-Value形式的，同时Kafka的log compaction机制可对历史数据做compact操作，保留每个Key对应的最后一个Value，从而在保证Key不丢失的前提下，减少总数据量，从而提高查询效率。</p>
<h1 id="ref-13"><a class="header" href="#ref-13">Ref</a></h1>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1149756">Kafka设计解析（七）- Kafka Stream</a></li>
<li><a href="https://www.orchome.com/335">Kafka Streams开发者指南</a></li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-kafka-streams/kafka-streams-internals-TaskManager.html">Kafka Streams Internal: TaskManager</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Architecture">Kafka Streams Architecture</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hotspot"><a class="header" href="#hotspot">hotspot</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h2 id="在osx下编译调试hotspot"><a class="header" href="#在osx下编译调试hotspot">在osx下编译调试hotspot</a></h2>
<h3 id="摘要-4"><a class="header" href="#摘要-4">摘要</a></h3>
<p>本文主要描述了在osx下编译hotspot debug版本以方便后续的hotspot代码研读，并尝试了使用gdb和lldb对hotspot进行debug。解决了Debug的时候会遇到的SIGSEGV问题，最后确定用lldb脚本来debug hotspot。</p>
<h3 id="准备工作-1"><a class="header" href="#准备工作-1">准备工作</a></h3>
<ol>
<li>安装freetype</li>
</ol>
<pre><code class="language-bash">$brew install freetype
</code></pre>
<ol start="2">
<li>获取openjdk repo代码</li>
</ol>
<pre><code class="language-bash">$git clone https://github.com/dmlloyd/openjdk.git
</code></pre>
<ol start="3">
<li>configure然后make slowdebug 版本, 开启<code>--with-native-debug-symbols=internal</code>选项以保留debug-symols</li>
</ol>
<pre><code class="language-bash">$bash ./configure  --with-target-bits=64 --with-freetype-include=/usr/X11/include/freetype2 --with-freetype-lib=/usr/X11/lib --disable-warnings-as-errors --with-debug-level=slowdebug  --with-native-debug-symbols=internal

$make
</code></pre>
<h3 id="gdb-调试"><a class="header" href="#gdb-调试">GDB 调试</a></h3>
<p>准备好HelloWorld.java, 然后用javac编译</p>
<pre><code class="language-java">public class HelloWorld {
    public static void main(String[] args) {
        System.out.println(&quot;hello,world&quot;);
    }
}
</code></pre>
<p>准备gdb 调试脚本, 这里面的file指向第一步编译好的java</p>
<pre><code>$sudo gdb -x hello.gdb
</code></pre>
<p>里面的hello.gdb内容如下：</p>
<pre><code>//hello.gdb
file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
handle SIGSEGV nostop noprint pass

# break points
break java.c:JavaMain
break InitializeJVM
break LoadJavaVM
break ContinueInNewThread

#in javaMain, after InitializeJVM
break java.c:477
commands
print &quot;vm is&quot;
print **vm
print &quot;env is&quot;
print **env
end

run HelloWorld
</code></pre>
<p>gdb脚本中的 break java.c:477 commands ... end 在到达断点的时候，会去执行commands中的命令，这样感觉非常方便~~. 在这边可以看到运行完InitializeJVM之后，vm和env这两个都初始化好了。</p>
<p>vm初始化之后是这样的, 绑定了几个函数指针, env中绑定的函数指针太多了，在此就不列举了。</p>
<pre><code>{reserved0 = 0x0, reserved1 = 0x0, reserved2 = 0x0,
    DestroyJavaVM = 0x104939bb0,
    AttachCurrentThread = 0x104939e20,
    DetachCurrentThread = 0x10493a2d0,
    GetEnv = 0x10493a470,
    AttachCurrentThreadAsDaemon = 0x10493a770
</code></pre>
<p>gdb 调试在mac下会有些问题，libjvm这个so中的符号看不到，无法打断点，网上研究了不少时间，最后发现是osx sierra和gdb兼容性问题，最后搞了半天，感觉太麻烦了。只好放弃，改用lldb.</p>
<h3 id="lldb-调试"><a class="header" href="#lldb-调试">lldb 调试</a></h3>
<p><a href="https://developer.apple.com/library/content/documentation/IDEs/Conceptual/gdb_to_lldb_transition_guide/document/lldb-basics.html">lldb</a>调试和gdb很类似. lldb类似的脚本如下, 感觉比gdb清晰些，但是也啰嗦了些~~。</p>
<p>由于lldb只有在进程跑起来的时候，才能加<code>process handle xxx</code>, 所以在main上加一个breakpoint，在那个时候把hanlde SIGSEGV这个加上，忽略SIGSEGV信号。 lldb中通过breakpoing command add 这个加断点的时候要执行的命令，以DONE作为结束。</p>
<pre><code class="language-gdb">file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;

#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

run HelloWorld
process handle SIGSEGV --notify false --pass true --stop false
</code></pre>
<p>通过下面命令执行lldb debug的脚本</p>
<pre><code class="language-sh">$lldb -s helloworld.lldb
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="hotspot代码研读-jvm-初始化时创建的线程"><a class="header" href="#hotspot代码研读-jvm-初始化时创建的线程">Hotspot代码研读: jvm 初始化时创建的线程</a></h2>
<h3 id="摘要-5"><a class="header" href="#摘要-5">摘要</a></h3>
<p>本文通过在pthread_create方法上打断点的方式，得到了jvm初始化的时候创建的线程。然后对里面主要线程JavaThread, VMThread， CompilerThread, GCthread 做了简要的分析。</p>
<h3 id="创建线程的callstack"><a class="header" href="#创建线程的callstack">创建线程的callstack</a></h3>
<p>由于创建线程最终肯定会调用pthread_create方法，所以为了研究jvm启动的时候，创立了哪些线程，准备了下面的lldb调试脚本。在pthread_create方法上打断点，然后用bt命令打印callstack， 然后continue接着执行, 去打印下一个pthread_create的callstack, 这样最后就可以得到所有的pthread_create的callstack了。</p>
<p>HelloWorld.java</p>
<pre><code class="language-java">public class HelloWorld {
    public static void main(String[] args) {
        System.out.println(&quot;hello,world&quot;);
    }
}
</code></pre>
<p>HelloWorld.lldb</p>
<pre><code class="language-sh">//hello.lldb
file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;
#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

breakpoint set --name pthread_create
breakpoint command add
bt
continue
DONE

run HelloWorld
</code></pre>
<p>执行lldb 脚本</p>
<pre><code class="language-bash">#编译HelloWorld.java
$javac HelloWorld.java
#执行lldb脚本
$lldb -s HelloWorld.lldb
</code></pre>
<p>最后得到的<a href="java/./src/pthread_create_bt.log">pthread_create_bt.log</a>, pthread call stack关系整理如下图:</p>
<img src="java/./images/jvm-threads-pthread-callstack.jpeg" />
<h3 id="线程创建的过程"><a class="header" href="#线程创建的过程">线程创建的过程</a></h3>
<p>main是java的laucher入口，在<code>main-&gt; JLI_LAUCH -&gt; LoadJavaVM </code>中会调用dlopen加载libjvm的so, 设置好JNI_CreateJavaVm的函数指针.</p>
<pre><code class="language-cpp">// main -&gt; JLI_LAUCH -&gt; LoadJavaVM:
// load libjvm so
#ifndef STATIC_BUILD
    libjvm = dlopen(jvmpath, RTLD_NOW + RTLD_GLOBAL);
#else
    libjvm = dlopen(NULL, RTLD_FIRST);
#endif
//other codes
ifn-&gt;CreateJavaVM = (CreateJavaVM_t)
       dlsym(libjvm, &quot;JNI_CreateJavaVM&quot;);
</code></pre>
<p>然再<code>main-&gt;JLI_LAUCH -&gt; JVMinit -&gt; ContinueInNewThread</code>创建一个新的线程。新的线程开始执行JavaMain函数.</p>
<p>在JavaMain中最终调用Threads::create_vm 创建java vm中的其他线程。</p>
<pre><code class="language-cpp">//JNI_CreateJavaVM jni.cpp:4028
frame #13: thread.cpp:3623: Threads::create_vm(JavaVMInitArgs*, bool*)
frame #14: jni.cpp:3938: JNI_CreateJavaVM_inner(JavaVM_**, void**, void*)
frame #15: jni.cpp:4033: ::JNI_CreateJavaVM(JavaVM **, void **, void *)
frame #16: java.c:1450: InitializeJVM
frame #17: java.c:402: JavaMain
</code></pre>
<h4 id="thread-之间的继承关系"><a class="header" href="#thread-之间的继承关系">Thread 之间的继承关系</a></h4>
<p>线程class之间的继承关系如下:</p>
<img src="java/./images/thread-inherit.jpeg"/>
<h3 id="javathread"><a class="header" href="#javathread">JavaThread</a></h3>
<p>// TODO</p>
<h3 id="vmthread"><a class="header" href="#vmthread">VMThread</a></h3>
<p>// TODO</p>
<h3 id="compilebroker"><a class="header" href="#compilebroker">CompileBroker</a></h3>
<p>// TODO</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="hotspot代码研读-class文件的加载和执行"><a class="header" href="#hotspot代码研读-class文件的加载和执行">Hotspot代码研读: class文件的加载和执行</a></h2>
<h4 id="摘要-6"><a class="header" href="#摘要-6">摘要</a></h4>
<p>本文首先描述了Helloworld.class文件的结构，然后分析了HelloWorld这个在Java中的类在hotpos jvm对应的instanceKlass实例。然后具体分析了HelloWorld中的<code>static main</code>函数字节码， 以及它被加载以后在JVM中存放的位置。之后描述了字节码解释器TemplateInterpreter初始化过程, 最后分析了static main这个java 代码入口函数被调用的过程, 以及<code>new</code>这个字节码执行的时候具体做了哪些工作。</p>
<h4 id="helloworldclass-字节码分析"><a class="header" href="#helloworldclass-字节码分析">HelloWorld.class 字节码分析</a></h4>
<p>这里先准备一个HelloWord.java，main函数里面new了一个HelloWorld对象，然后调用了该对象的一个成员函数hello方法。</p>
<pre><code class="language-java">public class HelloWorld {
    String m_name;
    int m_age = 0;

    public static void main(String[] args) {
        HelloWorld obj = new HelloWorld();
        obj.hello();
    }

    private void hello(){
        m_age ++;
        System.out.println(&quot;hello, world&quot;);
    }
}
</code></pre>
<p>编译完之后，可以用如下命令查看生成的class二进制文件, 包括常量池和方法的字节码。</p>
<pre><code class="language-bash">$javac HelloWorld.java
$javap -v HelloWorld &gt;HelloWorld-javap
</code></pre>
<p>class文件包含两部分，一部分是常量池，另外一部分是方法对应的字节码。常量池包含了这个class中涉及到的字符串，字面常量，methodref, classRef等各种引用。</p>
<pre><code>Constant pool:
   #1 = Methodref          #9.#23         // java/lang/Object.&quot;&lt;init&gt;&quot;:()V
   #2 = Fieldref           #3.#24         // HelloWorld.m_age:I
   #3 = Class              #25            // HelloWorld
   #4 = Methodref          #3.#23         // HelloWorld.&quot;&lt;init&gt;&quot;:()V
   ....
   #10 = Utf8               m_name
   #11 = Utf8               Ljava/lang/String;
</code></pre>
<p>下面HelloWorld.class的方法有三个，<code>&lt;init&gt;&quot;:()V</code>对应着HelloWorld的构造函数，还有main, hello这两个函数，下面主要看下HelloWorld main方法生成的字节码。new之后，做了一个dup(dup的原因是因为后面调用构造函数需要消耗一个，赋值操作也需要消耗一个)，调用了helloWorld的构造函数，对obj做了赋值, 最后调用了hello方法之后就返回了。</p>
<pre><code>  public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    flags: ACC_PUBLIC, ACC_STATIC
    Code:
      stack=2, locals=2, args_size=1
         0: new           #3                  // class HelloWorld
         3: dup
         4: invokespecial #4                  // Method &quot;&lt;init&gt;&quot;:()V
         7: astore_1
         8: aload_1
         9: invokespecial #5                  // Method hello:()V
        12: return
      LineNumberTable:
        line 6: 0
        line 7: 8
        line 8: 12
}
</code></pre>
<p>在hotspot的<code>vm/interpreter/bytecodes.hpp</code>中定义了一个字节码table，可查到上面各个指令对应的字节码值如下：</p>
<pre><code class="language-cpp">_new                  = 187, // 0xbb
_dup                  =  89, // 0x59
_invokespecial        = 183, // 0xb7
_astore_1             =  76, // 0x4c
_aload_1              =  43, // 0x2b
_invokespecial        = 183, // 0xb7
_return               = 177, // 0xb1
</code></pre>
<p>使用vim编辑<code>HelloWorld.class</code> 这块对应的二进制文件(从bb开头)如下:
<img src="java/./images/helloworld-main-byte-codes.jpeg"  width=400px/></p>
<h4 id="helloworldclass的加载"><a class="header" href="#helloworldclass的加载">HelloWorld.class的加载</a></h4>
<p>在HotSpot中由ClassFileParser负责解析class文件，并创建Class对应的instanceKlass实例，首先在<code>vm/classfile/classFileParser.cpp</code>中加入一段代码判断时候是HelloWorld.class的方法, 以方便打断点~~</p>
<pre><code class="language-cpp">InstanceKlass* ClassFileParser::create_instance_klass(bool changed_by_loadhook, TRAPS) {
  if ( _klass != NULL) {
    return _klass;
  }

  InstanceKlass* const ik =
    InstanceKlass::allocate_instance_klass(*this, CHECK_NULL);

  fill_instance_klass(ik, changed_by_loadhook, CHECK_NULL);
  //新加的代码，以在加载HelloWorld.class的时候才打断点
  if (ik-&gt;_name-&gt;index_of_at(0, &quot;HelloWorld&quot;, strlen(&quot;HelloWorld&quot;)) != -1){
      assert(_klass == ik, &quot;invariant&quot;);
  }
  //other code
 }
 __
</code></pre>
<p>然后准备的lldb调试脚本如下：</p>
<pre><code class="language-sh">file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;
#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

breakpoint set --file classFileParser.cpp --line 5229
breakpoint command add
print *ik
print ik-&gt;_methods-&gt;_data[0]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[0]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[0]-&gt;_constMethod-&gt;code_size()`

print ik-&gt;_methods-&gt;_data[1]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[1]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[1]-&gt;_constMethod-&gt;code_size()`

print ik-&gt;_methods-&gt;_data[2]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_size()`
DONE

run  -Xint HelloWorld
</code></pre>
<p>最后HelloWorld.main对应的的调试lldb输出如下:</p>
<pre><code class="language-log">(lldb)  print ik-&gt;_methods-&gt;_data[2]-&gt;name_and_sig_as_C_string()
(char *) $7 = 0x0000000101002d30 &quot;HelloWorld.main([Ljava/lang/String;)V&quot;
(lldb)  memory read  ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_size()`
0x121faab00: bb 00 03 59 b7 00 04 4c 2b b6 00 05 b1           �..Y�..L+�..�
</code></pre>
<p>通过上面的lldb调试可以看到，对于java中的<code>HelloWorld</code>这个类，hotspot创建了一个对应的<code>InstanceKlass</code>实例（假定为ik), <code>ik-&gt;_methods</code>中包含了helloworld中的方法。 HelloWorld中的方法对应的字节码, 保存在<code>ik-&gt;_methods-&gt;_data[i]-&gt;_constMethod-&gt;code_base()</code>指向保存它的字节码的内存，然后``ik-&gt;_mehods-&gt;_data[i]-&gt;_constMethod-&gt;_consts, 都指向了这个HelloWorld.class中对应的常量池。ConstPool-&gt;_tags这个数组标明了每个常量的类型（比如methodref对应这JVM_CONSTANT_Methodref,等等)</p>
<img src="java/./images/helloworld-main-method.jpeg" />
<h3 id="callstack-分析"><a class="header" href="#callstack-分析">callstack 分析</a></h3>
<p>通过<a href="java/./test/helloworld-main-load-stack">callstack</a>可以看到, 一个class的加载要通过下面的流程。SystemDictionary负责保存已经loadedclass的一个map, 如果mapl里面有了，就直接返回，如果没有，就调用classLoader去加载class文件，最后用klassFactory从class文件中创建出InstanceKlass来。</p>
<pre><code>Class.c --&gt; SystemDictionary --&gt; ClassLoader --&gt; klassFactory --&gt; classFileParser--&gt;Inputstream --&gt; HelloWorld.class文件
</code></pre>
<pre><code>frame #0: classFileParser.cpp:5229: ClassFileParser::create_instance_klass
frame #1: klassFactory.cpp:203: KlassFactory::create_from_stream
frame #2: systemDictionary.cpp:1142: SystemDictionary::resolve_from_stream
...
frame #5: ClassLoader.c:150: Java_java_lang_ClassLoader_defineClass1
...
frame #21: systemDictionary.cpp:1586: SystemDictionary::load_instance_class
...
frame #24: systemDictionary.cpp:185: SystemDictionary::resolve_or_fail
...
frame #27: Class.c:135: Java_java_lang_Class_forName0
....
 frame #38: java.c:1543: LoadMainClass
 frame #39: java.c:477: JavaMain
</code></pre>
<h3 id="instanceklass的link"><a class="header" href="#instanceklass的link">InstanceKlass的link</a></h3>
<p>常量池中符号的resolve, method的link.
//TODO</p>
<h2 id="interpreter"><a class="header" href="#interpreter">Interpreter</a></h2>
<p>在上面加载class的<a href="java/./test/helloworld-main-load-stack">callstack</a>中可以看到有几段的callstack是机器码， 那些是TemplateInterpreter初始化的时候，生成的StubCode(机器代码), java的字节码就是在StubCode中按字节码解释执行（或者直接编译好机器码，直接跑的)~~。</p>
<pre><code class="language-cpp">frame #27: Class.c:135: Java_java_lang_Class_forName0
frame #28: 0x000000010602c838 0x000000010602c838
frame #29: 0x000000010600b220 0x000000010600b220
frame #30: 0x000000010600b220 0x000000010600b220
frame #31: 0x000000010600b220 0x000000010600b220
frame #32: 0x00000001060009f1 0x00000001060009f1
frame #33: javaCalls.cpp:410: JavaCalls::call_helper
frame #34: os_bsd.cpp:3682: os::os_exception_wrapper
frame #35: javaCalls.cpp:306: JavaCalls::call
</code></pre>
<p>JavaCalls::call 这个是从jvm中调java方法的入口。可以在跑代码的时候加个<code>-XX:+PrintInterpreter</code>选项打印这些生成的studecode的代码。</p>
<h4 id="stubqueue的创建"><a class="header" href="#stubqueue的创建">StubQueue的创建</a></h4>
<p>在hotspot中有三种解释器：TemplateInterpreter，CppInterprete 还有遗留的bytecodeInterpreter。默认用的是TemplateInterpreter，TemplateInterpreter在初始化的时候会把字节码对应的执行代码通过MASM直接转对应平台(比如X86， X86-64)对应的机器代码, 这部分的机器码作为一个个Stub保存在StubQueue中，除了字节码, method_entry也会生成一个个的stub。 生成Stub的callstack如下：</p>
<pre><code>* frame #0: templateInterpreterGenerator.cpp:57: TemplateInterpreterGenerator::generate_all()
  frame #1: templateInterpreterGenerator.cpp:40: TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue*)
  frame #2: templateInterpreterGenerator.cpp:37: TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue*)
  frame #3: templateInterpreter.cpp:56: TemplateInterpreter::initialize()
  frame #4: interpreter.cpp:116: interpreter_init()
  frame #5: init.cpp:115: init_globals()
  frame #6: thread.cpp:3623: Threads::create_vm(JavaVMInitArgs*, bool*)
  frame #7: jni.cpp:3938: JNI_CreateJavaVM_inner(JavaVM_**, void**, void*)
  frame #8: jni.cpp:4033: ::JNI_CreateJavaVM(JavaVM **, void **, void *)
  frame #9: java.c:1450: InitializeJVM
  frame #10: java.c:402: JavaMain
</code></pre>
<h4 id="分配stubqueue内存"><a class="header" href="#分配stubqueue内存">分配StubQueue内存</a></h4>
<p>在<code>TemplateInterpreter::initialize</code>中，首先会去申请一块内存，存放stubcode, 然后下面TemplateInterpreterGenerator的代码都会保存到这块内存里面。</p>
<pre><code class="language-cpp">//TemplateInterpreter::initialize
// generate interpreter
 { ResourceMark rm;
   TraceTime timer(&quot;Interpreter generation&quot;, TRACETIME_LOG(Info, startuptime));
   int code_size = InterpreterCodeSize;
   NOT_PRODUCT(code_size*=4;)  // debug uses extra interpreter code space
   _code = new StubQueue(new InterpreterCodeletInterface, code_size, NULL,
                         &quot;Interpreter&quot;);
   TemplateInterpreterGenerator g(_code);
 }
 __
</code></pre>
<p>code_size 和各个平台是相关的，比如x86平台, 大小为224K。</p>
<pre><code>hotspot/src/cpu/x86/vm/templateInterpreterGenerator_x86.cpp
58:int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;
60:int TemplateInterpreter::InterpreterCodeSize = 224 * 1024;
</code></pre>
<h4 id="生成字节码对应的-stub"><a class="header" href="#生成字节码对应的-stub">生成字节码对应的 stub</a></h4>
<p>在set_entry_points_for_all_bytes里面，会遍历所有的bytecode，根据预先创建好的_template_table（这个表是在TemplateTable::initialize初始化的时候创建的）去生成字节码对应的code。比如字节码_new生成stubcode时候的callstack如下:</p>
<pre><code>   frame #0: templateTable_x86.cpp:3830: TemplateTable::_new()
   frame #1: templateTable.cpp:63: Template::generate(InterpreterMacroAssembler*)
   frame #2: templateInterpreterGenerator.cpp:396: TemplateInterpreterGenerator::generate_and_dispatch(Template*, TosState)
   frame #3: templateInterpreterGenerator_x86.cpp:1814: TemplateInterpreterGenerator::set_vtos_entry_points(Template*, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;)
   frame #4: templateInterpreterGenerator.cpp:364: TemplateInterpreterGenerator::set_short_entry_points(Template*, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;)
   frame #5: templateInterpreterGenerator.cpp:329: TemplateInterpreterGenerator::set_entry_points(Bytecodes::Code)
   frame #6: templateInterpreterGenerator.cpp:285: TemplateInterpreterGenerator::set_entry_points_for_all_bytes()
   frame #7: templateInterpreterGenerator.cpp:263: TemplateInterpreterGenerator::generate_all()
</code></pre>
<p>生成字节码对应的stub的函数如下，这里的_gen这个generate就是<code>TemplateInterpreter::_new</code>了。 masm最后的flush会把机器码都flush到StubQueue那边分配的buffer中。</p>
<pre><code class="language-cpp">void Template::generate(InterpreterMacroAssembler* masm) {
  // parameter passing
  TemplateTable::_desc = this;
  TemplateTable::_masm = masm;
  // code generation
  _gen(_arg);
  masm-&gt;flush();
}
</code></pre>
<p>字节码的stub生成完之后，interpreter会有个_normal_table 保存对这些bytecode对应的stubcode的引用, 这儿entry的一堆参数代表了寄存器， 在dispatch_next中会用到这个表。</p>
<pre><code class="language-cpp">EntryPoint entry(bep, zep, cep, sep, aep, iep, lep, fep, dep, vep);
Interpreter::_normal_table.set_entry(code, entry);
</code></pre>
<p>dispatch_next中会去取bytecode对应stubcode的地址，然后在dispatch_base中jmp到字节码对应的code去执行。</p>
<pre><code class="language-cpp">void InterpreterMacroAssembler::dispatch_next(TosState state, int step) {
  // load next bytecode (load before advancing _bcp_register to prevent AGI)
  load_unsigned_byte(rbx, Address(_bcp_register, step));
  // advance _bcp_register
  increment(_bcp_register, step);
  dispatch_base(state, Interpreter::dispatch_table(state));
}
__
</code></pre>
<h4 id="method-entry"><a class="header" href="#method-entry">method entry</a></h4>
<p>hotspot中给java的method分了好几类，这样对不同种类的method可以做专门的优化, 比如针对<code>java_lang_math_sin</code>这些常用的数学函数，对应的method entry就直接是sin的汇编代码了。</p>
<p>Method entry的种类定义在了AbstractInterpreter::MethodKind中，通常用的都是zerolocals, 同步的method入口就是<code>zerolocals_synchronized</code>这个了，相应的还有native, native_synchronized， native的方法。部分的methodKind如下:</p>
<pre><code class="language-cpp">    zerolocals,                                                 // method needs locals initialization
   zerolocals_synchronized,                                    // method needs locals initialization &amp; is synchronized
   native,                                                     // native method
   native_synchronized,                                        // native method &amp; is synchronized
   empty,                                                      // empty method (code: _return)
   accessor,                                                   // accessor method (code: _aload_0, _getfield, _(a|i)return)
   abstract,                                                   // abstract method (throws an AbstractMethodException)
   method_handle_invoke_FIRST,                                 // java.lang.invoke.MethodHandles::invokeExact, etc.
   java_lang_math_sin,                                         // implementation of java.lang.Math.sin   (x)

</code></pre>
<p>method entry定义如下：</p>
<pre><code class="language-cpp">#define method_entry(kind)                                              \
  { CodeletMark cm(_masm, &quot;method entry point (kind = &quot; #kind &quot;)&quot;); \
    Interpreter::_entry_table[Interpreter::kind] = generate_method_entry(Interpreter::kind); \
    Interpreter::update_cds_entry_table(Interpreter::kind); \
  }
__
</code></pre>
<p>对于zerolocals_synchronized和zerolocals对应的入口是: <code>generate_normal_entry </code>生成的stubcode, 可以看到, zerolocals_synchronized多了调了一个<code>lock_method</code>, 而且调用了dispatch_next jmp到bytecode对应的stubcode.</p>
<pre><code class="language-cpp">// address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
 const Address constMethod(rbx, Method::const_offset());
 const Address access_flags(rbx, Method::access_flags_offset());
 const Address size_of_parametersrdx,
 const Address size_of_locals(rdx, ConstMethod::size_of_locals_offset());


 // get parameter size (always needed)
 __ movptr(rdx, constMethod);


  //other code
 if (synchronized) {
    // Allocate monitor and lock method
    lock_method();
  }
  //other code
__ notify_method_entry();
  //other code
 __ dispatch_next(vtos);
  //other code
</code></pre>
<p>native方法入口generate_native_entry 生成部分如下, 最终会去call native的方法。</p>
<pre><code class="language-cpp">//address TemplateInterpreterGenerator::generate_native_entry(bool synchronized)
// allocate space for parameters
 __ get_method(method);
 __ movptr(t, Address(method, Method::const_offset()));
 __ load_unsigned_short(t, Address(t, ConstMethod::size_of_parameters_offset()));
//other code
__ call(t);
__ get_method(method);        // slow path can do a GC, reload RBX
//other code
</code></pre>
<h4 id="methodlink"><a class="header" href="#methodlink">method::link</a></h4>
<p>这个entry_table中的入口最后会在instancKlass中method::link的时候和method关联起来。</p>
<pre><code>//method::link代码片段
address entry = Interpreter::entry_for_method(h_method);
set_interpreter_entry(entry);

//native functions
if (is_native() &amp;&amp; !has_native_function()) {
  set_native_function(
    SharedRuntime::native_method_throw_unsatisfied_link_error_entry(),
    !native_bind_event_is_interesting);
}

//设置method的入口
void set_interpreter_entry(address entry) {
    assert(!is_shared(), &quot;shared method's interpreter entry should not be changed at run time&quot;);
    if (_i2i_entry != entry) {
      _i2i_entry = entry;
    }
    if (_from_interpreted_entry != entry) {
      _from_interpreted_entry = entry;
    }
  }
</code></pre>
<p>TemplateInterpreter初始画之后，各个table之前的关系图如下:</p>
<img src="java/./images/StubQueue.jpeg"/>
<h3 id="helloworld的static-main的执行"><a class="header" href="#helloworld的static-main的执行">HelloWorld的static main的执行</a></h3>
<p>经过上面的分析，再来看java代码中main被执行过程，首先在JavaMain中加载main class，然后获得class的static main method, 最后调用了这个static method，开始执行HelloWorld.class的Main method。</p>
<pre><code class="language-cpp">//JavaMain 代码片段
mainClass = LoadMainClass(env, mode, what);
//...some other code
mainID = (*env)-&gt;GetStaticMethodID(env, mainClass, &quot;main&quot;,
                                   &quot;([Ljava/lang/String;)V&quot;);

//...some other code
(*env)-&gt;CallStaticVoidMethod(env, mainClass, mainID, mainArgs);
</code></pre>
<p>这里先不管LoadMainClass的过程， 主要分析CallStaticVoidMethod这个方法。首先准备下面的lldb调试脚本, 在JavaMain 执行CallStaticVoidMethod之前打个断点，然后再在JavaCalls::call_helper中打个断点。</p>
<pre><code>#just a line before   (*env)-&gt;CallStaticVoidMethod(env, mainClass, mainID, mainArgs);
breakpoint set --file java.c --line 517
breakpoint command add
breakpoint set --method JavaCalls::call_helper
continue
DONE
</code></pre>
<p>可以看到callstack如下</p>
<pre><code>* frame #0: javaCalls.cpp:360: JavaCalls::call_helper(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #1: os_bsd.cpp:3682: os::os_exception_wrapper(void (*)(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*), JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #2: javaCalls.cpp:306: JavaCalls::call(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #3: jni.cpp:1120: jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)
    frame #4: jni.cpp:1990: ::jni_CallStaticVoidMethod(JNIEnv *, jclass, jmethodID, ...)
    frame #5: java.c:518: JavaMain
</code></pre>
<p><b>JavaCalls::call_helper</b></p>
<p>在JavaCalls::call_helper中关键代码片段如下， 首先设置好method的entry point， 这个entry point就是上文中所说的interpreter初始化的时候建立的method entry point（两者之间连接是在method::link的时候建立的）。</p>
<pre><code class="language-cpp">//JavaCalls::call_helper代码片段

//设置entry point
address entry_point = method-&gt;from_interpreted_entry();
if (JvmtiExport::can_post_interpreter_events() &amp;&amp; thread-&gt;is_interp_only_mode()) {
  entry_point = method-&gt;interpreter_entry();
}
//other code

// do call
  { JavaCallWrapper link(method, receiver, result, CHECK);
    { HandleMark hm(thread);  // HandleMark used by HandleMarkCleaner

      StubRoutines::call_stub()(
        (address)&amp;link,
        // (intptr_t*)&amp;(result-&gt;_value), // see NOTE above (compiler problem)
        result_val_address,          // see NOTE above (compiler problem)
        result_type,
        method(),
        entry_point,
        args-&gt;parameters(),
        args-&gt;size_of_parameters(),
        CHECK
      );

      result = link.result();  // circumvent MS C++ 5.0 compiler bug (result is clobbered across call)
      // Preserve oop return value across possible gc points
      if (oop_result_flag) {
        thread-&gt;set_vm_result((oop) result-&gt;get_jobject());
      }
    }

    //保存执行结果：
    if (oop_result_flag) {
       result-&gt;set_jobject((jobject)thread-&gt;vm_result());
       thread-&gt;set_vm_result(NULL);
     }
</code></pre>
<p><b>call_stub</b></p>
<p>这个call_stub也是一段汇编代码（定义在StubGenerator_x86_X64.cpp:203由generate_call_stub生成）它在保存好一堆寄存器和栈之后，就把用到的参数都压到寄存器里面，然后调method的entry point去执行，执行完了再把寄存器和栈恢复了。</p>
<p>其中的call Java function部分的汇编代码如下：</p>
<pre><code class="language-c">// call Java function
    __ BIND(parameters_done);
    __ movptr(rbx, method);             // get Method*
    __ movptr(c_rarg1, entry_point);    // get entry_point
    __ mov(r13, rsp);                   // set sender sp
    BLOCK_COMMENT(&quot;call Java function&quot;);
    __ call(c_rarg1);

    BLOCK_COMMENT(&quot;call_stub_return_address:&quot;);
    return_address = __ pc();
</code></pre>
<p><b>JavaCallWrapper</b></p>
<p>在JavaCallWrapper的构造函数中，会申请一个新的JNIHandleBlock，并把它设置为thread的active_handles，在析构函数中会恢复线程之前的JNIHandleBlock，并释放之前申请的JNIHandleBlock。</p>
<p>线程自身维护一个free_handle_block的list， 申请JNIHandleBlock的时候，就从这里面去取，如果freelist用完了，才回去加个mutex lock new一个JNIHanleBlock。释放的时候，就放回到这个list里面。</p>
<p>很多的java code都会去调用vm/prims/jvm.cpp中的JVM_ENTRY，而JVM_ENTRY会用<code>JNIHanleBlock-&gt;make_local</code>保存返回给java代码的的结果。在java code跑完以后,由JavaCallWrapper的destructor负责释放这些内存。</p>
<pre><code class="language-cpp">jobject JNIHandles::make_local(Thread* thread, oop obj) {
  if (obj == NULL) {
    return NULL;                // ignore null handles
  } else {
    assert(Universe::heap()-&gt;is_in_reserved(obj), &quot;sanity check&quot;);
    return thread-&gt;active_handles()-&gt;allocate_handle(obj);
  }
}
</code></pre>
<h3 id="bytecode对应的代码"><a class="header" href="#bytecode对应的代码">bytecode对应的代码</a></h3>
<p>hotspot默认用的是TemplateInterpreter把字节码对应的代码直接生成汇编代码，读起来比较难。hotspot中还有一个bytecodeInterpreter，这个完全是用cpp写的，两者从逻辑上看起来没区别。所以看每个bytecode对应的代码可以从bytecodeInterpreter入手。</p>
<h4 id="new"><a class="header" href="#new">new</a></h4>
<p>java中的new 一个class 有两个path： 一个是fastpath: class对应的instanceKlass已经解析，初始化好了，这种比较快，另外一种是
slowpath需要去掉interpreter的runtime 去link, init这个klass, 然后把它放到constpoll的cache里面， 然后分配内存。</p>
<p>在heap堆或者thread的localstorage上分配内存， 设置好oop的 mark head,还有iklass指针，iklass指向在jvm中代表该java类的instanceKlass。</p>
<p>首先判断instanceklass是否已经加载了并且初始化了</p>
<pre><code class="language-cpp">ConstantPool* constants = istate-&gt;method()-&gt;constants();
        if (!constants-&gt;tag_at(index).is_unresolved_klass()) {
          // Make sure klass is initialized and doesn't have a finalizer
          Klass* entry = constants-&gt;slot_at(index).get_klass();
          InstanceKlass* ik = InstanceKlass::cast(entry);
          if (ik-&gt;is_initialized() &amp;&amp; ik-&gt;can_be_fastpath_allocated() ) {
</code></pre>
<p>如果要求useTLAB的话，就分配在thread localstorage上</p>
<pre><code>size_t obj_size = ik-&gt;size_helper();
if (UseTLAB) {
    result = (oop) THREAD-&gt;tlab().allocate(obj_size);
}

</code></pre>
<p>否则从heap上分配</p>
<pre><code class="language-cpp">HeapWord* compare_to = *Universe::heap()-&gt;top_addr();
HeapWord* new_top = compare_to + obj_size;
if (new_top &lt;= *Universe::heap()-&gt;end_addr()) {
  if (Atomic::cmpxchg_ptr(new_top, Universe::heap()-&gt;top_addr(), compare_to) != compare_to) {
    goto retry;
  }
  result = (oop) compare_to;
}
</code></pre>
<p>然后就是初始化这块内存了，设置好对象的markhead和kclass指针, 最后把它放到栈上。</p>
<pre><code class="language-cpp">if (need_zero ) {
    HeapWord* to_zero = (HeapWord*) result + sizeof(oopDesc) / oopSize;
    obj_size -= sizeof(oopDesc) / oopSize;
    if (obj_size &gt; 0 ) {
      memset(to_zero, 0, obj_size * HeapWordSize);
    }
  }
  if (UseBiasedLocking) {
    result-&gt;set_mark(ik-&gt;prototype_header());
  } else {
    result-&gt;set_mark(markOopDesc::prototype());
  }
  result-&gt;set_klass_gap(0);
  result-&gt;set_klass(ik);
  // Must prevent reordering of stores for object initialization
  // with stores that publish the new object.
  OrderAccess::storestore();
  SET_STACK_OBJECT(result, 0);
  UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
</code></pre>
<p>对于slowcase回去调用<code>interpreterRuntime::_new</code>去创建对象</p>
<pre><code class="language-cpp">CALL_VM(InterpreterRuntime::_new(THREAD, METHOD-&gt;constants(), index),
             handle_exception);
     // Must prevent reordering of stores for object initialization
     // with stores that publish the new object.
     OrderAccess::storestore();
     SET_STACK_OBJECT(THREAD-&gt;vm_result(), 0);
     THREAD-&gt;set_vm_result(NULL);
     UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
___
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raft"><a class="header" href="#raft">raft</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raft-1"><a class="header" href="#raft-1">Raft</a></h1>
<!-- toc -->
<h2 id="leader-election"><a class="header" href="#leader-election">Leader Election</a></h2>
<p>在raft中，主要有leader, candidate, follower三种状态, 一个cluster只有一个leader, leader负责处理client的写请求，然后
leader将日志push给各个follower。</p>
<p>leader通过心跳机制告诉follower自己还活着，当follower有一段时间没收到leader的心跳后，认为leader已经挂掉后，就转变为candidate，
发起投票请求，尝试成为leader。</p>
<h3 id="term-任期"><a class="header" href="#term-任期">term: 任期</a></h3>
<p>在Raft中，任期扮演着逻辑时钟的角色，节点之间的请求和返回中都带上node当前的term。node在处理请求时，发现请求中的term比自己大，就
将自己term 改为该值，如果比自己小，就拒绝请求，并返回带上自己term。 </p>
<p>leader发送给follower的心跳中，如果收到的回复中, follower term比自己大，那么leader就知道自己stale了，就会step down.</p>
<p>candidate在发起RequestForVote时候，会将自己term +=1 , 然后经过一轮处理后，整个集群term都会增加。</p>
<h3 id="appendentries"><a class="header" href="#appendentries">AppendEntries</a></h3>
<p>AppendEntries 是由leader发送给follower的RPC请求，主要有两个作用:</p>
<ol>
<li>同步日志。</li>
<li>AppendEntries的log entriy可以为空，扮演着心跳的角色，心跳用于抑制follower转变为candidate。</li>
</ol>
<h3 id="majority-vote"><a class="header" href="#majority-vote">Majority Vote</a></h3>
<p>follower 变为candidate之后，会将自己term + 1, 并且会发送RequestForVote请求给所有成员，开始选举，如果收到了大部分成员的投票，则成为
新的任期的leader。 SplitVote是选举中要解决的主要问题。</p>
<h4 id="splitvote"><a class="header" href="#splitvote">SplitVote</a></h4>
<p>多个candidate 同时发起投票时候，可能每个candidate可能获得的选票都达不到大多数，为了解决这个问题，Raft采用了random election timeout的机制，每个
candidate的election timout是个随机值，可以在很大程度上保证一段时间内只有一个candidate在request for vote。</p>
<p><img src="papers/./raft-server-state.svg" alt="raft server state" /></p>
<h2 id="log-replication"><a class="header" href="#log-replication">Log Replication</a></h2>
<p>一条日志，只有被复制到cluster中大部分server上时候，才会被认为是commited。被commited日志才能apply 到raft的state machine上。
leader自己的日志只能append,不能rewrite，不然后面的commited index就没啥用了。</p>
<p>leader发送给follower的心跳请求中带了当前leader commited index， follower根据这个信息来判断一条日志能安全的apply 到statemachine上。</p>
<p>每条日志都有term和index，如果两条日志的term和index是一致的，那么这两条日志就被认为是一致的。
新leader当选后，需要向follower push自己的日志。leader需要找到和follower日志共同的起点，然后从该点同步follower日志。</p>
<p>Leader维护了一个NextIndex数组，NextIndex[i]表示下一次要向follower发送日志的index。</p>
<p><img src="papers/./raft-sub-problem.svg" alt="raft sub problem" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gfs"><a class="header" href="#gfs">GFS</a></h1>
<h2 id="questions-7"><a class="header" href="#questions-7">Questions</a></h2>
<ol>
<li>master 和chunk之间是怎么互相自动发现的？</li>
<li>master 和chunk之间心跳信息具体内容是啥</li>
<li>master信息存在哪儿？master挂了？集群都挂？</li>
<li>Cache怎么解决失效的问题？</li>
<li>谁负责写入多个副本？</li>
<li>副本的一致性是怎么保证的</li>
<li>Atomic Append是咋搞的</li>
<li>写入流程是怎样的？</li>
</ol>
<p><img src="papers/./gfs-arch.svg" alt="gfs arch" /></p>
<h3 id="chunksize"><a class="header" href="#chunksize">ChunkSize</a></h3>
<p>chunksize 64MB的好处</p>
<ol>
<li>减轻client 和master的通信.</li>
<li>client和chunk server长时间通信,减少需要和多个chunk server网络通信</li>
</ol>
<p>缺点：</p>
<ol>
<li>小文件只有一个或几个chunk，容易造成成为热点</li>
</ol>
<h3 id="metadata"><a class="header" href="#metadata">Metadata</a></h3>
<p>master 相当于一个路由表, master主要存储三种metadata</p>
<ol>
<li>the file and chunk namespace</li>
<li>mapping from files to chunk</li>
<li>location of each chunk replica
这三个信息都存储在内存中，前两个信息会通过operation log, 持久化存储到磁盘上
信息3没有存在磁盘上，master询问每个chunk server, 来构建这个信息.</li>
</ol>
<h2 id="写入流程-1"><a class="header" href="#写入流程-1">写入流程</a></h2>
<h3 id="lease-and-mutation-order"><a class="header" href="#lease-and-mutation-order">lease and mutation order</a></h3>
<p><img src="papers/./gfs-write.svg" alt="gfs write" /></p>
<p>如果修改的区域跨chunk了,上面的lease机制无法保证对多个chunk的修改，有一致的修改顺序。</p>
<h3 id="atomic-record-append"><a class="header" href="#atomic-record-append">Atomic record append</a></h3>
<p>这块没怎么看明白，好像是append时候，如果primary发现chunk size不够写，就直接先将当前chunk pad填满，并且
让secondary也pad，填充, 然后让client重试,为了避免过多的碎片，chunk append的record size 现在在<code>maxSize/4</code>
这样就避免了跨chunk写</p>
<h3 id="snapshot-3"><a class="header" href="#snapshot-3">snapshot</a></h3>
<p><img src="papers/./gfs-snapshot.svg" alt="gfs snapshot" /></p>
<p><img src="papers/./gfs-snapshot-cow.svg" alt="gfs snapshot cow" /></p>
<h2 id="master-operation"><a class="header" href="#master-operation">Master operation</a></h2>
<ol>
<li>namespace namespace operation </li>
<li>manages chunk replicas</li>
<li>placement decisions</li>
</ol>
<p><img src="papers/./gfs-master-operation.svg" alt="gfs master operation" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bw-tree"><a class="header" href="#bw-tree">Bw-tree</a></h1>
<p>the Bw-tree achieves its very high performance via a latch-free approach 
that effectively exploits the processor caches of modern multi-core chips.</p>
<p>is paper is on the main memory aspects of the Bw-tree. We describe the details of our latch-free tech- nique,</p>
<p>we need to get better at exploiting a large number of cores by addressing at least two important aspects</p>
<ol>
<li>Multi-core cpus mandate high concurrency. But, as the level of concurrency increases, latches are more likely to block, limiting scalability</li>
</ol>
<p>lock 限制了多核并发能力</p>
<ol start="2">
<li>Good multi-core processor performance depends on high CPU cache hit ratios. Updating memory in place results in cache invalidations, so how and when updates are done needs great care. </li>
</ol>
<p>cpu cache hit rate, 如何利用cpu cache hit rate</p>
<p>the Bw-tree performs “delta” updates that avoid updating a page in place, hence preserving previously cached lines of pages</p>

                        <div id="giscus-container"></div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>
        <script type="text/javascript" charset="utf-8">
        var pagePath = "print.md"
        </script>


        <!-- Custom JS scripts -->
        <script type="text/javascript" src="assets/custom.js"></script>
        <script type="text/javascript" src="assets/bigPicture.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>